{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 形参设定"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b7086c61f2f369d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(\"mini-batch HAN\")\n",
    "parser.add_argument(\"-s\", \"--seed\", type=int, default=1, help=\"Random seed\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "parser.add_argument(\"--num_neighbors\", type=int, default=20)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001)\n",
    "parser.add_argument(\"--num_heads\", type=list, default=[8])\n",
    "parser.add_argument(\"--hidden_units\", type=int, default=8)\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.6)\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0.001)\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=100)\n",
    "parser.add_argument(\"--patience\", type=int, default=10)\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"ACMRaw\")\n",
    "parser.add_argument(\"--device\", type=str, default=\"cpu\")\n",
    "\n",
    "args, _ = parser.parse_known_args()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:06:56.607971600Z",
     "start_time": "2023-11-08T08:06:56.488803500Z"
    }
   },
   "id": "53b1acef6a0c34c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 一、数据预处理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83e6eabd79437903"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 获取acm数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24d0d2d5cf7a0537"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:00.803871500Z",
     "start_time": "2023-11-08T08:06:56.508883800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\Wei Zhou\\.dgl/ACM.mat from https://data.dgl.ai/dataset/ACM.mat...\n"
     ]
    },
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Wei Zhou\\\\.dgl/ACM.mat'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dgl.data.utils import _get_dgl_url, download, get_download_dir\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from scipy import io as sio, sparse\n",
    "\n",
    "## 设定acm数据集下载路径\n",
    "url = \"dataset/ACM.mat\"\n",
    "data_path = get_download_dir() + \"/ACM.mat\"\n",
    "download(_get_dgl_url(url), path=data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 提取相关的矩阵以下有四种矩阵"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a564af9670cf5b1f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有12499篇paper，一共有73个领域\n",
      "一共有12499篇paper，一共有17431个作者\n",
      "一共有12499篇paper，一共有1903个专有名词\n",
      "一共有12499篇paper，一共有14个会议期刊\n"
     ]
    }
   ],
   "source": [
    "data = sio.loadmat(data_path)\n",
    "p_vs_l = data[\"PvsL\"]  # paper-field\n",
    "p_vs_a = data[\"PvsA\"]  # paper-author\n",
    "p_vs_t = data[\"PvsT\"]  # paper-term, bag of words\n",
    "p_vs_c = data[\"PvsC\"]  # paper-conference, labels come from that\n",
    "\n",
    "print(f'一共有{p_vs_l.shape[0]}篇paper，一共有{p_vs_l.shape[1]}个领域')\n",
    "print(f'一共有{p_vs_a.shape[0]}篇paper，一共有{p_vs_a.shape[1]}个作者')\n",
    "print(f'一共有{p_vs_t.shape[0]}篇paper，一共有{p_vs_t.shape[1]}个专有名词')\n",
    "print(f'一共有{p_vs_c.shape[0]}篇paper，一共有{p_vs_c.shape[1]}个会议期刊')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.162443Z",
     "start_time": "2023-11-08T08:07:00.804867900Z"
    }
   },
   "id": "cbe51356d7ef49ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 根据p_vs_c矩阵选取我们需要的相关领域的期刊会议的paper"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d216df7abfb9f50"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# We assign\n",
    "# (1) KDD papers as class 0 (data mining),\n",
    "# (2) SIGMOD and VLDB papers as class 1 (database),\n",
    "# (3) SIGCOMM and MOBICOMM papers as class 2 (communication)\n",
    "## 确定选取的期刊会议\n",
    "conf_ids = [0, 1, 9, 10, 13]\n",
    "## 对应期刊的领域的标签进行设定0为数据挖掘领域，1为数据库领域，2为通信领域\n",
    "label_ids = [0, 1, 2, 2, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.208094500Z",
     "start_time": "2023-11-08T08:07:01.163441900Z"
    }
   },
   "id": "ad0776520c4c0dec"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "##选取五个会议期刊\n",
    "p_vs_c_filter = p_vs_c[:, conf_ids]\n",
    "\n",
    "##将矩阵按照行求和，然后设定其不等于0从而提取和这五个期刊会议相关的paper，再获取这些paper的编码作为我们所需要的paper\n",
    "p_selected = (p_vs_c_filter.sum(1) != 0).A1.nonzero()[0]\n",
    "\n",
    "## 得到特定的paper后对所有的paper_vs的矩阵取出我们所需要的paper的信息\n",
    "p_vs_l = p_vs_l[p_selected]\n",
    "p_vs_a = p_vs_a[p_selected]\n",
    "p_vs_t = p_vs_t[p_selected]\n",
    "p_vs_c = p_vs_c[p_selected]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.216764600Z",
     "start_time": "2023-11-08T08:07:01.181447800Z"
    }
   },
   "id": "3a9e9565d00910eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 利用p_vs_a，p_vs_l构建异构图元路径"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e7a42f29e1479b7"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "hg = dgl.heterograph(\n",
    "    {\n",
    "        (\"paper\", \"pa\", \"author\"): p_vs_a.nonzero(),\n",
    "        (\"author\", \"ap\", \"paper\"): p_vs_a.transpose().nonzero(),\n",
    "        (\"paper\", \"pf\", \"field\"): p_vs_l.nonzero(),\n",
    "        (\"field\", \"fp\", \"paper\"): p_vs_l.transpose().nonzero(),\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.225523700Z",
     "start_time": "2023-11-08T08:07:01.194386800Z"
    }
   },
   "id": "d5f584f96fa9493c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 利用p_vs_t构建节点特征矩阵"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c839973448c42195"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "features_adj = torch.FloatTensor(p_vs_t.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.255703100Z",
     "start_time": "2023-11-08T08:07:01.226784900Z"
    }
   },
   "id": "bbab230f87c35aa8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 对被选取的paper进行重新编码"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f363150cb5a0eee"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "pc_p, pc_c = p_vs_c.nonzero()\n",
    "label = np.zeros(len(p_selected), dtype=np.int64)\n",
    "for conf_id, label_id in zip(conf_ids, label_ids):\n",
    "    label[pc_p[pc_c == conf_id]] = label_id\n",
    "label = torch.LongTensor(label)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.272146700Z",
     "start_time": "2023-11-08T08:07:01.256930300Z"
    }
   },
   "id": "549520565816b158"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 利用mask来决定哪些paper作为训练集，测试集，验证集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e50cc8065a3261"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "##随机生成的浮点数值，根据这些值将论文分配到不同的数据集中\n",
    "num_classes = 3\n",
    "float_mask = np.zeros(len(pc_p))\n",
    "for conf_id in conf_ids:\n",
    "    pc_c_mask = pc_c == conf_id\n",
    "    float_mask[pc_c_mask] = np.random.permutation(\n",
    "        np.linspace(0, 1, pc_c_mask.sum())\n",
    "    )\n",
    "train_idx = np.where(float_mask <= 0.2)[0]\n",
    "val_idx = np.where((float_mask > 0.2) & (float_mask <= 0.3))[0]\n",
    "test_idx = np.where(float_mask > 0.3)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.299542900Z",
     "start_time": "2023-11-08T08:07:01.273160100Z"
    }
   },
   "id": "8937bebd8816c129"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_binary_mask(total_size, indices):\n",
    "    mask = torch.zeros(total_size)\n",
    "    mask[indices] = 1\n",
    "    return mask.byte()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.308886900Z",
     "start_time": "2023-11-08T08:07:01.288582700Z"
    }
   },
   "id": "5fbb235375f21d32"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "num_nodes = hg.num_nodes(\"paper\")\n",
    "train_masks = get_binary_mask(num_nodes, train_idx)\n",
    "val_masks = get_binary_mask(num_nodes, val_idx)\n",
    "test_masks = get_binary_mask(num_nodes, test_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.318391500Z",
     "start_time": "2023-11-08T08:07:01.303323200Z"
    }
   },
   "id": "6e35f2f52b2af631"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 该数据预处理的结果"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5954d0b4b363f33"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "## 由paper和author，paper和filed的连接关系组成的异构图\n",
    "g=hg\n",
    "## 由paper所包含的学术词汇构成的节点特征矩阵\n",
    "features=features_adj\n",
    "## 节点类型即每个paper属于哪个领域\n",
    "labels=label\n",
    "## 分类数量\n",
    "n_classes=num_classes\n",
    "## 训练集的节点索引\n",
    "train_nid=train_idx\n",
    "## 验证集节点索引\n",
    "val_nid=val_idx\n",
    "## 测试集节点索引\n",
    "test_nid=test_idx\n",
    "## 索引对应的掩码\n",
    "train_mask=train_masks\n",
    "val_mask=val_masks\n",
    "test_mask=test_masks\n",
    "## 元路径list\n",
    "metapath_list = [[\"pa\", \"ap\"], [\"pf\", \"fp\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.337876800Z",
     "start_time": "2023-11-08T08:07:01.319388300Z"
    }
   },
   "id": "36e9ec9676a68825"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构建HAN采样——HANSampler 类的目标是为了构建用于训练的局部子图块，"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3fbce05158bf3554"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from dgl.sampling import RandomWalkNeighborSampler\n",
    "class HANSampler(object):\n",
    "    def __init__(self, g, metapath_list, num_neighbors):\n",
    "        self.sampler_list = []\n",
    "##遍历元路径列表中的每个路径，并对每个元路径进行随机采样并将采样结果加入到sampler_list中\n",
    "        for metapath in metapath_list:\n",
    "            # note: random walk may get same route(same edge), which will be removed in the sampled graph.\n",
    "            # So the sampled graph's edges may be less than num_random_walks(num_neighbors).\n",
    "            self.sampler_list.append(\n",
    "                RandomWalkNeighborSampler(\n",
    "                ##在哪个图上进行采样\n",
    "                    G=g,\n",
    "                ##每个采样器执行随机游走的次数\n",
    "                    num_traversals=1,\n",
    "                ## 控制随机游走的终止概率\n",
    "                    termination_prob=0,\n",
    "                ##每个节点执行几次随机游走\n",
    "                    num_random_walks=num_neighbors,\n",
    "                ##每个节点采样的邻居节点的数量\n",
    "                    num_neighbors=num_neighbors,\n",
    "                ##使用的元路径是哪个\n",
    "                    metapath=metapath,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "## 构建训练子图\n",
    "    def sample_blocks(self, seeds):\n",
    "        block_list = []\n",
    "##遍历采样列表\n",
    "        for sampler in self.sampler_list:\n",
    "        ##得到子图\n",
    "            frontier = sampler(seeds)\n",
    "        ##移除环\n",
    "            frontier = dgl.remove_self_loop(frontier)\n",
    "        ##添加自连接的边\n",
    "            frontier.add_edges(torch.tensor(seeds), torch.tensor(seeds))\n",
    "        ##将子图转化为block\n",
    "            block = dgl.to_block(frontier, seeds)\n",
    "            block_list.append(block)\n",
    "\n",
    "        return seeds, block_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.349383400Z",
     "start_time": "2023-11-08T08:07:01.334882700Z"
    }
   },
   "id": "2517852f99d04f1c"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "num_neighbors = args.num_neighbors\n",
    "han_sampler = HANSampler(g, metapath_list, num_neighbors)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.376817100Z",
     "start_time": "2023-11-08T08:07:01.351375500Z"
    }
   },
   "id": "40aabadad5b3bcaa"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "## 提取子图block的节点特征\n",
    "def load_subtensors(blocks, features):\n",
    "    h_list = []\n",
    "    for block in blocks:\n",
    "        input_nodes = block.srcdata[dgl.NID]\n",
    "        h_list.append(features[input_nodes])\n",
    "    return h_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.384336900Z",
     "start_time": "2023-11-08T08:07:01.364327Z"
    }
   },
   "id": "f675273e2ef9d25e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 二、定义加载器"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6469f414df742ae7"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.395448300Z",
     "start_time": "2023-11-08T08:07:01.379799300Z"
    }
   },
   "id": "227fd432c42a3b74"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "        dataset=train_nid,\n",
    "        batch_size=args.batch_size,\n",
    "##处理数据块（subgraph）的采样和组装。\n",
    "        collate_fn=han_sampler.sample_blocks,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0,\n",
    "##参数表示是否将加载的数据存储到CUDA固定内存中。\n",
    "        pin_memory=False\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.420262100Z",
     "start_time": "2023-11-08T08:07:01.396436700Z"
    }
   },
   "id": "6aa40345c52c803f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 三、HAN模型设定"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "315d34f2a1bd76b4"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import GATConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SemanticAttention(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size=128):\n",
    "        super(SemanticAttention, self).__init__()\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "    ##元路径在异构图中的重要性\n",
    "        w = self.project(z).mean(0)  # (M, 1)\n",
    "    ##元路径在异构图的注意力得分\n",
    "        beta = torch.softmax(w, dim=0)  # (M, 1)\n",
    "    ##对元路径注意力得分矩阵进行扩张，扩张为第一维度——节点数量，以便于最后的node_embedding生成的最终表示\n",
    "        beta = beta.expand((z.shape[0],) + beta.shape)  # (N, M, 1)\n",
    "    ##根据元路径的注意力得分矩阵和节点语义embedding生成最终的节点embedding\n",
    "        return (beta * z).sum(1)  # (N, D * K)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.428464300Z",
     "start_time": "2023-11-08T08:07:01.412130500Z"
    }
   },
   "id": "64fd6a00d761926b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HAN layer构建"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b78abf8def87479f"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class HANLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, num_metapath, in_size, out_size, layer_num_heads, dropout\n",
    "    ):\n",
    "        super(HANLayer, self).__init__()\n",
    "\n",
    "        # One GAT layer for each meta path based adjacency matrix\n",
    "        ## 节点级别的注意力机制 \n",
    "        #构建 type-specific transformation matrix 特征变化矩阵\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        for i in range(num_metapath):\n",
    "            self.gat_layers.append(\n",
    "                GATConv(\n",
    "                    in_size,\n",
    "                    out_size,\n",
    "                    layer_num_heads,\n",
    "                    dropout,\n",
    "                    dropout,\n",
    "                    activation=F.elu,\n",
    "                    allow_zero_in_degree=True,\n",
    "                )\n",
    "            )\n",
    "        ## 语义级别的注意力机制\n",
    "        self.semantic_attention = SemanticAttention(\n",
    "            in_size=out_size * layer_num_heads\n",
    "        )\n",
    "        self.num_metapath = num_metapath\n",
    "\n",
    "    def forward(self, block_list, h_list):\n",
    "        semantic_embeddings = []\n",
    "## 遍历每个元路径的子图列表并且获得基于该元路径的节点的语义embedding\n",
    "        for i, block in enumerate(block_list):\n",
    "            semantic_embeddings.append(\n",
    "                self.gat_layers[i](block, h_list[i]).flatten(1)\n",
    "            )\n",
    "        semantic_embeddings = torch.stack(\n",
    "            semantic_embeddings, dim=1\n",
    "        )  # (N, M, D * K)\n",
    "        print(semantic_embeddings.shape)\n",
    "## 计算不同元路径的重要性从而得到最终的节点embedding表示\n",
    "        return self.semantic_attention(semantic_embeddings)  # (N, D * K)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.439909700Z",
     "start_time": "2023-11-08T08:07:01.427467200Z"
    }
   },
   "id": "83dbce37ac83a53c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HAN模型构建"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe4b0890590b22b4"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class HAN(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_metapath, in_size, hidden_size, out_size, num_heads, dropout\n",
    "    ):\n",
    "        super(HAN, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(\n",
    "            HANLayer(num_metapath, in_size, hidden_size, num_heads[0], dropout)\n",
    "        )\n",
    "## 多头注意力机制\n",
    "        for l in range(1, len(num_heads)):\n",
    "            self.layers.append(\n",
    "                HANLayer(\n",
    "                    num_metapath,\n",
    "                    hidden_size * num_heads[l - 1],\n",
    "                    hidden_size,\n",
    "                    num_heads[l],\n",
    "                    dropout,\n",
    "                )\n",
    "            )\n",
    "        self.predict = nn.Linear(hidden_size * num_heads[-1], out_size)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for gnn in self.layers:\n",
    "            h = gnn(g, h)\n",
    "\n",
    "        return self.predict(h)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.472891800Z",
     "start_time": "2023-11-08T08:07:01.439909700Z"
    }
   },
   "id": "8e017ed690d639ed"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model = HAN(\n",
    "        num_metapath=len(metapath_list),\n",
    "        in_size=features.shape[1],\n",
    "        hidden_size=args.hidden_units,\n",
    "        out_size=n_classes,\n",
    "        num_heads=args.num_heads,\n",
    "        dropout=args.dropout,\n",
    "    ).to(args.device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.487520500Z",
     "start_time": "2023-11-08T08:07:01.459466500Z"
    }
   },
   "id": "2bf2c024bcb236ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 显示可训练参数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b4709fb02dba38e"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_params: 252611\n",
      "total trainable params: 252611\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"total_params: {:d}\".format(total_params))\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad\n",
    ")\n",
    "print(\"total trainable params: {:d}\".format(total_trainable_params))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.531421700Z",
     "start_time": "2023-11-08T08:07:01.488504800Z"
    }
   },
   "id": "f88e30beefae61d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 设定停止器"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2834eb783555309"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import datetime\n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, patience=10):\n",
    "        dt = datetime.datetime.now()\n",
    "## 设定文件名代入时间\n",
    "        self.filename = \"early_stop_{}_{:02d}-{:02d}-{:02d}.pth\".format(\n",
    "            dt.date(), dt.hour, dt.minute, dt.second\n",
    "        )\n",
    "## 表示连续多少个epoch中验证损失没有改善之后，停止训练\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_acc = None\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "## 每个epoch后调用的方法，用于检查是否应该停止训练\n",
    "    def step(self, loss, acc, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_acc = acc\n",
    "            self.best_loss = loss\n",
    "            self.save_checkpoint(model)\n",
    "## 如果当前的loss大于最佳的loss且acc小于最佳的acc，则增加计数器counter，并打印出早期停止的计数器信息。\n",
    "        elif (loss > self.best_loss) and (acc < self.best_acc):\n",
    "            self.counter += 1\n",
    "            print(\n",
    "                f\"EarlyStopping counter: {self.counter} out of {self.patience}\"\n",
    "            )\n",
    "    ##如果counter达到了设定的patience值，将early_stop标志设置为True，表示应该停止训练\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "## 如果当前的loss小于或等于最佳的loss且acc大于或等于最佳的acc，则更新最佳的loss和acc，并重置计数器counter为0\n",
    "        else:\n",
    "            if (loss <= self.best_loss) and (acc >= self.best_acc):\n",
    "                self.save_checkpoint(model)\n",
    "            self.best_loss = np.min((loss, self.best_loss))\n",
    "            self.best_acc = np.max((acc, self.best_acc))\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "## 保存当前模型的状态字典到文件中，以便后续可以加载该模型。文件名包含了时间戳信息。\n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        torch.save(model.state_dict(), self.filename)\n",
    "## 加载最近保存的模型的状态字典，以便恢复模型的权重和参数\n",
    "    def load_checkpoint(self, model):\n",
    "        \"\"\"Load the latest checkpoint.\"\"\"\n",
    "        model.load_state_dict(torch.load(self.filename))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.531925400Z",
     "start_time": "2023-11-08T08:07:01.507472500Z"
    }
   },
   "id": "96cdbaf6d4cb4528"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "stopper = EarlyStopping(patience=args.patience)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.540375500Z",
     "start_time": "2023-11-08T08:07:01.518985400Z"
    }
   },
   "id": "3aea6da9298620c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 损失函数设定"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d623e88243355e40"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.548332900Z",
     "start_time": "2023-11-08T08:07:01.534395600Z"
    }
   },
   "id": "3412698b5d75a703"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 优化器设定"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bcceff488ee5f29"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:01.582320300Z",
     "start_time": "2023-11-08T08:07:01.548332900Z"
    }
   },
   "id": "5a0460d4c1b3b3a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 得分函数设定"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7360c39fe2156340"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def score(logits, labels):\n",
    "##从模型的预测logits中找到每个样本的最大值及其对应的索引。\n",
    "    _, indices = torch.max(logits, dim=1)\n",
    "##将预测的类别索引从PyTorch张量转换为NumPy数组\n",
    "    prediction = indices.long().cpu().numpy()\n",
    "##将真实的标签从PyTorch张量转换为NumPy数组\n",
    "    labels = labels.cpu().numpy()\n",
    "## 计算分类准确率\n",
    "    accuracy = (prediction == labels).sum() / len(prediction)\n",
    "## 计算微平均F1分数\n",
    "    micro_f1 = f1_score(labels, prediction, average=\"micro\")\n",
    "## 宏平均F1分数\n",
    "    macro_f1 = f1_score(labels, prediction, average=\"macro\")\n",
    "\n",
    "    return accuracy, micro_f1, macro_f1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:02.233457100Z",
     "start_time": "2023-11-08T08:07:01.566331Z"
    }
   },
   "id": "e02f5323e34f60d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 评估函数设定"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2effc7a224956d92"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model,\n",
    "    g,\n",
    "    metapath_list,\n",
    "    num_neighbors,\n",
    "    features,\n",
    "    labels,\n",
    "    val_nid,\n",
    "    loss_fcn,\n",
    "    batch_size,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    han_valid_sampler = HANSampler(\n",
    "        g, metapath_list, num_neighbors=num_neighbors * 2\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset=val_nid,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=han_valid_sampler.sample_blocks,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    correct = total = 0\n",
    "## 这些列表用于存储每个数据批次的预测和真实标签。\n",
    "    prediction_list = []\n",
    "    labels_list = []\n",
    "    ##设置非梯度计算\n",
    "    with torch.no_grad():\n",
    "        for step, (seeds, blocks) in enumerate(dataloader):\n",
    "        ## 采样子图中节点的特征张量。\n",
    "            h_list = load_subtensors(blocks, features)\n",
    "            blocks = [block.to(args.device) for block in blocks]\n",
    "            hs = [h.to(args.device) for h in h_list]\n",
    "        \n",
    "            logits = model(blocks, hs)\n",
    "            loss = loss_fcn(\n",
    "                logits, labels[numpy.asarray(seeds)].to(args.device)\n",
    "            )\n",
    "            # get each predict label\n",
    "            _, indices = torch.max(logits, dim=1)\n",
    "            prediction = indices.long().cpu().numpy()\n",
    "            labels_batch = labels[numpy.asarray(seeds)].cpu().numpy()\n",
    "\n",
    "            prediction_list.append(prediction)\n",
    "            labels_list.append(labels_batch)\n",
    "\n",
    "            correct += (prediction == labels_batch).sum()\n",
    "            total += prediction.shape[0]\n",
    "\n",
    "    total_prediction = numpy.concatenate(prediction_list)\n",
    "    total_labels = numpy.concatenate(labels_list)\n",
    "    micro_f1 = f1_score(total_labels, total_prediction, average=\"micro\")\n",
    "    macro_f1 = f1_score(total_labels, total_prediction, average=\"macro\")\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return loss, accuracy, micro_f1, macro_f1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:02.249920200Z",
     "start_time": "2023-11-08T08:07:02.233457100Z"
    }
   },
   "id": "716b0ba47ffa3bc7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练过程和测试过程"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa342eed5fa15eef"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 1.0723 | train_acc: 0.4062 | train_micro_f1: 0.4062 | train_macro_f1: 0.3571\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 1.1265 | train_acc: 0.3438 | train_micro_f1: 0.3438 | train_macro_f1: 0.2667\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 1.0040 | train_acc: 0.6875 | train_micro_f1: 0.6875 | train_macro_f1: 0.3526\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 1.1134 | train_acc: 0.3750 | train_micro_f1: 0.3750 | train_macro_f1: 0.1905\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.9674 | train_acc: 0.5000 | train_micro_f1: 0.5000 | train_macro_f1: 0.2222\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 1.0071 | train_acc: 0.5000 | train_micro_f1: 0.5000 | train_macro_f1: 0.2222\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.9478 | train_acc: 0.5938 | train_micro_f1: 0.5938 | train_macro_f1: 0.3401\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.9524 | train_acc: 0.5938 | train_micro_f1: 0.5938 | train_macro_f1: 0.2484\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 1.0383 | train_acc: 0.4062 | train_micro_f1: 0.4062 | train_macro_f1: 0.1926\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 1.0566 | train_acc: 0.3750 | train_micro_f1: 0.3750 | train_macro_f1: 0.1818\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.9676 | train_acc: 0.4688 | train_micro_f1: 0.4688 | train_macro_f1: 0.2128\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 1.0081 | train_acc: 0.4375 | train_micro_f1: 0.4375 | train_macro_f1: 0.2029\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.9646 | train_acc: 0.5000 | train_micro_f1: 0.5000 | train_macro_f1: 0.2222\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.9148 | train_acc: 0.5938 | train_micro_f1: 0.5938 | train_macro_f1: 0.2484\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.8875 | train_acc: 0.5312 | train_micro_f1: 0.5312 | train_macro_f1: 0.2313\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.9065 | train_acc: 0.5312 | train_micro_f1: 0.5312 | train_macro_f1: 0.2313\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 1.0671 | train_acc: 0.3438 | train_micro_f1: 0.3438 | train_macro_f1: 0.1705\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.9047 | train_acc: 0.5625 | train_micro_f1: 0.5625 | train_macro_f1: 0.3028\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.9295 | train_acc: 0.4688 | train_micro_f1: 0.4688 | train_macro_f1: 0.2128\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.8622 | train_acc: 0.5625 | train_micro_f1: 0.5625 | train_macro_f1: 0.2400\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.8074 | train_acc: 0.6250 | train_micro_f1: 0.6250 | train_macro_f1: 0.3200\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.8492 | train_acc: 0.5938 | train_micro_f1: 0.5938 | train_macro_f1: 0.4377\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.8168 | train_acc: 0.5938 | train_micro_f1: 0.5938 | train_macro_f1: 0.4733\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.8415 | train_acc: 0.6250 | train_micro_f1: 0.6250 | train_macro_f1: 0.5381\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 1 | loss: 0.8423 | train_acc: 0.7188 | train_micro_f1: 0.7188 | train_macro_f1: 0.5444\n",
      "torch.Size([8, 2, 64])\n",
      "Epoch 1 | loss: 0.8921 | train_acc: 0.5000 | train_micro_f1: 0.5000 | train_macro_f1: 0.5000\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([17, 2, 64])\n",
      "Epoch 1 | Val loss 1.1438 | Val Accuracy 0.7082 | Val Micro f1 0.7082 | Val Macro f1 0.5694\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.8081 | train_acc: 0.7188 | train_micro_f1: 0.7188 | train_macro_f1: 0.6345\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.7875 | train_acc: 0.7500 | train_micro_f1: 0.7500 | train_macro_f1: 0.6866\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.8046 | train_acc: 0.7812 | train_micro_f1: 0.7812 | train_macro_f1: 0.5803\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.7549 | train_acc: 0.7812 | train_micro_f1: 0.7812 | train_macro_f1: 0.5902\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.8040 | train_acc: 0.7188 | train_micro_f1: 0.7188 | train_macro_f1: 0.7378\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.7595 | train_acc: 0.7812 | train_micro_f1: 0.7812 | train_macro_f1: 0.5968\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.7620 | train_acc: 0.7188 | train_micro_f1: 0.7188 | train_macro_f1: 0.5769\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.7638 | train_acc: 0.7188 | train_micro_f1: 0.7188 | train_macro_f1: 0.7268\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.6735 | train_acc: 0.8750 | train_micro_f1: 0.8750 | train_macro_f1: 0.8016\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.7405 | train_acc: 0.7812 | train_micro_f1: 0.7812 | train_macro_f1: 0.7623\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.7684 | train_acc: 0.6875 | train_micro_f1: 0.6875 | train_macro_f1: 0.6693\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.6863 | train_acc: 0.8750 | train_micro_f1: 0.8750 | train_macro_f1: 0.8315\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.6393 | train_acc: 0.8750 | train_micro_f1: 0.8750 | train_macro_f1: 0.8364\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.5796 | train_acc: 0.9062 | train_micro_f1: 0.9062 | train_macro_f1: 0.8928\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.7146 | train_acc: 0.6875 | train_micro_f1: 0.6875 | train_macro_f1: 0.6226\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.6697 | train_acc: 0.8750 | train_micro_f1: 0.8750 | train_macro_f1: 0.8016\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.6597 | train_acc: 0.7500 | train_micro_f1: 0.7500 | train_macro_f1: 0.4898\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.5816 | train_acc: 0.8750 | train_micro_f1: 0.8750 | train_macro_f1: 0.7490\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.6611 | train_acc: 0.8125 | train_micro_f1: 0.8125 | train_macro_f1: 0.7474\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.6168 | train_acc: 0.7812 | train_micro_f1: 0.7812 | train_macro_f1: 0.7154\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.7206 | train_acc: 0.6875 | train_micro_f1: 0.6875 | train_macro_f1: 0.5417\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.7114 | train_acc: 0.6875 | train_micro_f1: 0.6875 | train_macro_f1: 0.6639\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.5409 | train_acc: 0.8750 | train_micro_f1: 0.8750 | train_macro_f1: 0.7979\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.5662 | train_acc: 0.8125 | train_micro_f1: 0.8125 | train_macro_f1: 0.7721\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 2 | loss: 0.6078 | train_acc: 0.8438 | train_micro_f1: 0.8438 | train_macro_f1: 0.8349\n",
      "torch.Size([8, 2, 64])\n",
      "Epoch 2 | loss: 0.8005 | train_acc: 0.5000 | train_micro_f1: 0.5000 | train_macro_f1: 0.5333\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([32, 2, 64])\n",
      "torch.Size([17, 2, 64])\n",
      "Epoch 2 | Val loss 0.9346 | Val Accuracy 0.7855 | Val Micro f1 0.7855 | Val Macro f1 0.7221\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.5446 | train_acc: 0.8125 | train_micro_f1: 0.8125 | train_macro_f1: 0.8071\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.5457 | train_acc: 0.8125 | train_micro_f1: 0.8125 | train_macro_f1: 0.7934\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.5237 | train_acc: 0.8125 | train_micro_f1: 0.8125 | train_macro_f1: 0.6881\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.5301 | train_acc: 0.8125 | train_micro_f1: 0.8125 | train_macro_f1: 0.7736\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.5636 | train_acc: 0.8125 | train_micro_f1: 0.8125 | train_macro_f1: 0.8172\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.5194 | train_acc: 0.8125 | train_micro_f1: 0.8125 | train_macro_f1: 0.7340\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.4938 | train_acc: 0.9062 | train_micro_f1: 0.9062 | train_macro_f1: 0.8724\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.5397 | train_acc: 0.8125 | train_micro_f1: 0.8125 | train_macro_f1: 0.8278\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.5001 | train_acc: 0.8438 | train_micro_f1: 0.8438 | train_macro_f1: 0.8482\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.5087 | train_acc: 0.8438 | train_micro_f1: 0.8438 | train_macro_f1: 0.8078\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.4498 | train_acc: 0.9688 | train_micro_f1: 0.9688 | train_macro_f1: 0.9663\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.5323 | train_acc: 0.8125 | train_micro_f1: 0.8125 | train_macro_f1: 0.8175\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.5046 | train_acc: 0.8750 | train_micro_f1: 0.8750 | train_macro_f1: 0.8793\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.4518 | train_acc: 0.9688 | train_micro_f1: 0.9688 | train_macro_f1: 0.9548\n",
      "torch.Size([32, 2, 64])\n",
      "Epoch 3 | loss: 0.4436 | train_acc: 0.9062 | train_micro_f1: 0.9062 | train_macro_f1: 0.8945\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, (seeds, blocks) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[1;32m----> 5\u001B[0m     h_list \u001B[38;5;241m=\u001B[39m \u001B[43mload_subtensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblocks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m     blocks \u001B[38;5;241m=\u001B[39m [block\u001B[38;5;241m.\u001B[39mto(args\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m blocks]\n\u001B[0;32m      7\u001B[0m     hs \u001B[38;5;241m=\u001B[39m [h\u001B[38;5;241m.\u001B[39mto(args\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m h_list]\n",
      "Cell \u001B[1;32mIn[15], line 2\u001B[0m, in \u001B[0;36mload_subtensors\u001B[1;34m(blocks, features)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m## 提取子图block的节点特征\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_subtensors\u001B[39m(blocks, features):\n\u001B[0;32m      3\u001B[0m     h_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m blocks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "for epoch in range(args.num_epochs):\n",
    "    model.train()\n",
    "    for step, (seeds, blocks) in enumerate(dataloader):\n",
    "        h_list = load_subtensors(blocks, features)\n",
    "        blocks = [block.to(args.device) for block in blocks]\n",
    "        hs = [h.to(args.device) for h in h_list]\n",
    "\n",
    "        logits = model(blocks, hs)\n",
    "        loss = loss_fn(\n",
    "            logits, labels[numpy.asarray(seeds)].to(args.device)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print info in each batch\n",
    "        train_acc, train_micro_f1, train_macro_f1 = score(\n",
    "            logits, labels[numpy.asarray(seeds)]\n",
    "        )\n",
    "        print(\n",
    "            \"Epoch {:d} | loss: {:.4f} | train_acc: {:.4f} | train_micro_f1: {:.4f} | train_macro_f1: {:.4f}\".format(\n",
    "                epoch + 1, loss, train_acc, train_micro_f1, train_macro_f1\n",
    "            )\n",
    "        )\n",
    "    val_loss, val_acc, val_micro_f1, val_macro_f1 = evaluate(\n",
    "        model,\n",
    "        g,\n",
    "        metapath_list,\n",
    "        num_neighbors,\n",
    "        features,\n",
    "        labels,\n",
    "        val_nid,\n",
    "        loss_fn,\n",
    "        args.batch_size,\n",
    "    )\n",
    "    early_stop = stopper.step(val_loss.data.item(), val_acc, model)\n",
    "\n",
    "    print(\n",
    "        \"Epoch {:d} | Val loss {:.4f} | Val Accuracy {:.4f} | Val Micro f1 {:.4f} | Val Macro f1 {:.4f}\".format(\n",
    "            epoch + 1, val_loss.item(), val_acc, val_micro_f1, val_macro_f1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if early_stop:\n",
    "        break\n",
    "\n",
    "stopper.load_checkpoint(model)\n",
    "test_loss, test_acc, test_micro_f1, test_macro_f1 = evaluate(\n",
    "    model,\n",
    "    g,\n",
    "    metapath_list,\n",
    "    num_neighbors,\n",
    "    features,\n",
    "    labels,\n",
    "    test_nid,\n",
    "    loss_fn,\n",
    "    args.batch_size,\n",
    ")\n",
    "print(\n",
    "    \"Test loss {:.4f} | Test Accuracy {:.4f} | Test Micro f1 {:.4f} | Test Macro f1 {:.4f}\".format(\n",
    "        test_loss.item(), test_acc, test_micro_f1, test_macro_f1\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T08:07:06.721694200Z",
     "start_time": "2023-11-08T08:07:02.252455200Z"
    }
   },
   "id": "d8f6a8126e6ec4c3"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7bf637f1a833bf4e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
