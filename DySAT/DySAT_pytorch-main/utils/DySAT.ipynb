{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1.设定形参"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5aba74e86caf8538"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:47.555271600Z",
     "start_time": "2023-11-25T03:53:47.446658400Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--time_steps', type=int, nargs='?', default=16,\n",
    "                        help=\"total time steps used for train, eval and test\")\n",
    "    # Experimental settings.\n",
    "parser.add_argument('--dataset', type=str, nargs='?', default='Enron',\n",
    "                        help='dataset name')\n",
    "\n",
    "parser.add_argument('--epochs', type=int, nargs='?', default=200,\n",
    "                        help='# epochs')\n",
    "parser.add_argument('--val_freq', type=int, nargs='?', default=1,\n",
    "                        help='Validation frequency (in epochs)')\n",
    "parser.add_argument('--test_freq', type=int, nargs='?', default=1,\n",
    "                        help='Testing frequency (in epochs)')\n",
    "parser.add_argument('--batch_size', type=int, nargs='?', default=512,\n",
    "                        help='Batch size (# nodes)')\n",
    "parser.add_argument('--featureless', type=bool, nargs='?', default=True,\n",
    "                    help='True if one-hot encoding.')\n",
    "parser.add_argument(\"--early_stop\", type=int, default=10,\n",
    "                        help=\"patient\")\n",
    "    # 1-hot encoding is input as a sparse matrix - hence no scalability issue for large datasets.\n",
    "    # Tunable hyper-params\n",
    "    # TODO: Implementation has not been verified, performance may not be good.\n",
    "parser.add_argument('--residual', type=bool, nargs='?', default=True,\n",
    "                        help='Use residual')\n",
    "    # Number of negative samples per positive pair.\n",
    "parser.add_argument('--neg_sample_size', type=int, nargs='?', default=10,\n",
    "                        help='# negative samples per positive')\n",
    "    # Walk length for random walk sampling.\n",
    "parser.add_argument('--walk_len', type=int, nargs='?', default=20,\n",
    "                        help='Walk length for random walk sampling')\n",
    "    # Weight for negative samples in the binary cross-entropy loss function.\n",
    "parser.add_argument('--neg_weight', type=float, nargs='?', default=1.0,\n",
    "                        help='Weightage for negative samples')\n",
    "parser.add_argument('--learning_rate', type=float, nargs='?', default=0.01,\n",
    "                        help='Initial learning rate for self-attention model.')\n",
    "parser.add_argument('--spatial_drop', type=float, nargs='?', default=0.1,\n",
    "                        help='Spatial (structural) attention Dropout (1 - keep probability).')\n",
    "parser.add_argument('--temporal_drop', type=float, nargs='?', default=0.5,\n",
    "                        help='Temporal attention Dropout (1 - keep probability).')\n",
    "parser.add_argument('--weight_decay', type=float, nargs='?', default=0.0005,\n",
    "                        help='Initial learning rate for self-attention model.')\n",
    "    # Architecture params\n",
    "parser.add_argument('--structural_head_config', type=str, nargs='?', default='16,8,8',\n",
    "                        help='Encoder layer config: # attention heads in each GAT layer')\n",
    "parser.add_argument('--structural_layer_config', type=str, nargs='?', default='128',\n",
    "                        help='Encoder layer config: # units in each GAT layer')\n",
    "parser.add_argument('--temporal_head_config', type=str, nargs='?', default='16',\n",
    "                        help='Encoder layer config: # attention heads in each Temporal layer')\n",
    "parser.add_argument('--temporal_layer_config', type=str, nargs='?', default='128',\n",
    "                        help='Encoder layer config: # units in each Temporal layer')\n",
    "parser.add_argument('--position_ffn', type=str, nargs='?', default='True',\n",
    "                        help='Position wise feedforward')\n",
    "parser.add_argument('--window', type=int, nargs='?', default=-1,\n",
    "                        help='Window for temporal attention (default : -1 => full)')\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.导入数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e20b97f815b31cb"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "def load_graphs(dataset_str):\n",
    "    with open(\"../data/{}/{}\".format(dataset_str, \"graph.pkl\"), \"rb\") as f:\n",
    "        graphs = pkl.load(f)\n",
    "    print(\"Loaded {} graphs \".format(len(graphs)))\n",
    "    adjs = [nx.adjacency_matrix(g) for g in graphs]\n",
    "    return graphs, adjs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:47.677039800Z",
     "start_time": "2023-11-25T03:53:47.460546900Z"
    }
   },
   "id": "1c1197b48c333e76"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 graphs \n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Zhou\\AppData\\Local\\Temp\\ipykernel_19000\\904226592.py:5: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  graphs = pkl.load(f)\n"
     ]
    }
   ],
   "source": [
    "graphs, adjs = load_graphs(args.dataset)\n",
    "print(len(graphs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:47.758430100Z",
     "start_time": "2023-11-25T03:53:47.474637100Z"
    }
   },
   "id": "e65979daa2cb68e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 没有节点特征的话使用独热编码创建特征"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e65715ba3492568f"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个图，包含18个节点,节点特征维度143\n",
      "第2个图，包含23个节点,节点特征维度143\n",
      "第3个图，包含24个节点,节点特征维度143\n",
      "第4个图，包含50个节点,节点特征维度143\n",
      "第5个图，包含66个节点,节点特征维度143\n",
      "第6个图，包含79个节点,节点特征维度143\n",
      "第7个图，包含98个节点,节点特征维度143\n",
      "第8个图，包含110个节点,节点特征维度143\n",
      "第9个图，包含117个节点,节点特征维度143\n",
      "第10个图，包含125个节点,节点特征维度143\n",
      "第11个图，包含131个节点,节点特征维度143\n",
      "第12个图，包含135个节点,节点特征维度143\n",
      "第13个图，包含137个节点,节点特征维度143\n",
      "第14个图，包含138个节点,节点特征维度143\n",
      "第15个图，包含141个节点,节点特征维度143\n",
      "第16个图，包含143个节点,节点特征维度143\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "if args.featureless == True:\n",
    "##创建单位阵 ## 最后一个时间点包括的节点数量并对其进行独热编码从而得到节点特征矩阵\n",
    "    feats = [scipy.sparse.identity(adjs[args.time_steps - 1].shape[0]).tocsr()[range(0, x.shape[0]), :] for x in adjs if\n",
    "            x.shape[0] <= adjs[args.time_steps - 1].shape[0]]\n",
    "\n",
    "##查看每个图的节点信息和节点特征维度\n",
    "for x in range(len(feats)):\n",
    "    print(f'第{len(feats[:x+1])}个图，包含{feats[x].shape[0]}个节点,节点特征维度{feats[x].shape[1]}')\n",
    "\n",
    "assert args.time_steps <= len(adjs), \"Time steps is illegal\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:47.758430100Z",
     "start_time": "2023-11-25T03:53:47.522491500Z"
    }
   },
   "id": "9725293ced7d1671"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 在每个图上进行随机游走"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bd51bb317943023"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def get_context_pairs(graphs, adjs):\n",
    "   ##遍历每个时间步骤的图\n",
    "    context_pairs_train = []\n",
    "   ## 遍历所有时间快照生成的图，从而每个静态图进行随机游走采样4-1 context_pairs_train\n",
    "    for i in range(len(graphs)):\n",
    "        context_pairs_train.append(run_random_walks_n2v(graphs[i], adjs[i], num_walks=10, walk_len=20))\n",
    "\n",
    "    return context_pairs_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:47.759393700Z",
     "start_time": "2023-11-25T03:53:47.535829100Z"
    }
   },
   "id": "2e0d394155ae8524"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4-1 run_random_walks_n2v"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43e1155513ecc81a"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "##4-1 随机游走函数\n",
    "def run_random_walks_n2v(graph, adj, num_walks, walk_len):\n",
    "    ## 建立一个空图\n",
    "    nx_G = nx.Graph()\n",
    "    \n",
    "    ##加入的是有边连接的节点\n",
    "    for e in graph.edges():\n",
    "        nx_G.add_edge(e[0], e[1])\n",
    "        \n",
    "    ##将边的权重也加入也就是连接的次数    \n",
    "    for edge in graph.edges():\n",
    "        nx_G[edge[0]][edge[1]]['weight'] = adj[edge[0], edge[1]]\n",
    "\n",
    "    ## 实例化Graph_RandomWalk该类，在每张经过重构为nx_G的时间快照图上进行随机游走\n",
    "    G = Graph_RandomWalk(nx_G, False, 1.0, 1.0)\n",
    "    \n",
    "    ## 4-1-1 preprocess_transition_probs 随机游走概率计算 \n",
    "    G.preprocess_transition_probs()\n",
    "    \n",
    "    ## 4-1-2 simulate_walks 正式进行随机游走-得到随机采样序列 \n",
    "    walks = G.simulate_walks(num_walks, walk_len)\n",
    "\n",
    "## 4-1-3 通过滑动窗口生成节点对\n",
    "    WINDOW_SIZE = 10\n",
    "    pairs = defaultdict(list)\n",
    "    pairs_cnt = 0\n",
    "    for walk in walks:\n",
    "        for word_index, word in enumerate(walk):\n",
    "            for nb_word in walk[max(word_index - WINDOW_SIZE, 0): min(word_index + WINDOW_SIZE, len(walk)) + 1]:\n",
    "                if nb_word != word:\n",
    "                    pairs[word].append(nb_word)\n",
    "                    pairs_cnt += 1\n",
    "    print(\"# nodes with random walk samples: {}\".format(len(pairs)))\n",
    "    print(\"# sampled pairs: {}\".format(pairs_cnt))\n",
    "    return pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:47.769027400Z",
     "start_time": "2023-11-25T03:53:47.554273500Z"
    }
   },
   "id": "e2db281a4f13b0df"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "## 定义图上的随机游走\n",
    "class Graph_RandomWalk():\n",
    "    def __init__(self, nx_G, is_directed, p, q):\n",
    "        self.G = nx_G\n",
    "        self.is_directed = is_directed\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "\n",
    "    ##4-1-1 preprocess_transition_probs 随机游走概率计算    \n",
    "    def preprocess_transition_probs(self):\n",
    "        G = self.G\n",
    "        is_directed = self.is_directed\n",
    "        alias_nodes = {}\n",
    "    ##遍历图中所有节点\n",
    "        for node in G.nodes():\n",
    "    ##求邻居节点所对应的边的权重\n",
    "            unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
    "    ##将权重进行归一化得出不同邻居节点被采样的概率\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "    ##4-1-1-1 alias_setup使用alias方法进行采样\n",
    "            alias_nodes[node] = alias_setup(normalized_probs)\n",
    "        alias_edges = {}\n",
    "        triads = {}\n",
    "    ##4-1-1-2 get_alias_edge根据是否是有向图来进行边的构建（这里是false）\n",
    "        if is_directed:\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "        else:\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "                alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
    "        ##节点到下一个节点的概率\n",
    "        self.alias_nodes = alias_nodes\n",
    "        ## node2vec采样概率\n",
    "        self.alias_edges = alias_edges\n",
    "        return\n",
    "    \n",
    "    \n",
    "    ## 4-1-2 simulate_walks 正式进行随机游走-得到随机采样序列 \n",
    "    def simulate_walks(self, num_walks, walk_length):\n",
    "        '''\n",
    "        Repeatedly simulate random walks from each node.\n",
    "        '''\n",
    "        G = self.G\n",
    "        walks = []\n",
    "        nodes = list(G.nodes())\n",
    "        ##打乱节点，次数为随机游走的次数\n",
    "        for walk_iter in range(num_walks):\n",
    "            random.shuffle(nodes)\n",
    "        ##遍历节点的每次随机游走路径\n",
    "            for node in nodes:\n",
    "        ## 4-1-2-1 随机游走策略 node2vec_walk\n",
    "                walks.append(self.node2vec_walk(walk_length=walk_length, start_node=node))\n",
    "\n",
    "        return walks\n",
    "    \n",
    "    ## 4-1-2-1  单个节点进行随机游走生成walk \n",
    "    def node2vec_walk(self, walk_length, start_node):\n",
    "        '''\n",
    "        Simulate a random walk starting from start node.\n",
    "        '''\n",
    "        G = self.G\n",
    "        alias_nodes = self.alias_nodes\n",
    "        alias_edges = self.alias_edges\n",
    "\n",
    "        walk = [start_node]\n",
    "        \n",
    "        ##游走长度20\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = sorted(G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                ## 初始节点\n",
    "                if len(walk) == 1:\n",
    "                    ##alias_draw \n",
    "                    walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                \n",
    "                ##之后的节点\n",
    "                else:\n",
    "                    prev = walk[-2]\n",
    "                    ## node2vec采样\n",
    "                    next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0],\n",
    "                        alias_edges[(prev, cur)][1])]\n",
    "                    walk.append(next)\n",
    "            else:\n",
    "                break\n",
    "        return walk\n",
    "    \n",
    "\n",
    "##4-1-1-2 get_alias_edge\n",
    "    def get_alias_edge(self, src, dst):\n",
    "        '''\n",
    "        Get the alias edge setup lists for a given edge.\n",
    "        '''\n",
    "        G = self.G\n",
    "        p = self.p\n",
    "        q = self.q\n",
    "\n",
    "        unnormalized_probs = []\n",
    "        \n",
    "    ##遍历终止节点的邻居节点\n",
    "        for dst_nbr in sorted(G.neighbors(dst)):\n",
    "        ##如果终止节点的邻居节点等于初始节点-往回走\n",
    "            if dst_nbr == src:\n",
    "            ##其概率等于其权重乘1/p\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
    "        ##如果终止节点的邻居节点有1个边连接\n",
    "            elif G.has_edge(dst_nbr, src):\n",
    "            ##其概率为权重乘1\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
    "        ##如果是初始节点的多阶邻居节点\n",
    "            else:\n",
    "            ##其概率等于其权重乘1/q\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
    "        ## 对其进行归一化        \n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "        \n",
    "        ##alias_setup 使用alias采样方式\n",
    "        return alias_setup(normalized_probs)\n",
    "\n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:47.769027400Z",
     "start_time": "2023-11-25T03:53:47.567424400Z"
    }
   },
   "id": "8af08a74b5e13351"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "## 4-1-1-1  alias_setup 构建alia的随机采样策略\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "def alias_setup(probs):\n",
    "    ##多少个邻居节点可选择\n",
    "    K = len(probs)\n",
    "    ##每个邻居节点被直接抽中的概率\n",
    "    q = np.zeros(K)\n",
    "    ##为那些原本概率不足以直接被抽中的结果提供了一个替代邻居节点\n",
    "    J = np.zeros(K, dtype=int)\n",
    "    smaller = []\n",
    "    larger = []\n",
    "    ##kk为邻居节点，prob为被选择的原始概率面积\n",
    "    for kk, prob in enumerate(probs):\n",
    "     ##扩大关系：原始概率面积乘K\n",
    "        q[kk] = K*prob\n",
    "        ##判断大于1即需要切割用larger去收集，小于1需要去填补的用smaller去收集。\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)  \n",
    "    ## 循环执行，直到 smaller 或 larger 其中一个为空\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        ## 从 smaller 中弹出一个元素，称为 small。\n",
    "        small = smaller.pop()\n",
    "        ## 从 larger 中弹出一个元素，称为 large。\n",
    "        large = larger.pop()\n",
    "        ## 这建立了 small 的“别名”。‘别名’可以简单理解为small选不到后的备选选项。\n",
    "        J[small] = large\n",
    "        ## 更新 q[large]，并根据其新值重新分配到 smaller 或 larger\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "    return J, q\n",
    "\n",
    "## 4-1-2-1-1 alias_draw alias采样概率\n",
    "def alias_draw(J, q):\n",
    "    '''\n",
    "    Draw sample from a non-uniform discrete distribution using alias sampling.\n",
    "    '''\n",
    "    ## 邻居节点个数\n",
    "    K = len(J)\n",
    "    ## 1~K采样 \n",
    "    kk = int(np.floor(np.random.rand()*K))\n",
    "    ##0~1生成随机数\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:47.769027400Z",
     "start_time": "2023-11-25T03:53:47.582614400Z"
    }
   },
   "id": "42176ea48605a2aa"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes with random walk samples: 18\n",
      "# sampled pairs: 40894\n",
      "# nodes with random walk samples: 18\n",
      "# sampled pairs: 39116\n",
      "# nodes with random walk samples: 14\n",
      "# sampled pairs: 31992\n",
      "# nodes with random walk samples: 47\n",
      "# sampled pairs: 103876\n",
      "# nodes with random walk samples: 57\n",
      "# sampled pairs: 129048\n",
      "# nodes with random walk samples: 65\n",
      "# sampled pairs: 149074\n",
      "# nodes with random walk samples: 79\n",
      "# sampled pairs: 194682\n",
      "# nodes with random walk samples: 97\n",
      "# sampled pairs: 240298\n",
      "# nodes with random walk samples: 101\n",
      "# sampled pairs: 246332\n",
      "# nodes with random walk samples: 106\n",
      "# sampled pairs: 256254\n",
      "# nodes with random walk samples: 103\n",
      "# sampled pairs: 253480\n",
      "# nodes with random walk samples: 113\n",
      "# sampled pairs: 280986\n",
      "# nodes with random walk samples: 98\n",
      "# sampled pairs: 232264\n",
      "# nodes with random walk samples: 79\n",
      "# sampled pairs: 181354\n",
      "# nodes with random walk samples: 94\n",
      "# sampled pairs: 233352\n",
      "# nodes with random walk samples: 93\n",
      "# sampled pairs: 227608\n"
     ]
    }
   ],
   "source": [
    "## 实操获得随机游走的上下文节点\n",
    "context_pairs_train = get_context_pairs(graphs, adjs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:48.872366400Z",
     "start_time": "2023-11-25T03:53:47.599346600Z"
    }
   },
   "id": "9b49b3ff9eca2c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 任务：预测最后一张图"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "519650efd590a7fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. 获取用于预测的数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49095b3d4be729b1"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def get_evaluation_data(graphs):\n",
    "##获取倒数第二张图\n",
    "    eval_idx = len(graphs) - 2\n",
    "## 训练语料\n",
    "    eval_graph = graphs[eval_idx]\n",
    "## 测试集定为下一张图\n",
    "    next_graph = graphs[eval_idx+1]\n",
    "\n",
    "    train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false = \\\n",
    "        create_data_splits(eval_graph, next_graph, val_mask_fraction=0.2, \n",
    "                            test_mask_fraction=0.6)\n",
    "\n",
    "    return train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:48.887648400Z",
     "start_time": "2023-11-25T03:53:48.873391600Z"
    }
   },
   "id": "df5e921364b674a8"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "## 划分数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_data_splits(graph, next_graph, val_mask_fraction=0.2, test_mask_fraction=0.6):\n",
    "    \n",
    "##已numpyarray的方式得到下一张图的所有边\n",
    "    edges_next = np.array(list(nx.Graph(next_graph).edges()))\n",
    "## 得出下一张图和本张图正关系的边\n",
    "    edges_positive = [] \n",
    "##遍历下一张图的边\n",
    "    for e in edges_next:\n",
    "    ##如果是本张图包含下张图边连接的两个节点，将该边添加到正关系边中\n",
    "        if graph.has_node(e[0]) and graph.has_node(e[1]):\n",
    "            edges_positive.append(e)\n",
    "    edges_positive = np.array(edges_positive) \n",
    "##进行负采样\n",
    "    edges_negative = negative_sample(edges_positive, graph.number_of_nodes(), next_graph)\n",
    "    \n",
    "## 将正采样边和负采样边 划分训练集，测试集，验证集\n",
    "    train_edges_pos, test_pos, train_edges_neg, test_neg = train_test_split(edges_positive, \n",
    "            edges_negative, test_size=val_mask_fraction+test_mask_fraction)\n",
    "\n",
    "    val_edges_pos, test_edges_pos, val_edges_neg, test_edges_neg = train_test_split(test_pos, \n",
    "            test_neg, test_size=test_mask_fraction/(test_mask_fraction+val_mask_fraction))\n",
    "\n",
    "    return train_edges_pos, train_edges_neg, val_edges_pos, val_edges_neg, test_edges_pos, test_edges_neg\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:48.914321800Z",
     "start_time": "2023-11-25T03:53:48.888649600Z"
    }
   },
   "id": "5bba1f9c121e0949"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "##负采样\n",
    "def negative_sample(edges_pos, nodes_num, next_graph):\n",
    "    edges_neg = []\n",
    "    ##采样和正关系的边相同数量的负关系的边\n",
    "    while len(edges_neg) < len(edges_pos):\n",
    "       ##随机选取节点\n",
    "        idx_i = np.random.randint(0, nodes_num)\n",
    "        idx_j = np.random.randint(0, nodes_num)\n",
    "        ##自连接不加入\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        ##采样到下一张图的正关系边也不加入\n",
    "        if next_graph.has_edge(idx_i, idx_j) or next_graph.has_edge(idx_j, idx_i):\n",
    "            continue\n",
    "        ## 重复出现的负采样的边也不加入\n",
    "        if edges_neg:\n",
    "            if [idx_i, idx_j] in edges_neg or [idx_j, idx_i] in edges_neg:\n",
    "                continue\n",
    "        edges_neg.append([idx_i, idx_j])\n",
    "    return edges_neg"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:48.930760Z",
     "start_time": "2023-11-25T03:53:48.905633200Z"
    }
   },
   "id": "589bb4059ac51994"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. Train: Pos=46, Neg=46 \n",
      "No. Val: Pos=46, Neg=46 \n",
      "No. Test: Pos=140, Neg=140\n"
     ]
    }
   ],
   "source": [
    "## 分割出训练集，测试集，验证集\n",
    "train_edges_pos, train_edges_neg, val_edges_pos, val_edges_neg, \\\n",
    "    test_edges_pos, test_edges_neg = get_evaluation_data(graphs)\n",
    "print(\"No. Train: Pos={}, Neg={} \\nNo. Val: Pos={}, Neg={} \\nNo. Test: Pos={}, Neg={}\".format(\n",
    "len(train_edges_pos), len(train_edges_neg), len(val_edges_pos), len(val_edges_neg),\n",
    "len(test_edges_pos), len(test_edges_neg)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:48.944823200Z",
     "start_time": "2023-11-25T03:53:48.920237100Z"
    }
   },
   "id": "328f240570a74540"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. 建立归纳推导图"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c73686db5e3e8f77"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "## 建立归纳推导图，将former图的边和later图的节点加入到归纳推导图中\n",
    "def inductive_graph(graph_former, graph_later):\n",
    "    newG = nx.MultiGraph()\n",
    "    newG.add_nodes_from(graph_later.nodes(data=True))\n",
    "    newG.add_edges_from(graph_former.edges(data=False))\n",
    "    return newG"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:48.979190800Z",
     "start_time": "2023-11-25T03:53:48.935801400Z"
    }
   },
   "id": "b2c97fce957b90b2"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "## 将最后一张图替换为归纳推导图\n",
    "new_G = inductive_graph(graphs[args.time_steps-2], graphs[args.time_steps-1])\n",
    "graphs[args.time_steps-1] = new_G\n",
    "adjs[args.time_steps-1] = nx.adjacency_matrix(new_G)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:48.990061300Z",
     "start_time": "2023-11-25T03:53:48.950803Z"
    }
   },
   "id": "25e395bfc68b7948"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. 建立数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fc632e0bcd3871d"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from utilities import fixed_unigram_candidate_sampler\n",
    "import torch_geometric as tg\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, args, graphs, features, adjs,  context_pairs):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.args = args\n",
    "        ## 所有时刻的图\n",
    "        self.graphs = graphs\n",
    "        ## 7-1 _preprocess_features特征预处理—— 16个图和16个图的图特征-进行归一化操作\n",
    "        self.features = [self._preprocess_features(feat) for feat in features]\n",
    "        ## 7-2 _normalize_graph_gcn标准化邻接矩阵邻接矩阵归一化\n",
    "        self.adjs = [self._normalize_graph_gcn(a)  for a  in adjs]\n",
    "        ##时间步\n",
    "        self.time_steps = args.time_steps\n",
    "        ##随机游走序列\n",
    "        self.context_pairs = context_pairs\n",
    "        ##负采样数量\n",
    "        self.max_positive = args.neg_sample_size\n",
    "        ##提取最后一个时间快照下的所有节点作为训练节点\n",
    "        self.train_nodes = list(self.graphs[self.time_steps-1].nodes()) \n",
    "        ##最小时间步\n",
    "        self.min_t = max(self.time_steps - self.args.window - 1, 0) if args.window > 0 else 0\n",
    "        ##7-3 construct_degs计算每个时间步中节点的度\n",
    "        self.degs = self.construct_degs()\n",
    "        ##7-4 _build_pyg_graphs  定义data loader\n",
    "        self.pyg_graphs = self._build_pyg_graphs()\n",
    "        ##7-5 __createitems__ 创建训练语料\n",
    "        self.__createitems__()\n",
    "\n",
    "## 7-1 _preprocess_features 特征归一化        \n",
    "    def _preprocess_features(self, features):\n",
    "        features = np.array(features.todense())\n",
    "        ## 按行求和\n",
    "        rowsum = np.array(features.sum(1))\n",
    "        ## 特征和的倒数\n",
    "        r_inv = np.power(rowsum, -1).flatten()\n",
    "        ## 无穷值赋值为0\n",
    "        r_inv[np.isinf(r_inv)] = 0.\n",
    "        ## 转化为对角阵\n",
    "        r_mat_inv = sp.diags(r_inv)\n",
    "        features = r_mat_inv.dot(features)\n",
    "        return features\n",
    "    \n",
    "## 7-2 _normalize_graph_gcn 对邻接矩阵进行归一化\n",
    "    def _normalize_graph_gcn(self, adj):\n",
    "        adj = sp.coo_matrix(adj, dtype=np.float32)\n",
    "        ##添加自连接的边\n",
    "        adj_ = adj + sp.eye(adj.shape[0], dtype=np.float32)\n",
    "        ## 计算节点的度\n",
    "        rowsum = np.array(adj_.sum(1), dtype=np.float32)\n",
    "        ## 计算度矩阵的逆平方根\n",
    "        degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten(), dtype=np.float32)\n",
    "        ## 自连接邻接矩阵和度矩阵的逆平方根相乘，再乘度矩阵逆平方根的转置\n",
    "        adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "        return adj_normalized\n",
    "    \n",
    "## 7-3 construct_degs计算每个时间步中节点的度 \n",
    "    def construct_degs(self):\n",
    "        degs = []\n",
    "        for i in range(self.min_t, self.time_steps):\n",
    "            G = self.graphs[i]\n",
    "            deg = []\n",
    "            for nodeid in G.nodes():\n",
    "                deg.append(G.degree(nodeid))\n",
    "            degs.append(deg)\n",
    "        return degs\n",
    "    \n",
    "## 7-4 _build_pyg_graphs 建立pyg图\n",
    "    def _build_pyg_graphs(self):\n",
    "        pyg_graphs = []\n",
    "    ##特征和邻接矩阵\n",
    "        for feat, adj in zip(self.features, self.adjs):\n",
    "            x = torch.Tensor(feat)\n",
    "    ##将稀疏矩阵转化为index和value的形式\n",
    "            edge_index, edge_weight = tg.utils.from_scipy_sparse_matrix(adj)\n",
    "            data = Data(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
    "            pyg_graphs.append(data)\n",
    "        return pyg_graphs\n",
    "\n",
    "    \n",
    "##7-5 __createitems__ 创建训练语料\n",
    "    def __createitems__(self):\n",
    "        self.data_items = {}\n",
    "    ##遍历最后一个时间快照中的所有节点\n",
    "        for node in list(self.graphs[self.time_steps-1].nodes()):\n",
    "            feed_dict = {}\n",
    "            node_1_all_time = []\n",
    "            node_2_all_time = []\n",
    "            ##遍历所有时间快照\n",
    "            for t in range(self.min_t, self.time_steps):\n",
    "                node_1 = []\n",
    "                node_2 = []\n",
    "            ##如果当前节点在这个时间步骤的上下文节点数量超过了设定的最大正样本数量\n",
    "                if len(self.context_pairs[t][node]) > self.max_positive:\n",
    "            ##随机选择10个训练语料\n",
    "                    node_1.extend([node]* self.max_positive)\n",
    "                    node_2.extend(np.random.choice(self.context_pairs[t][node], self.max_positive, replace=False))\n",
    "            ##否则，使用所有上下文节点\n",
    "                else:\n",
    "                    node_1.extend([node]* len(self.context_pairs[t][node]))\n",
    "                    node_2.extend(self.context_pairs[t][node])\n",
    "                assert len(node_1) == len(node_2)\n",
    "            ##得到node_1_all_time所对应的上下文节点node_2_all_time\n",
    "                node_1_all_time.append(node_1)\n",
    "                node_2_all_time.append(node_2)\n",
    "            node_1_list = [torch.LongTensor(node) for node in node_1_all_time]\n",
    "            node_2_list = [torch.LongTensor(node) for node in node_2_all_time] \n",
    "    ##进行负采样\n",
    "            node_2_negative = []\n",
    "            for t in range(len(node_2_list)):\n",
    "            ##得到每个上下文节点的度\n",
    "                degree = self.degs[t]\n",
    "            ##正向关系\n",
    "                node_positive = node_2_list[t][:, None]\n",
    "    \n",
    "            ##7-5-1根据负采样函数fixed_unigram_candidate_sampler\n",
    "                node_negative =fixed_unigram_candidate_sampler(true_clasees=node_positive,num_true=1,num_sampled=self.args.neg_sample_size,unique=False,distortion=0.75,unigrams=degree)\n",
    "                node_2_negative.append(node_negative)\n",
    "            node_2_neg_list = [torch.LongTensor(node) for node in node_2_negative]\n",
    "            ##中心节点\n",
    "            feed_dict['node_1']=node_1_list\n",
    "            ##中心节点的上下文正样本节点\n",
    "            feed_dict['node_2']=node_2_list\n",
    "            ##中心节点的上下文负样本节点\n",
    "            feed_dict['node_2_neg']=node_2_neg_list\n",
    "            ##图信息\n",
    "            feed_dict[\"graphs\"] = self.pyg_graphs\n",
    "            ##节点所对应的节点\n",
    "            self.data_items[node] = feed_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(samples):\n",
    "        batch_dict = {}\n",
    "        for key in [\"node_1\", \"node_2\", \"node_2_neg\"]:\n",
    "        #node_1节点本身，node_2节点对应的10个pos节点，node_2_neg:节点对应到的[10,10]的负采样节点\n",
    "            data_list = []\n",
    "        \n",
    "        ## 每个节点的所有信息\n",
    "            for sample in samples:\n",
    "                data_list.append(sample[key])\n",
    "        \n",
    "        ## 按照时间步涉及到的节点\n",
    "            concate = []\n",
    "            ## 遍历每个时间步\n",
    "            for t in range(len(data_list[0])):\n",
    "            ##对所有节点，都选择t这个时间步中的节点信息\n",
    "                concate.append(torch.cat([data[t] for data in data_list]))\n",
    "            ##key下所有时间涉及到的节点\n",
    "            batch_dict[key] = concate\n",
    "        batch_dict[\"graphs\"] = samples[0][\"graphs\"]\n",
    "        ##每个类别下，所有时间中涉及到的节点（16个时间步，每个时间步中的节点flatten\n",
    "        return batch_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_nodes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        node = self.train_nodes[index]\n",
    "        return self.data_items[node]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:48.991065900Z",
     "start_time": "2023-11-25T03:53:48.965665Z"
    }
   },
   "id": "13ad33b67e372f81"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "## 7-5-1 负采样函数\n",
    "import copy\n",
    "def fixed_unigram_candidate_sampler(true_clasees, \n",
    "                                    num_true, \n",
    "                                    num_sampled, \n",
    "                                    unique,  \n",
    "                                    distortion, \n",
    "                                    unigrams):\n",
    "    # TODO: implementate distortion to unigrams\n",
    "    assert true_clasees.shape[1] == num_true\n",
    "    samples = []\n",
    "    ## 遍历正样本节点\n",
    "    for i in range(true_clasees.shape[0]):\n",
    "        ##节点的degree\n",
    "        dist = copy.deepcopy(unigrams)\n",
    "        ##得到候补节点\n",
    "        candidate = list(range(len(dist)))\n",
    "        ##获得正样本节点\n",
    "        taboo = true_clasees[i].cpu().tolist()\n",
    "        for tabo in sorted(taboo, reverse=True):\n",
    "            ##剔除正样本节点\n",
    "            candidate.remove(tabo)\n",
    "            ##剔除正样本节点的degree\n",
    "            dist.pop(tabo)\n",
    "        ##每个正样本采样十个负样本\n",
    "        sample = np.random.choice(candidate, size=num_sampled, replace=unique, p=dist/np.sum(dist))\n",
    "        samples.append(sample)\n",
    "    return samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:49.003972800Z",
     "start_time": "2023-11-25T03:53:48.982358700Z"
    }
   },
   "id": "e723c3b2ea46b53a"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "## 实际操作\n",
    "device = torch.device(\"cpu\")\n",
    "dataset = MyDataset(args, graphs, feats, adjs, context_pairs_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:50.541474600Z",
     "start_time": "2023-11-25T03:53:48.996973600Z"
    }
   },
   "id": "6aabdb6f843494c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义数据加载器-dataloader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f52fcee6c5839955"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset, \n",
    "                            batch_size=args.batch_size, \n",
    "                            shuffle=True, \n",
    "                            num_workers=0, \n",
    "                            collate_fn=MyDataset.collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:50.556809100Z",
     "start_time": "2023-11-25T03:53:50.541474600Z"
    }
   },
   "id": "9ebba2d6569e67f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义DySAT模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a1e8c6dc4d3693c"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "\n",
    "class DySAT(nn.Module):\n",
    "    def __init__(self, args, num_features, time_length):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            args ([type]): [description]\n",
    "            time_length (int): Total timesteps in dataset.\n",
    "        \"\"\"\n",
    "        super(DySAT, self).__init__()\n",
    "        self.args = args\n",
    "        if args.window < 0:\n",
    "            self.num_time_steps = time_length\n",
    "        else:\n",
    "            self.num_time_steps = min(time_length, args.window + 1)  # window = 0 => only self.\n",
    "        self.num_features = num_features\n",
    "    ## 结构多头数量信息\n",
    "        self.structural_head_config = list(map(int, args.structural_head_config.split(\",\")))\n",
    "    ## 结构layer层信息\n",
    "        self.structural_layer_config = list(map(int, args.structural_layer_config.split(\",\")))\n",
    "    ## 时序多头数量信息\n",
    "        self.temporal_head_config = list(map(int, args.temporal_head_config.split(\",\")))\n",
    "    ## 时序layer层信息 \n",
    "        self.temporal_layer_config = list(map(int, args.temporal_layer_config.split(\",\")))\n",
    "    ##dropout的定义\n",
    "        self.spatial_drop = args.spatial_drop\n",
    "        self.temporal_drop = args.temporal_drop\n",
    "    ## 定义mould\n",
    "        self.structural_attn, self.temporal_attn = self.build_model()\n",
    "    ##定义loss函数：sigmoid和crossentropy\n",
    "        self.bceloss = BCEWithLogitsLoss()\n",
    "\n",
    "## 8-2模型定义函数\n",
    "    def build_model(self):\n",
    "        input_dim = self.num_features\n",
    "\n",
    "# 8-2-1: Structural Attention Layers\n",
    "    ##结构层信息\n",
    "        structural_attention_layers = nn.Sequential()\n",
    "        for i in range(len(self.structural_layer_config)):\n",
    "            layer = StructuralAttentionLayer(input_dim=input_dim,\n",
    "                                             output_dim=self.structural_layer_config[i],##output维度\n",
    "                                             n_heads=self.structural_head_config[i],##多头个数\n",
    "                                             attn_drop=self.spatial_drop,##drop参数\n",
    "                                             ffd_drop=self.spatial_drop,\n",
    "                                             residual=self.args.residual)##残差连接\n",
    "            structural_attention_layers.add_module(name=\"structural_layer_{}\".format(i), module=layer)\n",
    "            ##下一层的input等于上一层的输出维度\n",
    "            input_dim = self.structural_layer_config[i]\n",
    "# 8-2-2: Temporal Attention Layers\n",
    "    ## 时序层信息\n",
    "        input_dim = self.structural_layer_config[-1]\n",
    "        temporal_attention_layers = nn.Sequential()\n",
    "        for i in range(len(self.temporal_layer_config)):\n",
    "            layer = TemporalAttentionLayer(input_dim=input_dim,##输入维度\n",
    "                                           n_heads=self.temporal_head_config[i],##多头数量\n",
    "                                           num_time_steps=self.num_time_steps,##时间维度\n",
    "                                           attn_drop=self.temporal_drop,##dropout\n",
    "                                           residual=self.args.residual)##残差连接\n",
    "            temporal_attention_layers.add_module(name=\"temporal_layer_{}\".format(i), module=layer)\n",
    "            input_dim = self.temporal_layer_config[i]\n",
    "        return structural_attention_layers, temporal_attention_layers\n",
    "    \n",
    "    def forward(self, graphs):\n",
    "    # Structural Attention forward-结构注意力机制\n",
    "        structural_out = []\n",
    "        ## 遍历每一个时间步\n",
    "        for t in range(0, self.num_time_steps):\n",
    "            structural_out.append(self.structural_attn(graphs[t]))\n",
    "        ##节点聚合邻居后的特征\n",
    "        structural_outputs = [g.x[:,None,:] for g in structural_out] # list of [Ni, 1, F]\n",
    "\n",
    "    # padding outputs along with Ni\n",
    "        ##获取节点数量\n",
    "        maximum_node_num = structural_outputs[-1].shape[0]\n",
    "        ## 输出特征数量\n",
    "        out_dim = structural_outputs[-1].shape[-1]\n",
    "        structural_outputs_padded = []\n",
    "        ##对节点进行补0，使其为同一维度即每个图都有143（全部）个节点\n",
    "        for out in structural_outputs:\n",
    "            zero_padding = torch.zeros(maximum_node_num-out.shape[0], 1, out_dim).to(out.device)\n",
    "            padded = torch.cat((out, zero_padding), dim=0)\n",
    "            structural_outputs_padded.append(padded)\n",
    "        ##将16个时刻的拼接再一起；structural最终输出的节点特征\n",
    "        structural_outputs_padded = torch.cat(structural_outputs_padded, dim=1) # [N, T, F]\n",
    "        \n",
    "    # Temporal Attention forward-时序注意力机制\n",
    "        temporal_out = self.temporal_attn(structural_outputs_padded)\n",
    "        \n",
    "        return temporal_out\n",
    "\n",
    "    \n",
    "\n",
    "    def get_loss(self, feed_dict):\n",
    "        node_1, node_2, node_2_negative, graphs = feed_dict.values()\n",
    "        # run gnn\n",
    "        final_emb = self.forward(graphs) # [N, T, F]\n",
    "        self.graph_loss = 0\n",
    "    ##遍历每个时间步骤\n",
    "        for t in range(self.num_time_steps - 1):\n",
    "        ##这一刻时间所对应所有节点的embedding\n",
    "            emb_t = final_emb[:, t, :].squeeze() #[N, F]\n",
    "        ##本身节点\n",
    "            source_node_emb = emb_t[node_1[t]]\n",
    "        ##上下文节点-正采样的\n",
    "            tart_node_pos_emb = emb_t[node_2[t]]\n",
    "        ##负采样节点\n",
    "            tart_node_neg_emb = emb_t[node_2_negative[t]]\n",
    "        ##进行本身节点和上下文的正采样节点求内积\n",
    "            pos_score = torch.sum(source_node_emb*tart_node_pos_emb, dim=1)\n",
    "        ##负采样节点进行内积\n",
    "            neg_score = -torch.sum(source_node_emb[:, None, :]*tart_node_neg_emb, dim=2).flatten()\n",
    "        ##利用定义的损失函数\n",
    "            pos_loss = self.bceloss(pos_score, torch.ones_like(pos_score))\n",
    "            neg_loss = self.bceloss(neg_score, torch.ones_like(neg_score))\n",
    "            graphloss = pos_loss + self.args.neg_weight*neg_loss\n",
    "            self.graph_loss += graphloss\n",
    "        return self.graph_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:50.665840100Z",
     "start_time": "2023-11-25T03:53:50.645531700Z"
    }
   },
   "id": "3718018966f601bb"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "## 结构注意力层\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_scatter import scatter\n",
    "\n",
    "class StructuralAttentionLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim, \n",
    "                output_dim, \n",
    "                n_heads, \n",
    "                attn_drop, \n",
    "                ffd_drop,\n",
    "                residual):\n",
    "        super(StructuralAttentionLayer, self).__init__()\n",
    "    ##out_dim每个头特征的维度\n",
    "        self.out_dim = output_dim // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.act = nn.ELU()\n",
    "        \n",
    "    ##定义线性层[143,128]-143为节点特征维度，128为多头个数（16）乘每个头的特征维度(8)\n",
    "        self.lin = nn.Linear(input_dim, n_heads * self.out_dim, bias=False)\n",
    "        \n",
    "    ##att_l初始参数是否需要好的优化shape为（1，16，8）\n",
    "        self.att_l = nn.Parameter(torch.Tensor(1, n_heads, self.out_dim))\n",
    "        self.att_r = nn.Parameter(torch.Tensor(1, n_heads, self.out_dim))\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "    ##定义drop out\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.ffd_drop = nn.Dropout(ffd_drop)\n",
    "        \n",
    "    ##定义残差\n",
    "        self.residual = residual\n",
    "        if self.residual:\n",
    "            self.lin_residual = nn.Linear(input_dim, n_heads * self.out_dim, bias=False)\n",
    "\n",
    "        self.xavier_init()\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph = copy.deepcopy(graph)\n",
    "        ##点边关系\n",
    "        edge_index = graph.edge_index\n",
    "        ##边权重\n",
    "        edge_weight = graph.edge_weight.reshape(-1, 1)\n",
    "        H, C = self.n_heads, self.out_dim\n",
    "        x = self.lin(graph.x).view(-1, H, C) # [N, heads, out_dim]\n",
    "    # attention\n",
    "        ##初始节点的attention值\n",
    "        alpha_l = (x * self.att_l).sum(dim=-1).squeeze() # [N, heads]\n",
    "        ##终止节点的attention值\n",
    "        alpha_r = (x * self.att_r).sum(dim=-1).squeeze()\n",
    "        ## 获得每条边初始节点的注意力得分——这里其实是进行了一种映射关系，其实可以理解edge_index是从节点关系到边关系的一种映射。\n",
    "        alpha_l = alpha_l[edge_index[0]] # [num_edges, heads]\n",
    "        ##获得每条边的终止节点的注意力得分\n",
    "        alpha_r = alpha_r[edge_index[1]]\n",
    "        ##将初始节点和终止节点的attention拼接一起构成边的注意力得分\n",
    "        alpha = alpha_r + alpha_l\n",
    "        ##将alpha乘一个边权重\n",
    "        alpha = edge_weight * alpha\n",
    "        alpha = self.leaky_relu(alpha)  \n",
    "        ##用softmax对同一个注意力头对应的不同相连的邻居节点进行归一化\n",
    "        coefficients = softmax(alpha, edge_index[1]) # [num_edges, heads]\n",
    "    # dropout\n",
    "        if self.training:\n",
    "            coefficients = self.attn_drop(coefficients)\n",
    "            x = self.ffd_drop(x)\n",
    "        ##定义初始节点特征\n",
    "        x_j = x[edge_index[0]]  # [num_edges, heads, out_dim]\n",
    "    # output\n",
    "        ##利用scatter将同种index的值进行结合（原本是num_edges,直接变成num_nodes）\n",
    "        out = self.act(scatter(x_j * coefficients[:, :, None], edge_index[1], dim=0, reduce=\"sum\"))\n",
    "        ## 将多头的数量和每个头的维度进行拼接\n",
    "        out = out.reshape(-1, self.n_heads*self.out_dim) #[num_nodes, output_dim]\n",
    "        ##进行残差操作\n",
    "        if self.residual:\n",
    "            out = out + self.lin_residual(graph.x)\n",
    "        ##计算后的attention节点特征赋予到图上\n",
    "        graph.x = out\n",
    "        return graph\n",
    "\n",
    "    def xavier_init(self):\n",
    "        nn.init.xavier_uniform_(self.att_l)\n",
    "        nn.init.xavier_uniform_(self.att_r)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:50.682799Z",
     "start_time": "2023-11-25T03:53:50.665840100Z"
    }
   },
   "id": "adb768294b36f949"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "## 时序注意力层\n",
    "class TemporalAttentionLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim, \n",
    "                n_heads, \n",
    "                num_time_steps, \n",
    "                attn_drop, \n",
    "                residual):\n",
    "        super(TemporalAttentionLayer, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.residual = residual\n",
    "\n",
    "    \n",
    "    ## 位置embedding信息-以图为单位\n",
    "        self.position_embeddings = nn.Parameter(torch.Tensor(num_time_steps, input_dim))\n",
    "    ## 定义QKV三个向量   \n",
    "        self.Q_embedding_weights = nn.Parameter(torch.Tensor(input_dim, input_dim))\n",
    "        self.K_embedding_weights = nn.Parameter(torch.Tensor(input_dim, input_dim))\n",
    "        self.V_embedding_weights = nn.Parameter(torch.Tensor(input_dim, input_dim))\n",
    "        # ff\n",
    "        self.lin = nn.Linear(input_dim, input_dim, bias=True)\n",
    "        # dropout \n",
    "        self.attn_dp = nn.Dropout(attn_drop)\n",
    "        self.xavier_init()\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"In:  attn_outputs (of StructuralAttentionLayer at each snapshot):= [N, T, F]\"\"\"\n",
    "    # 1: Add position embeddings to input\n",
    "        ##构建position embedding 最终shape为 (N,T)\n",
    "        position_inputs = torch.arange(0,self.num_time_steps).reshape(1, -1).repeat(inputs.shape[0], 1).long().to(inputs.device)\n",
    "        ##将结构的attention输出结果和position embedding相加\n",
    "        temporal_inputs = inputs + self.position_embeddings[position_inputs] # [N, T, F]\n",
    "\n",
    "    # 2: Query, Key based multi-head self attention.\n",
    "        ##第一个矩阵的第三个维度 乘 第二个矩阵的第1个维度，添加时间上的自注意力机制\n",
    "        q = torch.tensordot(temporal_inputs, self.Q_embedding_weights, dims=([2],[0])) # [N, T, F]\n",
    "        k = torch.tensordot(temporal_inputs, self.K_embedding_weights, dims=([2],[0])) # [N, T, F]\n",
    "        v = torch.tensordot(temporal_inputs, self.V_embedding_weights, dims=([2],[0])) # [N, T, F]\n",
    "\n",
    "    # 3: Split, concat and scale.\n",
    "        ##将第三维按照注意力机制头的数量进行切分 \n",
    "        ## [143,16,128]=>[143,16,16*8]\n",
    "        split_size = int(q.shape[-1]/self.n_heads)\n",
    "        \n",
    "        ## 将每个头单独计算\n",
    "        ##[143,16,128]=>[2288,16,8]\n",
    "        q_ = torch.cat(torch.split(q, split_size_or_sections=split_size, dim=2), dim=0) # [hN, T, F/h]\n",
    "        k_ = torch.cat(torch.split(k, split_size_or_sections=split_size, dim=2), dim=0) # [hN, T, F/h]\n",
    "        v_ = torch.cat(torch.split(v, split_size_or_sections=split_size, dim=2), dim=0) # [hN, T, F/h]\n",
    "        \n",
    "        ##将Q和K内积\n",
    "        outputs = torch.matmul(q_, k_.permute(0,2,1)) # [hN, T, T]\n",
    "        ## scale 操作\n",
    "        outputs = outputs / (self.num_time_steps ** 0.5)\n",
    "        \n",
    "    # 4: Masked (causal) softmax to compute attention weights.\n",
    "        ## 建立全为1的时间步矩阵，这是一个三维的矩阵\n",
    "        diag_val = torch.ones_like(outputs[0])\n",
    "        ## 建立下三角阵从而为聚合不同时间步的信息提供依据，是在hN这个维度下对每个T，T矩阵建立他的下三角矩阵\n",
    "        tril = torch.tril(diag_val)\n",
    "        ## 构建mask\n",
    "        masks = tril[None, :, :].repeat(outputs.shape[0], 1, 1) # [h*N, T, T]\n",
    "        ## 构建负无穷的时间步矩阵 \n",
    "        padding = torch.ones_like(masks) * (-2**32+1)\n",
    "        ## 将mask等于0的替换乘负无穷\n",
    "        outputs = torch.where(masks==0, padding, outputs)\n",
    "\n",
    "        ##softmax操作得到不同时间步下的时间维度的注意力得分\n",
    "        outputs = F.softmax(outputs, dim=2)\n",
    "        self.attn_wts_all = outputs # [h*N, T, T]\n",
    "                \n",
    "    # 5: Dropout on attention weights.\n",
    "        ##将输出结果和V相乘-将注意力机制的头重新组合\n",
    "        if self.training:\n",
    "            outputs = self.attn_dp(outputs)\n",
    "        outputs = torch.matmul(outputs, v_)  # [hN, T, F/h]\n",
    "        outputs = torch.cat(torch.split(outputs, split_size_or_sections=int(outputs.shape[0]/self.n_heads), dim=0), dim=2) # [N, T, F]\n",
    "        \n",
    "    # 6: Feedforward and residual\n",
    "        ##引入残差\n",
    "        outputs = self.feedforward(outputs)\n",
    "        ##所有节点聚合时序self-attention后的节点embedding\n",
    "        if self.residual:\n",
    "            outputs = outputs + temporal_inputs\n",
    "        return outputs\n",
    "    ##经过线性层\n",
    "    def feedforward(self, inputs):\n",
    "        outputs = F.relu(self.lin(inputs))\n",
    "        return outputs + inputs\n",
    "\n",
    "\n",
    "    def xavier_init(self):\n",
    "        nn.init.xavier_uniform_(self.position_embeddings)\n",
    "        nn.init.xavier_uniform_(self.Q_embedding_weights)\n",
    "        nn.init.xavier_uniform_(self.K_embedding_weights)\n",
    "        nn.init.xavier_uniform_(self.V_embedding_weights)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:50.710340900Z",
     "start_time": "2023-11-25T03:53:50.689120Z"
    }
   },
   "id": "b791be69bfc1e806"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "model = DySAT(args, feats[0].shape[1], args.time_steps).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:50.724860700Z",
     "start_time": "2023-11-25T03:53:50.698214600Z"
    }
   },
   "id": "475146482be4a070"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义优化器"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb88df84c7bd651e"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:50.730840900Z",
     "start_time": "2023-11-25T03:53:50.713947500Z"
    }
   },
   "id": "90da577c193f2cff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 进行训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c98fba468b394881"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "##对使用设备进行定义\n",
    "def to_device(batch, device):\n",
    "    feed_dict = copy.deepcopy(batch)\n",
    "    node_1, node_2, node_2_negative, graphs = feed_dict.values()\n",
    "    # to device\n",
    "    feed_dict[\"node_1\"] = [x.to(device) for x in node_1]\n",
    "    feed_dict[\"node_2\"] = [x.to(device) for x in node_2]\n",
    "    feed_dict[\"node_2_neg\"] = [x.to(device) for x in node_2_negative]\n",
    "    feed_dict[\"graphs\"] = [g.to(device) for g in graphs]\n",
    "\n",
    "    return feed_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:50.773475100Z",
     "start_time": "2023-11-25T03:53:50.729851600Z"
    }
   },
   "id": "4f7c237a42f933bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 进行之前提及的任务-已知倒数第二张图，求倒数第一张图的边关系"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "996293d3a0000e77"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "np.random.seed(123)\n",
    "operatorTypes = [\"HAD\"]\n",
    "\n",
    "def evaluate_classifier(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg, source_embeds, target_embeds):\n",
    "    \"\"\"Downstream logistic regression classifier to evaluate link prediction\"\"\"\n",
    "    test_results = defaultdict(lambda: [])\n",
    "    val_results = defaultdict(lambda: [])\n",
    "\n",
    "##得到roc的值\n",
    "    test_auc = get_roc_score_t(test_pos, test_neg, source_embeds, target_embeds)\n",
    "    val_auc = get_roc_score_t(val_pos, val_neg, source_embeds, target_embeds)\n",
    "    # Compute AUC based on sigmoid(u^T v) without classifier training.\n",
    "    test_results['SIGMOID'].extend([test_auc, test_auc])\n",
    "    val_results['SIGMOID'].extend([val_auc, val_auc])\n",
    "\n",
    "    test_pred_true = defaultdict(lambda: [])\n",
    "    val_pred_true = defaultdict(lambda: [])\n",
    "\n",
    "##获取边的特征\n",
    "    for operator in operatorTypes:\n",
    "        train_pos_feats = np.array(get_link_feats(train_pos, source_embeds, target_embeds, operator))\n",
    "        train_neg_feats = np.array(get_link_feats(train_neg, source_embeds, target_embeds, operator))\n",
    "        val_pos_feats = np.array(get_link_feats(val_pos, source_embeds, target_embeds, operator))\n",
    "        val_neg_feats = np.array(get_link_feats(val_neg, source_embeds, target_embeds, operator))\n",
    "        test_pos_feats = np.array(get_link_feats(test_pos, source_embeds, target_embeds, operator))\n",
    "        test_neg_feats = np.array(get_link_feats(test_neg, source_embeds, target_embeds, operator))\n",
    "        \n",
    "## 对label进行拼接\n",
    "        train_pos_labels = np.array([1] * len(train_pos_feats))\n",
    "        train_neg_labels = np.array([-1] * len(train_neg_feats))\n",
    "        val_pos_labels = np.array([1] * len(val_pos_feats))\n",
    "        val_neg_labels = np.array([-1] * len(val_neg_feats))\n",
    "\n",
    "        test_pos_labels = np.array([1] * len(test_pos_feats))\n",
    "        test_neg_labels = np.array([-1] * len(test_neg_feats))\n",
    "        train_data = np.vstack((train_pos_feats, train_neg_feats))\n",
    "        train_labels = np.append(train_pos_labels, train_neg_labels)\n",
    "\n",
    "        val_data = np.vstack((val_pos_feats, val_neg_feats))\n",
    "        val_labels = np.append(val_pos_labels, val_neg_labels)\n",
    "\n",
    "        test_data = np.vstack((test_pos_feats, test_neg_feats))\n",
    "        test_labels = np.append(test_pos_labels, test_neg_labels)\n",
    "        \n",
    "## 使用逻辑回归计算\n",
    "        logistic = linear_model.LogisticRegression()\n",
    "        logistic.fit(train_data, train_labels)\n",
    "        test_predict = logistic.predict_proba(test_data)[:, 1]\n",
    "        val_predict = logistic.predict_proba(val_data)[:, 1]\n",
    "\n",
    "        test_roc_score = roc_auc_score(test_labels, test_predict)\n",
    "        val_roc_score = roc_auc_score(val_labels, val_predict)\n",
    "\n",
    "        val_results[operator].extend([val_roc_score, val_roc_score])\n",
    "        test_results[operator].extend([test_roc_score, test_roc_score])\n",
    "\n",
    "        val_pred_true[operator].extend(zip(val_predict, val_labels))\n",
    "        test_pred_true[operator].extend(zip(test_predict, test_labels))\n",
    "\n",
    "    return val_results, test_results, val_pred_true, test_pred_true"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:50.775780100Z",
     "start_time": "2023-11-25T03:53:50.749879900Z"
    }
   },
   "id": "a96ce4d940009eea"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "## 计算roc值\n",
    "def get_roc_score_t(edges_pos, edges_neg, source_emb, target_emb):\n",
    "    \"\"\"Given test examples, edges_pos: +ve edges, edges_neg: -ve edges, return ROC scores for a given snapshot\"\"\"\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "##获取两两节点的关系\n",
    "    ##正向关系\n",
    "    adj_rec = np.dot(source_emb, target_emb.T)\n",
    "    pred = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        ##进行内积并且通过sigmoid函数\n",
    "        pred.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        ##标签为1\n",
    "        pos.append(1.0)\n",
    "    ##负向关系\n",
    "    pred_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        pred_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        ##标签为0\n",
    "        neg.append(0.0)\n",
    "        \n",
    "##拼接正向和负向\n",
    "    pred_all = np.hstack([pred, pred_neg])\n",
    "    labels_all = np.hstack([np.ones(len(pred)), np.zeros(len(pred_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, pred_all)\n",
    "    return roc_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:50.776782400Z",
     "start_time": "2023-11-25T03:53:50.760946600Z"
    }
   },
   "id": "fec94059ad549039"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "## 获取边特征的函数\n",
    "def get_link_feats(links, source_embeddings, target_embeddings, operator):\n",
    "    \"\"\"Compute link features for a list of pairs\"\"\"\n",
    "    features = []\n",
    "    for l in links:\n",
    "        a, b = l[0], l[1]\n",
    "        f = get_link_score(source_embeddings[a], target_embeddings[b], operator)\n",
    "        features.append(f)\n",
    "    return features\n",
    "\n",
    "def get_link_score(fu, fv, operator):\n",
    "    \"\"\"Given a pair of embeddings, compute link feature based on operator (such as Hadammad product, etc.)\"\"\"\n",
    "    fu = np.array(fu)\n",
    "    fv = np.array(fv)\n",
    "    ##将节点相乘作为边特征\n",
    "    if operator == \"HAD\":\n",
    "        return np.multiply(fu, fv)\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:50.791641700Z",
     "start_time": "2023-11-25T03:53:50.777772300Z"
    }
   },
   "id": "9f7320be7036e978"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  5.77it/s]\u001B[A\n",
      "  0%|          | 1/200 [00:00<00:45,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  ,  Loss = 40.665, Val AUC 0.565 Test AUC 0.679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  5.80it/s]\u001B[A\n",
      "  1%|          | 2/200 [00:00<00:45,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ,  Loss = 29.227, Val AUC 0.601 Test AUC 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.03it/s]\u001B[A\n",
      "  2%|▏         | 3/200 [00:00<00:44,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2  ,  Loss = 26.145, Val AUC 0.621 Test AUC 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.16it/s]\u001B[A\n",
      "  2%|▏         | 4/200 [00:00<00:43,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3  ,  Loss = 24.117, Val AUC 0.648 Test AUC 0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.12it/s]\u001B[A\n",
      "  2%|▎         | 5/200 [00:01<00:43,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4  ,  Loss = 22.806, Val AUC 0.719 Test AUC 0.837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.11it/s]\u001B[A\n",
      "  3%|▎         | 6/200 [00:01<00:42,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5  ,  Loss = 21.879, Val AUC 0.814 Test AUC 0.869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  5.90it/s]\u001B[A\n",
      "  4%|▎         | 7/200 [00:01<00:42,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6  ,  Loss = 20.975, Val AUC 0.860 Test AUC 0.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.00it/s]\u001B[A\n",
      "  4%|▍         | 8/200 [00:01<00:42,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7  ,  Loss = 20.244, Val AUC 0.882 Test AUC 0.881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  5.88it/s]\u001B[A\n",
      "  4%|▍         | 9/200 [00:02<00:42,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8  ,  Loss = 19.521, Val AUC 0.898 Test AUC 0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.07it/s]\u001B[A\n",
      "  5%|▌         | 10/200 [00:02<00:42,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9  ,  Loss = 18.924, Val AUC 0.902 Test AUC 0.881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  5.98it/s]\u001B[A\n",
      "  6%|▌         | 11/200 [00:02<00:42,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 ,  Loss = 18.405, Val AUC 0.907 Test AUC 0.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.09it/s]\u001B[A\n",
      "  6%|▌         | 12/200 [00:02<00:41,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 ,  Loss = 18.040, Val AUC 0.905 Test AUC 0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.00it/s]\u001B[A\n",
      "  6%|▋         | 13/200 [00:02<00:41,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 ,  Loss = 17.807, Val AUC 0.903 Test AUC 0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.04it/s]\u001B[A\n",
      "  7%|▋         | 14/200 [00:03<00:41,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 ,  Loss = 17.527, Val AUC 0.902 Test AUC 0.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  5.95it/s]\u001B[A\n",
      "  8%|▊         | 15/200 [00:03<00:41,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 ,  Loss = 17.394, Val AUC 0.900 Test AUC 0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.10it/s]\u001B[A\n",
      "  8%|▊         | 16/200 [00:03<00:40,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 ,  Loss = 17.190, Val AUC 0.900 Test AUC 0.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.09it/s]\u001B[A\n",
      "  8%|▊         | 17/200 [00:03<00:40,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 ,  Loss = 17.018, Val AUC 0.902 Test AUC 0.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.14it/s]\u001B[A\n",
      "  9%|▉         | 18/200 [00:04<00:40,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 ,  Loss = 16.859, Val AUC 0.903 Test AUC 0.909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  5.99it/s]\u001B[A\n",
      " 10%|▉         | 19/200 [00:04<00:39,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 ,  Loss = 16.723, Val AUC 0.900 Test AUC 0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.22it/s]\u001B[A\n",
      " 10%|█         | 20/200 [00:04<00:39,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 ,  Loss = 16.639, Val AUC 0.899 Test AUC 0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  6.10it/s]\u001B[A\n",
      " 10%|█         | 21/200 [00:04<00:39,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 ,  Loss = 16.548, Val AUC 0.903 Test AUC 0.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001B[A\n",
      "1it [00:00,  5.90it/s]\u001B[A\n",
      " 10%|█         | 21/200 [00:04<00:41,  4.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_epoch_val = 0\n",
    "patient = 0\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    for idx, feed_dict in tqdm(enumerate(dataloader)):\n",
    "            feed_dict = to_device(feed_dict, device)\n",
    "            opt.zero_grad()\n",
    "            loss = model.get_loss(feed_dict)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "    ## 进行训练\n",
    "    model.eval()\n",
    "    ##获取倒数第二层的节点embedding，并且训练最后一层的边连接情况\n",
    "    emb = model(feed_dict[\"graphs\"])[:, -2, :].detach().cpu().numpy()\n",
    "    val_results, test_results, _, _ = evaluate_classifier(train_edges_pos,\n",
    "                                                            train_edges_neg,\n",
    "                                                            val_edges_pos, \n",
    "                                                            val_edges_neg, \n",
    "                                                            test_edges_pos,\n",
    "                                                            test_edges_neg, \n",
    "                                                            emb, \n",
    "                                                            emb)\n",
    "    epoch_auc_val = val_results[\"HAD\"][1]\n",
    "    epoch_auc_test = test_results[\"HAD\"][1]\n",
    "\n",
    "    if epoch_auc_val > best_epoch_val:\n",
    "        best_epoch_val = epoch_auc_val\n",
    "        torch.save(model.state_dict(), \"../model_checkpoints/model.pt\")\n",
    "        patient = 0\n",
    "    else:\n",
    "        patient += 1\n",
    "        if patient > args.early_stop:\n",
    "            break\n",
    "    print(\"Epoch {:<3},  Loss = {:.3f}, Val AUC {:.3f} Test AUC {:.3f}\".format(epoch,np.mean(epoch_loss),epoch_auc_val, epoch_auc_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:53:55.694245700Z",
     "start_time": "2023-11-25T03:53:50.794842Z"
    }
   },
   "id": "2b75dd608a361980"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test AUC = 0.884\n"
     ]
    }
   ],
   "source": [
    "##进行测试\n",
    "model.load_state_dict(torch.load(\"../model_checkpoints/model.pt\"))\n",
    "model.eval()\n",
    "emb = model(feed_dict[\"graphs\"])[:, -2, :].detach().cpu().numpy()\n",
    "val_results, test_results, _, _ = evaluate_classifier(train_edges_pos,\n",
    "                                                        train_edges_neg,\n",
    "                                                        val_edges_pos, \n",
    "                                                        val_edges_neg, \n",
    "                                                        test_edges_pos,\n",
    "                                                        test_edges_neg, \n",
    "                                                        emb, \n",
    "                                                        emb)\n",
    "auc_val = val_results[\"HAD\"][1]\n",
    "auc_test = test_results[\"HAD\"][1]\n",
    "print(\"Best Test AUC = {:.3f}\".format(auc_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T03:54:02.778748700Z",
     "start_time": "2023-11-25T03:54:02.694734200Z"
    }
   },
   "id": "4e2211a35fd044f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
