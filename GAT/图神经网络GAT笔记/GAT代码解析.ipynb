{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## GAT\n",
    "1.解释：计算一个节点和其他与之相邻节点的注意力得分将该得分用于权重平均邻居节点的特征从而生成新的节点特征表示\n",
    "2.注意力得分：将输出后的节点进行拼接并与一个可学习的权重进行内积然后再使用LeakyReLU进行非线性结合\n",
    "3.GAT对比GCN的优势，GCN 假设所有邻居节点都是等价的（或者通过手动设置的边权重来进行区分），而GAT可以通过注意力得分从而使得依据邻接节点更新节点更加合理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36ad735789a06ef2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 初始设置"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80be1f82fa949500"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:37.175762Z",
     "start_time": "2023-10-25T03:43:37.075543900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x27dff944590>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
    "parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.')\n",
    "parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=10000, help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=8, help='Number of hidden units.')\n",
    "parser.add_argument('--nb_heads', type=int, default=8, help='Number of head attentions.')\n",
    "parser.add_argument('--dropout', type=float, default=0.6, help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "parser.add_argument('--patience', type=int, default=100, help='Patience')\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 导入数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71de975e41e5ae9"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## 导入one-hot编码函数\n",
    "def encode_onehot(labels):\n",
    "    # The classes must be sorted before encoding to enable static class encoding.\n",
    "    # In other words, make sure the first class always maps to index 0.\n",
    "    classes = sorted(list(set(labels)))\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:37.245749600Z",
     "start_time": "2023-10-25T03:43:37.089119500Z"
    }
   },
   "id": "711eb3d753d4b00d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-1 处理节点数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d466b8199e83d5bc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['31336' '0' '0' ... '0' '0' 'Neural_Networks']\n",
      " ['1061127' '0' '0' ... '0' '0' 'Rule_Learning']\n",
      " ['1106406' '0' '0' ... '0' '0' 'Reinforcement_Learning']\n",
      " ...\n",
      " ['1128978' '0' '0' ... '0' '0' 'Genetic_Algorithms']\n",
      " ['117328' '0' '0' ... '0' '0' 'Case_Based']\n",
      " ['24043' '0' '0' ... '0' '0' 'Neural_Networks']]\n"
     ]
    }
   ],
   "source": [
    "## 导入节点数据\n",
    "import scipy.sparse as sp\n",
    "idx_features_labels = np.genfromtxt(\"C:\\\\Users\\\\Wei Zhou\\\\Desktop\\\\test\\\\图神经网络几个算法\\\\cora.content\", dtype=np.dtype(str))\n",
    "print(idx_features_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:38.815866100Z",
     "start_time": "2023-10-25T03:43:37.095778300Z"
    }
   },
   "id": "861243c07b73299a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## 构建节点特征的稀疏矩阵\n",
    "features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:39.886125100Z",
     "start_time": "2023-10-25T03:43:38.815866100Z"
    }
   },
   "id": "8f8b39f9c165d54a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "## 提取标签并进行one_hot编码\n",
    "labels = encode_onehot(idx_features_labels[:, -1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:39.895685400Z",
     "start_time": "2023-10-25T03:43:39.886125100Z"
    }
   },
   "id": "311dacb4634d69fa"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "## 提取索引并进行重新编码\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "idx_map = {j: i for i, j in enumerate(idx)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:39.905923500Z",
     "start_time": "2023-10-25T03:43:39.895685400Z"
    }
   },
   "id": "5c74e9a492f9e424"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-2处理边数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11151804a746980d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "## 导入边数据\n",
    "edges_unordered = np.genfromtxt(\"C:\\\\Users\\\\Wei Zhou\\\\Desktop\\\\test\\\\图神经网络几个算法\\\\cora.cites\", dtype=np.int32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:39.935906200Z",
     "start_time": "2023-10-25T03:43:39.905923500Z"
    }
   },
   "id": "342fb8fb3ad58cb"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 163  402]\n",
      " [ 163  659]\n",
      " [ 163 1696]\n",
      " ...\n",
      " [1887 2258]\n",
      " [1902 1887]\n",
      " [ 837 1686]]\n"
     ]
    }
   ],
   "source": [
    "## 利用之间的更新的节点标签更新边并以数组形式表示\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
    "print(edges)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:39.935906200Z",
     "start_time": "2023-10-25T03:43:39.926139300Z"
    }
   },
   "id": "48d16bd84b36be75"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "## 构建边的稀疏矩阵\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:39.945907Z",
     "start_time": "2023-10-25T03:43:39.935906200Z"
    }
   },
   "id": "3cc9c9d48836a4bd"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 将有向图转化为无向图\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:39.955930500Z",
     "start_time": "2023-10-25T03:43:39.945907Z"
    }
   },
   "id": "77c1cc102589ee02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-3 对节点特征和邻接矩阵进行标准化处理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94732475e93556b"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "##邻接矩阵的归一化方式\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "## 节点的归一化方式\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.015714900Z",
     "start_time": "2023-10-25T03:43:39.955930500Z"
    }
   },
   "id": "3715d8efa72c9dcb"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1426)\t0.05\n",
      "  (0, 1352)\t0.05\n",
      "  (0, 1236)\t0.05\n",
      "  (0, 1209)\t0.05\n",
      "  (0, 1205)\t0.05\n",
      "  (0, 902)\t0.05\n",
      "  (0, 845)\t0.05\n",
      "  (0, 734)\t0.05\n",
      "  (0, 702)\t0.05\n",
      "  (0, 698)\t0.05\n",
      "  (0, 648)\t0.05\n",
      "  (0, 619)\t0.05\n",
      "  (0, 521)\t0.05\n",
      "  (0, 507)\t0.05\n",
      "  (0, 456)\t0.05\n",
      "  (0, 351)\t0.05\n",
      "  (0, 252)\t0.05\n",
      "  (0, 176)\t0.05\n",
      "  (0, 125)\t0.05\n",
      "  (0, 118)\t0.05\n",
      "  (1, 1425)\t0.05882353\n",
      "  (1, 1389)\t0.05882353\n",
      "  (1, 1332)\t0.05882353\n",
      "  (1, 1266)\t0.05882353\n",
      "  (1, 1263)\t0.05882353\n",
      "  :\t:\n",
      "  (2706, 475)\t0.05263158\n",
      "  (2706, 287)\t0.05263158\n",
      "  (2706, 132)\t0.05263158\n",
      "  (2706, 54)\t0.05263158\n",
      "  (2706, 48)\t0.05263158\n",
      "  (2706, 4)\t0.05263158\n",
      "  (2707, 1351)\t0.05263158\n",
      "  (2707, 1335)\t0.05263158\n",
      "  (2707, 1333)\t0.05263158\n",
      "  (2707, 1301)\t0.05263158\n",
      "  (2707, 1205)\t0.05263158\n",
      "  (2707, 1203)\t0.05263158\n",
      "  (2707, 1178)\t0.05263158\n",
      "  (2707, 1156)\t0.05263158\n",
      "  (2707, 1075)\t0.05263158\n",
      "  (2707, 1073)\t0.05263158\n",
      "  (2707, 877)\t0.05263158\n",
      "  (2707, 774)\t0.05263158\n",
      "  (2707, 737)\t0.05263158\n",
      "  (2707, 564)\t0.05263158\n",
      "  (2707, 422)\t0.05263158\n",
      "  (2707, 304)\t0.05263158\n",
      "  (2707, 136)\t0.05263158\n",
      "  (2707, 67)\t0.05263158\n",
      "  (2707, 19)\t0.05263158\n",
      "  (0, 0)\t0.16666666666666666\n",
      "  (8, 0)\t0.16666666666666666\n",
      "  (14, 0)\t0.09128709291752768\n",
      "  (258, 0)\t0.11785113019775792\n",
      "  (435, 0)\t0.16666666666666666\n",
      "  (544, 0)\t0.18257418583505536\n",
      "  (1, 1)\t0.5000000000000001\n",
      "  (344, 1)\t0.12500000000000003\n",
      "  (2, 2)\t0.19999999999999998\n",
      "  (410, 2)\t0.18257418583505536\n",
      "  (471, 2)\t0.15811388300841897\n",
      "  (552, 2)\t0.06819943394704735\n",
      "  (565, 2)\t0.05031546054266276\n",
      "  (3, 3)\t0.25\n",
      "  (197, 3)\t0.22360679774997896\n",
      "  (463, 3)\t0.2041241452319315\n",
      "  (601, 3)\t0.2041241452319315\n",
      "  (4, 4)\t0.5000000000000001\n",
      "  (170, 4)\t0.408248290463863\n",
      "  (5, 5)\t0.3333333333333333\n",
      "  (490, 5)\t0.2041241452319315\n",
      "  (2164, 5)\t0.23570226039551584\n",
      "  (6, 6)\t0.3333333333333333\n",
      "  (251, 6)\t0.19245008972987523\n",
      "  (490, 6)\t0.2041241452319315\n",
      "  :\t:\n",
      "  (2701, 2701)\t0.3333333333333333\n",
      "  (881, 2702)\t0.14907119849998599\n",
      "  (2624, 2702)\t0.2041241452319315\n",
      "  (2702, 2702)\t0.3333333333333333\n",
      "  (1221, 2703)\t0.25\n",
      "  (1409, 2703)\t0.1767766952966369\n",
      "  (2200, 2703)\t0.22360679774997896\n",
      "  (2703, 2703)\t0.25\n",
      "  (209, 2704)\t0.28867513459481287\n",
      "  (2407, 2704)\t0.3333333333333333\n",
      "  (2704, 2704)\t0.3333333333333333\n",
      "  (1784, 2705)\t0.14907119849998596\n",
      "  (1839, 2705)\t0.22360679774997896\n",
      "  (1840, 2705)\t0.2581988897471611\n",
      "  (2216, 2705)\t0.22360679774997896\n",
      "  (2705, 2705)\t0.19999999999999998\n",
      "  (1046, 2706)\t0.18257418583505536\n",
      "  (1138, 2706)\t0.16903085094570333\n",
      "  (1640, 2706)\t0.16903085094570333\n",
      "  (1752, 2706)\t0.31622776601683794\n",
      "  (2706, 2706)\t0.19999999999999998\n",
      "  (774, 2707)\t0.25\n",
      "  (1389, 2707)\t0.22360679774997896\n",
      "  (2344, 2707)\t0.3535533905932738\n",
      "  (2707, 2707)\t0.25\n"
     ]
    }
   ],
   "source": [
    "## 执行归一化\n",
    "features = normalize_features(features)\n",
    "adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "print(features)\n",
    "print(adj)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.015714900Z",
     "start_time": "2023-10-25T03:43:39.965842Z"
    }
   },
   "id": "c7738ad523e58c83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-4 转化为tensor形式好进行输入"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44369536d768901f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "features = torch.FloatTensor(np.array(features.todense()))\n",
    "labels = torch.LongTensor(np.where(labels)[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.075809700Z",
     "start_time": "2023-10-25T03:43:39.996086400Z"
    }
   },
   "id": "69b3ac2439d07a3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2-5 划分数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6ea3bca91a6be3e"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "idx_train = range(140)\n",
    "idx_val = range(200, 500)\n",
    "idx_test = range(500, 1500)\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.075809700Z",
     "start_time": "2023-10-25T03:43:40.045939200Z"
    }
   },
   "id": "f0755b875041531a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 模型搭建"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2b18e7914f4d6a0"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.075809700Z",
     "start_time": "2023-10-25T03:43:40.055876100Z"
    }
   },
   "id": "6a3c1cd14e7b58b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 多头注意力机制\n",
    "1. 每个“头”（head）在多头注意力（Multi-Head Attention）中都有自己独立的注意力机制，包括自己的参数（如权重和偏置）。这意味着，如果你有 N 个头，你实际上有 N 套独立的注意力机制参数\n",
    "2. 多头注意力机制有点类似于卷积核"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e1757860249d283"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "##构建GAT模型\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.075809700Z",
     "start_time": "2023-10-25T03:43:40.065666700Z"
    }
   },
   "id": "b626f91783959451"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "## 构建GraphAttentionLayer\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "    ##防止过拟合\n",
    "        self.dropout = dropout\n",
    "    ##输入特征维度\n",
    "        self.in_features = in_features\n",
    "    ##输出特征维度\n",
    "        self.out_features = out_features\n",
    "    ##LeakyReLU激活函数的负斜率\n",
    "        self.alpha = alpha\n",
    "    ##是否在多个头之间进行链接\n",
    "        self.concat = concat\n",
    "    \n",
    "    ##节点输入维度到输出维度的一个学习参数\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "    ##进行参数的初始化\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        ##gain 是一个可选的缩放因子，用于调整初始化的范围。在这个例子中，gain 被设置为根号2 （大约是1.414）。这通常用于当激活函数是 ReLU 或者 ReLU 的变体（比如 LeakyReLU）时，以帮助保持方差。\n",
    "        \n",
    "    ##用于计算注意力机制的权重\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "    ##进行参数的初始化\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "    ##进行非线性处理\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "    ##对节点特征矩阵进行W的线性变换\n",
    "        Wh = torch.mm(h, self.W) \n",
    "    ##计算注意力机制的输入\n",
    "        e = self._prepare_attentional_mechanism_input(Wh)\n",
    "    ##以-9e15作为e的mask\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "    ##如果如果两个节点有边链接则使其注意力得分为e，如果没有则使用-9e15的mask作为其得分\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "    ##使用 softmax 函数对注意力得分进行归一化\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "    ##dropout，为模型的训练增加了一些正则化\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "    ##将注意力机制的矩阵和相邻的节点特征矩阵进行矩阵相乘\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "    ##将多头注意力得分进行拼接\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "        \n",
    "        \n",
    "        \n",
    "##节点间的原始注意力得分\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "    ##Wh和self.a的前一半进行矩阵乘法\n",
    "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
    "    ##Wh和self.a的后一半进行矩阵乘法\n",
    "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
    "    ##Wh1和Wh2的转置进行广播加法（broadcast add）操作，得到原始注意力得分矩阵 e。\n",
    "        e = Wh1 + Wh2.T\n",
    "    ##通过LeakyReLU激活函数对注意力得分进行非线性变换。\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.091054Z",
     "start_time": "2023-10-25T03:43:40.075809700Z"
    }
   },
   "id": "d673f565cd9dae9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SpGAT模型和GAT的区别\n",
    "1. GAT 依据节点的重要性来进行注意力的得分的打分。\n",
    "2. SPGAT 引入了引入了结构化的位置信息，会通过位置信息来调节注意力分数，链接模式和位置会被考虑\n",
    "3. GAT更灵活，可以处理各类型的图数据/SPGAT偏向于处理节点相对位置的问题"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23dccc0ffe58c2a7"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "##构建SpGAT模型\n",
    "class SpGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        super(SpGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [SpGraphAttentionLayer(nfeat, \n",
    "                                                 nhid, \n",
    "                                                 dropout=dropout, \n",
    "                                                 alpha=alpha, \n",
    "                                                 concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n",
    "                                             nclass, \n",
    "                                             dropout=dropout, \n",
    "                                             alpha=alpha, \n",
    "                                             concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.102637100Z",
     "start_time": "2023-10-25T03:43:40.086051400Z"
    }
   },
   "id": "3ec24a28f70f40e4"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "## 构建\n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()#进行稀疏矩阵和密集矩阵的乘法操作\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "    ## 获取输入矩阵的行数，即图中节点的数量。\n",
    "        N = input.size()[0]\n",
    "    ## 找出邻接矩阵中非零元素的索引，即存在边的节点对。\n",
    "        edge = adj.nonzero().t()\n",
    "    ## 通过与权重矩阵 self.W 相乘来变换节点特征\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "    ## 拼接每一条边两端节点的特征\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "    ## 计算注意力系数\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "    ##  对注意力系数进行归一化\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "    \n",
    "    ## 应用 Dropout\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "        \n",
    "    ## 使用注意力系数来聚合节点特征\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "    \n",
    "    ## 使用归一化的注意力系数进一步更新节点特征\n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.126074Z",
     "start_time": "2023-10-25T03:43:40.102637100Z"
    }
   },
   "id": "727215a878a5af2f"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.126074Z",
     "start_time": "2023-10-25T03:43:40.105796200Z"
    }
   },
   "id": "bc8505dc403d9fbd"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "    ##断言索引不需要梯度，即它们是常数。\n",
    "        assert indices.requires_grad == False\n",
    "    ##创建一个稀疏COO（Coordinate Format）张量\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "    ##保存输入的 a 和 b 以供 backward 方法使用。\n",
    "        ctx.save_for_backward(a, b)\n",
    "    ##保存输入稀疏矩阵的行数\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        ##从上下文中恢复保存的张量\n",
    "        a, b = ctx.saved_tensors\n",
    "        ## 初始化梯度值\n",
    "        grad_values = grad_b = None\n",
    "        ##检查是否需要计算 values 的梯度\n",
    "        if ctx.needs_input_grad[1]:\n",
    "        ##（反向传播的梯度）和 b 的转置的矩阵乘法\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "        ##计算边的索引\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "        ##使用计算出的索引提取梯度\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        ##检查是否需要计算 b 的梯度\n",
    "        if ctx.needs_input_grad[3]:\n",
    "        ##计算 a 的转置和 grad_output 的矩阵乘法\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.126074Z",
     "start_time": "2023-10-25T03:43:40.115981500Z"
    }
   },
   "id": "bd2f8546c517a02a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型选择"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24142d4bef169cb0"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "if args.sparse:\n",
    "    model = SpGAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "else:\n",
    "    model = GAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.weight_decay)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.141030600Z",
     "start_time": "2023-10-25T03:43:40.126074Z"
    }
   },
   "id": "c5c62315e9951b63"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 用gpu进行训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b730cc8c3b8ec459"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.215700900Z",
     "start_time": "2023-10-25T03:43:40.136027800Z"
    }
   },
   "id": "21a47be22c18be1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构建训练流程"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2df8afe1b7a30194"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import time\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.data.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.225880800Z",
     "start_time": "2023-10-25T03:43:40.215700900Z"
    }
   },
   "id": "4c09755ad4d5412e"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:43:40.235879900Z",
     "start_time": "2023-10-25T03:43:40.225880800Z"
    }
   },
   "id": "3db9da91cad57766"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 执行训练模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30f99c04061e441f"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9525 acc_train: 0.1500 loss_val: 1.9401 acc_val: 0.3033 time: 2.7198s\n",
      "Epoch: 0002 loss_train: 1.9417 acc_train: 0.2071 loss_val: 1.9312 acc_val: 0.4600 time: 0.0708s\n",
      "Epoch: 0003 loss_train: 1.9250 acc_train: 0.2714 loss_val: 1.9225 acc_val: 0.5533 time: 0.0701s\n",
      "Epoch: 0004 loss_train: 1.9133 acc_train: 0.4071 loss_val: 1.9134 acc_val: 0.5933 time: 0.0640s\n",
      "Epoch: 0005 loss_train: 1.9069 acc_train: 0.5143 loss_val: 1.9044 acc_val: 0.6033 time: 0.0702s\n",
      "Epoch: 0006 loss_train: 1.8998 acc_train: 0.4429 loss_val: 1.8955 acc_val: 0.5933 time: 0.0696s\n",
      "Epoch: 0007 loss_train: 1.8963 acc_train: 0.5214 loss_val: 1.8866 acc_val: 0.5933 time: 0.0698s\n",
      "Epoch: 0008 loss_train: 1.8793 acc_train: 0.5214 loss_val: 1.8776 acc_val: 0.6033 time: 0.0690s\n",
      "Epoch: 0009 loss_train: 1.8467 acc_train: 0.5857 loss_val: 1.8681 acc_val: 0.6067 time: 0.0707s\n",
      "Epoch: 0010 loss_train: 1.8370 acc_train: 0.5643 loss_val: 1.8583 acc_val: 0.6067 time: 0.0794s\n",
      "Epoch: 0011 loss_train: 1.8400 acc_train: 0.5500 loss_val: 1.8483 acc_val: 0.6100 time: 0.0700s\n",
      "Epoch: 0012 loss_train: 1.8349 acc_train: 0.5500 loss_val: 1.8381 acc_val: 0.5967 time: 0.0600s\n",
      "Epoch: 0013 loss_train: 1.8127 acc_train: 0.5357 loss_val: 1.8276 acc_val: 0.6000 time: 0.0592s\n",
      "Epoch: 0014 loss_train: 1.8023 acc_train: 0.5714 loss_val: 1.8170 acc_val: 0.6000 time: 0.0700s\n",
      "Epoch: 0015 loss_train: 1.7790 acc_train: 0.5714 loss_val: 1.8060 acc_val: 0.6000 time: 0.0803s\n",
      "Epoch: 0016 loss_train: 1.7374 acc_train: 0.6571 loss_val: 1.7946 acc_val: 0.5967 time: 0.0898s\n",
      "Epoch: 0017 loss_train: 1.7459 acc_train: 0.6286 loss_val: 1.7830 acc_val: 0.5967 time: 0.0800s\n",
      "Epoch: 0018 loss_train: 1.7324 acc_train: 0.6214 loss_val: 1.7711 acc_val: 0.5967 time: 0.0710s\n",
      "Epoch: 0019 loss_train: 1.7535 acc_train: 0.5286 loss_val: 1.7591 acc_val: 0.5967 time: 0.0788s\n",
      "Epoch: 0020 loss_train: 1.7740 acc_train: 0.5071 loss_val: 1.7472 acc_val: 0.5900 time: 0.0808s\n",
      "Epoch: 0021 loss_train: 1.7061 acc_train: 0.5429 loss_val: 1.7351 acc_val: 0.5833 time: 0.0894s\n",
      "Epoch: 0022 loss_train: 1.6918 acc_train: 0.5714 loss_val: 1.7228 acc_val: 0.5800 time: 0.0767s\n",
      "Epoch: 0023 loss_train: 1.6814 acc_train: 0.5071 loss_val: 1.7104 acc_val: 0.5767 time: 0.0708s\n",
      "Epoch: 0024 loss_train: 1.6445 acc_train: 0.5643 loss_val: 1.6978 acc_val: 0.5733 time: 0.0897s\n",
      "Epoch: 0025 loss_train: 1.6404 acc_train: 0.5143 loss_val: 1.6851 acc_val: 0.5733 time: 0.0797s\n",
      "Epoch: 0026 loss_train: 1.6237 acc_train: 0.5143 loss_val: 1.6723 acc_val: 0.5833 time: 0.0711s\n",
      "Epoch: 0027 loss_train: 1.6249 acc_train: 0.6214 loss_val: 1.6595 acc_val: 0.5867 time: 0.0799s\n",
      "Epoch: 0028 loss_train: 1.5732 acc_train: 0.5857 loss_val: 1.6464 acc_val: 0.5867 time: 0.0802s\n",
      "Epoch: 0029 loss_train: 1.5473 acc_train: 0.6214 loss_val: 1.6333 acc_val: 0.5967 time: 0.0850s\n",
      "Epoch: 0030 loss_train: 1.5740 acc_train: 0.5357 loss_val: 1.6202 acc_val: 0.6000 time: 0.0799s\n",
      "Epoch: 0031 loss_train: 1.5744 acc_train: 0.5429 loss_val: 1.6073 acc_val: 0.6000 time: 0.0711s\n",
      "Epoch: 0032 loss_train: 1.5613 acc_train: 0.5929 loss_val: 1.5945 acc_val: 0.6133 time: 0.0793s\n",
      "Epoch: 0033 loss_train: 1.5459 acc_train: 0.5929 loss_val: 1.5816 acc_val: 0.6133 time: 0.0799s\n",
      "Epoch: 0034 loss_train: 1.5639 acc_train: 0.5143 loss_val: 1.5689 acc_val: 0.6100 time: 0.0802s\n",
      "Epoch: 0035 loss_train: 1.4657 acc_train: 0.5929 loss_val: 1.5562 acc_val: 0.6133 time: 0.0807s\n",
      "Epoch: 0036 loss_train: 1.4800 acc_train: 0.6000 loss_val: 1.5435 acc_val: 0.6200 time: 0.0698s\n",
      "Epoch: 0037 loss_train: 1.5199 acc_train: 0.5429 loss_val: 1.5311 acc_val: 0.6300 time: 0.0796s\n",
      "Epoch: 0038 loss_train: 1.4606 acc_train: 0.5857 loss_val: 1.5188 acc_val: 0.6400 time: 0.0792s\n",
      "Epoch: 0039 loss_train: 1.4360 acc_train: 0.5929 loss_val: 1.5066 acc_val: 0.6433 time: 0.0851s\n",
      "Epoch: 0040 loss_train: 1.4696 acc_train: 0.5929 loss_val: 1.4945 acc_val: 0.6467 time: 0.0747s\n",
      "Epoch: 0041 loss_train: 1.4426 acc_train: 0.6357 loss_val: 1.4825 acc_val: 0.6600 time: 0.0851s\n",
      "Epoch: 0042 loss_train: 1.3874 acc_train: 0.5929 loss_val: 1.4705 acc_val: 0.6667 time: 0.0821s\n",
      "Epoch: 0043 loss_train: 1.3803 acc_train: 0.5857 loss_val: 1.4589 acc_val: 0.6667 time: 0.0802s\n",
      "Epoch: 0044 loss_train: 1.4060 acc_train: 0.6786 loss_val: 1.4473 acc_val: 0.6667 time: 0.0749s\n",
      "Epoch: 0045 loss_train: 1.3229 acc_train: 0.7143 loss_val: 1.4357 acc_val: 0.6867 time: 0.0663s\n",
      "Epoch: 0046 loss_train: 1.3812 acc_train: 0.6143 loss_val: 1.4244 acc_val: 0.7000 time: 0.0680s\n",
      "Epoch: 0047 loss_train: 1.4089 acc_train: 0.5929 loss_val: 1.4131 acc_val: 0.7033 time: 0.0675s\n",
      "Epoch: 0048 loss_train: 1.3557 acc_train: 0.6429 loss_val: 1.4019 acc_val: 0.7067 time: 0.0804s\n",
      "Epoch: 0049 loss_train: 1.2879 acc_train: 0.7500 loss_val: 1.3909 acc_val: 0.7100 time: 0.0855s\n",
      "Epoch: 0050 loss_train: 1.3604 acc_train: 0.6500 loss_val: 1.3800 acc_val: 0.7200 time: 0.0792s\n",
      "Epoch: 0051 loss_train: 1.3266 acc_train: 0.6643 loss_val: 1.3691 acc_val: 0.7233 time: 0.0586s\n",
      "Epoch: 0052 loss_train: 1.3071 acc_train: 0.6857 loss_val: 1.3587 acc_val: 0.7300 time: 0.0599s\n",
      "Epoch: 0053 loss_train: 1.2233 acc_train: 0.7500 loss_val: 1.3481 acc_val: 0.7367 time: 0.0699s\n",
      "Epoch: 0054 loss_train: 1.2993 acc_train: 0.6929 loss_val: 1.3376 acc_val: 0.7433 time: 0.0634s\n",
      "Epoch: 0055 loss_train: 1.2810 acc_train: 0.6929 loss_val: 1.3273 acc_val: 0.7467 time: 0.0602s\n",
      "Epoch: 0056 loss_train: 1.2715 acc_train: 0.7214 loss_val: 1.3170 acc_val: 0.7533 time: 0.0795s\n",
      "Epoch: 0057 loss_train: 1.2516 acc_train: 0.7071 loss_val: 1.3071 acc_val: 0.7600 time: 0.0900s\n",
      "Epoch: 0058 loss_train: 1.2655 acc_train: 0.6929 loss_val: 1.2974 acc_val: 0.7700 time: 0.0747s\n",
      "Epoch: 0059 loss_train: 1.2221 acc_train: 0.7500 loss_val: 1.2876 acc_val: 0.7767 time: 0.0805s\n",
      "Epoch: 0060 loss_train: 1.1529 acc_train: 0.8000 loss_val: 1.2777 acc_val: 0.7767 time: 0.0799s\n",
      "Epoch: 0061 loss_train: 1.2998 acc_train: 0.6857 loss_val: 1.2682 acc_val: 0.7800 time: 0.0801s\n",
      "Epoch: 0062 loss_train: 1.2487 acc_train: 0.7071 loss_val: 1.2588 acc_val: 0.7867 time: 0.0794s\n",
      "Epoch: 0063 loss_train: 1.2026 acc_train: 0.7143 loss_val: 1.2495 acc_val: 0.7967 time: 0.0849s\n",
      "Epoch: 0064 loss_train: 1.2581 acc_train: 0.7000 loss_val: 1.2407 acc_val: 0.8000 time: 0.0700s\n",
      "Epoch: 0065 loss_train: 1.1611 acc_train: 0.7571 loss_val: 1.2318 acc_val: 0.7967 time: 0.0700s\n",
      "Epoch: 0066 loss_train: 1.1958 acc_train: 0.6500 loss_val: 1.2230 acc_val: 0.7967 time: 0.0711s\n",
      "Epoch: 0067 loss_train: 1.1470 acc_train: 0.7429 loss_val: 1.2142 acc_val: 0.8033 time: 0.0794s\n",
      "Epoch: 0068 loss_train: 1.1648 acc_train: 0.6929 loss_val: 1.2055 acc_val: 0.8033 time: 0.0798s\n",
      "Epoch: 0069 loss_train: 1.1883 acc_train: 0.7357 loss_val: 1.1975 acc_val: 0.8033 time: 0.0813s\n",
      "Epoch: 0070 loss_train: 1.1319 acc_train: 0.7214 loss_val: 1.1896 acc_val: 0.8100 time: 0.0890s\n",
      "Epoch: 0071 loss_train: 1.0964 acc_train: 0.7500 loss_val: 1.1819 acc_val: 0.8067 time: 0.0704s\n",
      "Epoch: 0072 loss_train: 1.0877 acc_train: 0.7857 loss_val: 1.1739 acc_val: 0.8067 time: 0.0796s\n",
      "Epoch: 0073 loss_train: 1.1673 acc_train: 0.7286 loss_val: 1.1660 acc_val: 0.8067 time: 0.0794s\n",
      "Epoch: 0074 loss_train: 1.1032 acc_train: 0.7857 loss_val: 1.1582 acc_val: 0.8067 time: 0.0700s\n",
      "Epoch: 0075 loss_train: 1.1666 acc_train: 0.7214 loss_val: 1.1505 acc_val: 0.8133 time: 0.0754s\n",
      "Epoch: 0076 loss_train: 1.0634 acc_train: 0.7786 loss_val: 1.1427 acc_val: 0.8167 time: 0.0644s\n",
      "Epoch: 0077 loss_train: 1.0985 acc_train: 0.7929 loss_val: 1.1351 acc_val: 0.8167 time: 0.0699s\n",
      "Epoch: 0078 loss_train: 1.0750 acc_train: 0.7214 loss_val: 1.1277 acc_val: 0.8200 time: 0.0792s\n",
      "Epoch: 0079 loss_train: 1.1089 acc_train: 0.7357 loss_val: 1.1206 acc_val: 0.8200 time: 0.0900s\n",
      "Epoch: 0080 loss_train: 1.0283 acc_train: 0.8071 loss_val: 1.1135 acc_val: 0.8200 time: 0.0857s\n",
      "Epoch: 0081 loss_train: 1.1469 acc_train: 0.7214 loss_val: 1.1065 acc_val: 0.8233 time: 0.0724s\n",
      "Epoch: 0082 loss_train: 1.0090 acc_train: 0.7429 loss_val: 1.0995 acc_val: 0.8233 time: 0.0670s\n",
      "Epoch: 0083 loss_train: 1.0469 acc_train: 0.7786 loss_val: 1.0926 acc_val: 0.8267 time: 0.0667s\n",
      "Epoch: 0084 loss_train: 1.1208 acc_train: 0.7357 loss_val: 1.0860 acc_val: 0.8267 time: 0.0679s\n",
      "Epoch: 0085 loss_train: 1.0152 acc_train: 0.7429 loss_val: 1.0795 acc_val: 0.8367 time: 0.0639s\n",
      "Epoch: 0086 loss_train: 1.0415 acc_train: 0.7571 loss_val: 1.0729 acc_val: 0.8367 time: 0.0706s\n",
      "Epoch: 0087 loss_train: 1.0109 acc_train: 0.7929 loss_val: 1.0663 acc_val: 0.8367 time: 0.0797s\n",
      "Epoch: 0088 loss_train: 0.9385 acc_train: 0.7429 loss_val: 1.0599 acc_val: 0.8367 time: 0.0850s\n",
      "Epoch: 0089 loss_train: 1.0548 acc_train: 0.8071 loss_val: 1.0539 acc_val: 0.8400 time: 0.0803s\n",
      "Epoch: 0090 loss_train: 1.0149 acc_train: 0.7643 loss_val: 1.0478 acc_val: 0.8400 time: 0.0706s\n",
      "Epoch: 0091 loss_train: 1.0560 acc_train: 0.7429 loss_val: 1.0420 acc_val: 0.8467 time: 0.0593s\n",
      "Epoch: 0092 loss_train: 0.9477 acc_train: 0.7643 loss_val: 1.0361 acc_val: 0.8467 time: 0.0658s\n",
      "Epoch: 0093 loss_train: 1.0140 acc_train: 0.7357 loss_val: 1.0305 acc_val: 0.8467 time: 0.0785s\n",
      "Epoch: 0094 loss_train: 0.9679 acc_train: 0.7714 loss_val: 1.0250 acc_val: 0.8467 time: 0.0789s\n",
      "Epoch: 0095 loss_train: 1.0270 acc_train: 0.7500 loss_val: 1.0199 acc_val: 0.8467 time: 0.0906s\n",
      "Epoch: 0096 loss_train: 1.0144 acc_train: 0.7357 loss_val: 1.0150 acc_val: 0.8467 time: 0.0700s\n",
      "Epoch: 0097 loss_train: 1.0226 acc_train: 0.7429 loss_val: 1.0099 acc_val: 0.8467 time: 0.0696s\n",
      "Epoch: 0098 loss_train: 0.9609 acc_train: 0.8071 loss_val: 1.0048 acc_val: 0.8467 time: 0.0635s\n",
      "Epoch: 0099 loss_train: 1.0486 acc_train: 0.7429 loss_val: 1.0000 acc_val: 0.8433 time: 0.0706s\n",
      "Epoch: 0100 loss_train: 0.9938 acc_train: 0.7929 loss_val: 0.9953 acc_val: 0.8433 time: 0.0800s\n",
      "Epoch: 0101 loss_train: 0.9486 acc_train: 0.7500 loss_val: 0.9908 acc_val: 0.8433 time: 0.0859s\n",
      "Epoch: 0102 loss_train: 0.9068 acc_train: 0.7786 loss_val: 0.9862 acc_val: 0.8400 time: 0.0811s\n",
      "Epoch: 0103 loss_train: 0.8829 acc_train: 0.8071 loss_val: 0.9816 acc_val: 0.8400 time: 0.0801s\n",
      "Epoch: 0104 loss_train: 0.9182 acc_train: 0.7786 loss_val: 0.9770 acc_val: 0.8400 time: 0.0717s\n",
      "Epoch: 0105 loss_train: 0.8864 acc_train: 0.7786 loss_val: 0.9724 acc_val: 0.8400 time: 0.0707s\n",
      "Epoch: 0106 loss_train: 0.9343 acc_train: 0.7571 loss_val: 0.9678 acc_val: 0.8400 time: 0.0602s\n",
      "Epoch: 0107 loss_train: 0.9374 acc_train: 0.7786 loss_val: 0.9636 acc_val: 0.8367 time: 0.0645s\n",
      "Epoch: 0108 loss_train: 0.9956 acc_train: 0.7571 loss_val: 0.9594 acc_val: 0.8367 time: 0.0778s\n",
      "Epoch: 0109 loss_train: 0.8528 acc_train: 0.7929 loss_val: 0.9552 acc_val: 0.8367 time: 0.0697s\n",
      "Epoch: 0110 loss_train: 0.9424 acc_train: 0.7714 loss_val: 0.9512 acc_val: 0.8367 time: 0.0852s\n",
      "Epoch: 0111 loss_train: 0.9095 acc_train: 0.8000 loss_val: 0.9474 acc_val: 0.8333 time: 0.0798s\n",
      "Epoch: 0112 loss_train: 0.9306 acc_train: 0.7786 loss_val: 0.9440 acc_val: 0.8333 time: 0.0804s\n",
      "Epoch: 0113 loss_train: 0.7925 acc_train: 0.8143 loss_val: 0.9404 acc_val: 0.8333 time: 0.0698s\n",
      "Epoch: 0114 loss_train: 0.8743 acc_train: 0.7500 loss_val: 0.9369 acc_val: 0.8333 time: 0.0674s\n",
      "Epoch: 0115 loss_train: 0.8641 acc_train: 0.8000 loss_val: 0.9334 acc_val: 0.8333 time: 0.0702s\n",
      "Epoch: 0116 loss_train: 0.9114 acc_train: 0.8000 loss_val: 0.9298 acc_val: 0.8333 time: 0.0648s\n",
      "Epoch: 0117 loss_train: 0.9227 acc_train: 0.7286 loss_val: 0.9265 acc_val: 0.8400 time: 0.0799s\n",
      "Epoch: 0118 loss_train: 0.9079 acc_train: 0.7786 loss_val: 0.9233 acc_val: 0.8400 time: 0.0902s\n",
      "Epoch: 0119 loss_train: 0.8071 acc_train: 0.8214 loss_val: 0.9201 acc_val: 0.8367 time: 0.0803s\n",
      "Epoch: 0120 loss_train: 0.9068 acc_train: 0.7643 loss_val: 0.9173 acc_val: 0.8367 time: 0.0695s\n",
      "Epoch: 0121 loss_train: 0.8426 acc_train: 0.8214 loss_val: 0.9143 acc_val: 0.8367 time: 0.0821s\n",
      "Epoch: 0122 loss_train: 0.8627 acc_train: 0.7786 loss_val: 0.9114 acc_val: 0.8400 time: 0.0782s\n",
      "Epoch: 0123 loss_train: 0.8734 acc_train: 0.8000 loss_val: 0.9083 acc_val: 0.8400 time: 0.0851s\n",
      "Epoch: 0124 loss_train: 0.8823 acc_train: 0.8000 loss_val: 0.9051 acc_val: 0.8400 time: 0.0800s\n",
      "Epoch: 0125 loss_train: 0.7700 acc_train: 0.8429 loss_val: 0.9019 acc_val: 0.8400 time: 0.0699s\n",
      "Epoch: 0126 loss_train: 0.8832 acc_train: 0.8214 loss_val: 0.8990 acc_val: 0.8400 time: 0.0804s\n",
      "Epoch: 0127 loss_train: 0.8811 acc_train: 0.7429 loss_val: 0.8959 acc_val: 0.8400 time: 0.0895s\n",
      "Epoch: 0128 loss_train: 0.9332 acc_train: 0.7357 loss_val: 0.8931 acc_val: 0.8400 time: 0.0781s\n",
      "Epoch: 0129 loss_train: 0.8522 acc_train: 0.7857 loss_val: 0.8902 acc_val: 0.8367 time: 0.0794s\n",
      "Epoch: 0130 loss_train: 0.8715 acc_train: 0.7857 loss_val: 0.8876 acc_val: 0.8367 time: 0.0695s\n",
      "Epoch: 0131 loss_train: 0.9268 acc_train: 0.8000 loss_val: 0.8849 acc_val: 0.8367 time: 0.0651s\n",
      "Epoch: 0132 loss_train: 0.7966 acc_train: 0.8071 loss_val: 0.8822 acc_val: 0.8367 time: 0.0599s\n",
      "Epoch: 0133 loss_train: 0.8259 acc_train: 0.7929 loss_val: 0.8793 acc_val: 0.8367 time: 0.0695s\n",
      "Epoch: 0134 loss_train: 0.8580 acc_train: 0.8000 loss_val: 0.8766 acc_val: 0.8333 time: 0.0702s\n",
      "Epoch: 0135 loss_train: 0.8119 acc_train: 0.7857 loss_val: 0.8738 acc_val: 0.8367 time: 0.0807s\n",
      "Epoch: 0136 loss_train: 0.9307 acc_train: 0.7714 loss_val: 0.8712 acc_val: 0.8333 time: 0.0819s\n",
      "Epoch: 0137 loss_train: 0.8465 acc_train: 0.7429 loss_val: 0.8686 acc_val: 0.8367 time: 0.0802s\n",
      "Epoch: 0138 loss_train: 0.8367 acc_train: 0.7714 loss_val: 0.8661 acc_val: 0.8367 time: 0.0708s\n",
      "Epoch: 0139 loss_train: 0.8423 acc_train: 0.7786 loss_val: 0.8636 acc_val: 0.8367 time: 0.0617s\n",
      "Epoch: 0140 loss_train: 0.7961 acc_train: 0.8143 loss_val: 0.8613 acc_val: 0.8367 time: 0.0754s\n",
      "Epoch: 0141 loss_train: 0.8282 acc_train: 0.8000 loss_val: 0.8589 acc_val: 0.8367 time: 0.0603s\n",
      "Epoch: 0142 loss_train: 0.8608 acc_train: 0.7571 loss_val: 0.8566 acc_val: 0.8367 time: 0.0696s\n",
      "Epoch: 0143 loss_train: 0.7529 acc_train: 0.8571 loss_val: 0.8543 acc_val: 0.8367 time: 0.0718s\n",
      "Epoch: 0144 loss_train: 0.8449 acc_train: 0.8000 loss_val: 0.8520 acc_val: 0.8367 time: 0.0643s\n",
      "Epoch: 0145 loss_train: 0.8593 acc_train: 0.7929 loss_val: 0.8497 acc_val: 0.8367 time: 0.0650s\n",
      "Epoch: 0146 loss_train: 0.8263 acc_train: 0.7857 loss_val: 0.8474 acc_val: 0.8367 time: 0.0750s\n",
      "Epoch: 0147 loss_train: 0.8724 acc_train: 0.7929 loss_val: 0.8451 acc_val: 0.8367 time: 0.0800s\n",
      "Epoch: 0148 loss_train: 0.8520 acc_train: 0.7500 loss_val: 0.8429 acc_val: 0.8367 time: 0.0901s\n",
      "Epoch: 0149 loss_train: 0.9002 acc_train: 0.7214 loss_val: 0.8411 acc_val: 0.8367 time: 0.0810s\n",
      "Epoch: 0150 loss_train: 0.8028 acc_train: 0.8357 loss_val: 0.8392 acc_val: 0.8367 time: 0.0711s\n",
      "Epoch: 0151 loss_train: 0.7528 acc_train: 0.7857 loss_val: 0.8373 acc_val: 0.8367 time: 0.0674s\n",
      "Epoch: 0152 loss_train: 0.8615 acc_train: 0.7714 loss_val: 0.8358 acc_val: 0.8367 time: 0.0668s\n",
      "Epoch: 0153 loss_train: 0.8022 acc_train: 0.8071 loss_val: 0.8342 acc_val: 0.8367 time: 0.0709s\n",
      "Epoch: 0154 loss_train: 0.9016 acc_train: 0.7643 loss_val: 0.8323 acc_val: 0.8367 time: 0.0708s\n",
      "Epoch: 0155 loss_train: 0.7711 acc_train: 0.8143 loss_val: 0.8305 acc_val: 0.8367 time: 0.0770s\n",
      "Epoch: 0156 loss_train: 0.7700 acc_train: 0.8214 loss_val: 0.8287 acc_val: 0.8367 time: 0.0800s\n",
      "Epoch: 0157 loss_train: 0.7905 acc_train: 0.7643 loss_val: 0.8267 acc_val: 0.8367 time: 0.0805s\n",
      "Epoch: 0158 loss_train: 0.8470 acc_train: 0.7571 loss_val: 0.8248 acc_val: 0.8367 time: 0.0773s\n",
      "Epoch: 0159 loss_train: 0.7557 acc_train: 0.7929 loss_val: 0.8230 acc_val: 0.8367 time: 0.0701s\n",
      "Epoch: 0160 loss_train: 0.7617 acc_train: 0.7857 loss_val: 0.8213 acc_val: 0.8333 time: 0.0697s\n",
      "Epoch: 0161 loss_train: 0.6655 acc_train: 0.8857 loss_val: 0.8194 acc_val: 0.8400 time: 0.0710s\n",
      "Epoch: 0162 loss_train: 0.7216 acc_train: 0.8429 loss_val: 0.8173 acc_val: 0.8400 time: 0.0745s\n",
      "Epoch: 0163 loss_train: 0.8018 acc_train: 0.7714 loss_val: 0.8154 acc_val: 0.8400 time: 0.0649s\n",
      "Epoch: 0164 loss_train: 0.7824 acc_train: 0.7714 loss_val: 0.8135 acc_val: 0.8367 time: 0.0647s\n",
      "Epoch: 0165 loss_train: 0.7644 acc_train: 0.7857 loss_val: 0.8115 acc_val: 0.8367 time: 0.0702s\n",
      "Epoch: 0166 loss_train: 0.7219 acc_train: 0.8214 loss_val: 0.8096 acc_val: 0.8367 time: 0.0702s\n",
      "Epoch: 0167 loss_train: 0.8425 acc_train: 0.7714 loss_val: 0.8076 acc_val: 0.8367 time: 0.0798s\n",
      "Epoch: 0168 loss_train: 0.7791 acc_train: 0.7857 loss_val: 0.8058 acc_val: 0.8367 time: 0.0797s\n",
      "Epoch: 0169 loss_train: 0.8130 acc_train: 0.7714 loss_val: 0.8039 acc_val: 0.8367 time: 0.0700s\n",
      "Epoch: 0170 loss_train: 0.7817 acc_train: 0.8214 loss_val: 0.8020 acc_val: 0.8367 time: 0.0651s\n",
      "Epoch: 0171 loss_train: 0.6761 acc_train: 0.8500 loss_val: 0.8000 acc_val: 0.8367 time: 0.0749s\n",
      "Epoch: 0172 loss_train: 0.8556 acc_train: 0.8071 loss_val: 0.7979 acc_val: 0.8367 time: 0.0652s\n",
      "Epoch: 0173 loss_train: 0.8387 acc_train: 0.7714 loss_val: 0.7962 acc_val: 0.8367 time: 0.0699s\n",
      "Epoch: 0174 loss_train: 0.7835 acc_train: 0.7500 loss_val: 0.7944 acc_val: 0.8367 time: 0.0699s\n",
      "Epoch: 0175 loss_train: 0.8302 acc_train: 0.7714 loss_val: 0.7929 acc_val: 0.8367 time: 0.0707s\n",
      "Epoch: 0176 loss_train: 0.7310 acc_train: 0.8429 loss_val: 0.7911 acc_val: 0.8367 time: 0.0793s\n",
      "Epoch: 0177 loss_train: 0.7822 acc_train: 0.7786 loss_val: 0.7894 acc_val: 0.8367 time: 0.0760s\n",
      "Epoch: 0178 loss_train: 0.7588 acc_train: 0.7857 loss_val: 0.7876 acc_val: 0.8367 time: 0.0890s\n",
      "Epoch: 0179 loss_train: 0.7363 acc_train: 0.8214 loss_val: 0.7858 acc_val: 0.8367 time: 0.0754s\n",
      "Epoch: 0180 loss_train: 0.8402 acc_train: 0.7929 loss_val: 0.7839 acc_val: 0.8367 time: 0.0700s\n",
      "Epoch: 0181 loss_train: 0.7402 acc_train: 0.8071 loss_val: 0.7819 acc_val: 0.8367 time: 0.0698s\n",
      "Epoch: 0182 loss_train: 0.7468 acc_train: 0.7786 loss_val: 0.7801 acc_val: 0.8400 time: 0.0599s\n",
      "Epoch: 0183 loss_train: 0.7556 acc_train: 0.8071 loss_val: 0.7785 acc_val: 0.8400 time: 0.0699s\n",
      "Epoch: 0184 loss_train: 0.7520 acc_train: 0.8143 loss_val: 0.7769 acc_val: 0.8400 time: 0.0697s\n",
      "Epoch: 0185 loss_train: 0.7854 acc_train: 0.7786 loss_val: 0.7753 acc_val: 0.8400 time: 0.0607s\n",
      "Epoch: 0186 loss_train: 0.7113 acc_train: 0.8071 loss_val: 0.7737 acc_val: 0.8400 time: 0.0698s\n",
      "Epoch: 0187 loss_train: 0.6476 acc_train: 0.8429 loss_val: 0.7723 acc_val: 0.8433 time: 0.0802s\n",
      "Epoch: 0188 loss_train: 0.7635 acc_train: 0.8000 loss_val: 0.7709 acc_val: 0.8433 time: 0.0752s\n",
      "Epoch: 0189 loss_train: 0.7703 acc_train: 0.7929 loss_val: 0.7693 acc_val: 0.8433 time: 0.0850s\n",
      "Epoch: 0190 loss_train: 0.7494 acc_train: 0.7857 loss_val: 0.7680 acc_val: 0.8433 time: 0.0789s\n",
      "Epoch: 0191 loss_train: 0.7160 acc_train: 0.7929 loss_val: 0.7668 acc_val: 0.8433 time: 0.0664s\n",
      "Epoch: 0192 loss_train: 0.7961 acc_train: 0.7929 loss_val: 0.7659 acc_val: 0.8433 time: 0.0649s\n",
      "Epoch: 0193 loss_train: 0.7384 acc_train: 0.8071 loss_val: 0.7651 acc_val: 0.8400 time: 0.0720s\n",
      "Epoch: 0194 loss_train: 0.8018 acc_train: 0.7786 loss_val: 0.7643 acc_val: 0.8400 time: 0.0640s\n",
      "Epoch: 0195 loss_train: 0.8068 acc_train: 0.7571 loss_val: 0.7635 acc_val: 0.8400 time: 0.0700s\n",
      "Epoch: 0196 loss_train: 0.7907 acc_train: 0.7857 loss_val: 0.7630 acc_val: 0.8333 time: 0.0701s\n",
      "Epoch: 0197 loss_train: 0.7206 acc_train: 0.7857 loss_val: 0.7627 acc_val: 0.8333 time: 0.0650s\n",
      "Epoch: 0198 loss_train: 0.8315 acc_train: 0.7429 loss_val: 0.7624 acc_val: 0.8333 time: 0.0601s\n",
      "Epoch: 0199 loss_train: 0.7439 acc_train: 0.8000 loss_val: 0.7620 acc_val: 0.8333 time: 0.0700s\n",
      "Epoch: 0200 loss_train: 0.7862 acc_train: 0.8071 loss_val: 0.7614 acc_val: 0.8333 time: 0.0598s\n",
      "Epoch: 0201 loss_train: 0.8225 acc_train: 0.8071 loss_val: 0.7609 acc_val: 0.8367 time: 0.0697s\n",
      "Epoch: 0202 loss_train: 0.6829 acc_train: 0.8214 loss_val: 0.7601 acc_val: 0.8367 time: 0.0702s\n",
      "Epoch: 0203 loss_train: 0.7712 acc_train: 0.8071 loss_val: 0.7593 acc_val: 0.8333 time: 0.0701s\n",
      "Epoch: 0204 loss_train: 0.7301 acc_train: 0.8214 loss_val: 0.7584 acc_val: 0.8333 time: 0.0670s\n",
      "Epoch: 0205 loss_train: 0.7353 acc_train: 0.7857 loss_val: 0.7577 acc_val: 0.8300 time: 0.0703s\n",
      "Epoch: 0206 loss_train: 0.8227 acc_train: 0.7500 loss_val: 0.7569 acc_val: 0.8300 time: 0.0697s\n",
      "Epoch: 0207 loss_train: 0.7044 acc_train: 0.8214 loss_val: 0.7562 acc_val: 0.8300 time: 0.0601s\n",
      "Epoch: 0208 loss_train: 0.7266 acc_train: 0.8214 loss_val: 0.7554 acc_val: 0.8300 time: 0.0706s\n",
      "Epoch: 0209 loss_train: 0.7680 acc_train: 0.8214 loss_val: 0.7546 acc_val: 0.8300 time: 0.0691s\n",
      "Epoch: 0210 loss_train: 0.7141 acc_train: 0.8643 loss_val: 0.7537 acc_val: 0.8300 time: 0.0702s\n",
      "Epoch: 0211 loss_train: 0.7567 acc_train: 0.8000 loss_val: 0.7527 acc_val: 0.8300 time: 0.0798s\n",
      "Epoch: 0212 loss_train: 0.7796 acc_train: 0.8143 loss_val: 0.7514 acc_val: 0.8300 time: 0.0801s\n",
      "Epoch: 0213 loss_train: 0.6720 acc_train: 0.8143 loss_val: 0.7503 acc_val: 0.8300 time: 0.0804s\n",
      "Epoch: 0214 loss_train: 0.6525 acc_train: 0.8357 loss_val: 0.7489 acc_val: 0.8300 time: 0.0804s\n",
      "Epoch: 0215 loss_train: 0.7076 acc_train: 0.8143 loss_val: 0.7475 acc_val: 0.8300 time: 0.0749s\n",
      "Epoch: 0216 loss_train: 0.6885 acc_train: 0.8500 loss_val: 0.7460 acc_val: 0.8267 time: 0.0851s\n",
      "Epoch: 0217 loss_train: 0.7666 acc_train: 0.7929 loss_val: 0.7449 acc_val: 0.8267 time: 0.0851s\n",
      "Epoch: 0218 loss_train: 0.7999 acc_train: 0.7786 loss_val: 0.7439 acc_val: 0.8267 time: 0.0703s\n",
      "Epoch: 0219 loss_train: 0.7928 acc_train: 0.7786 loss_val: 0.7428 acc_val: 0.8233 time: 0.0701s\n",
      "Epoch: 0220 loss_train: 0.7210 acc_train: 0.7857 loss_val: 0.7418 acc_val: 0.8233 time: 0.0599s\n",
      "Epoch: 0221 loss_train: 0.6101 acc_train: 0.8500 loss_val: 0.7409 acc_val: 0.8233 time: 0.0799s\n",
      "Epoch: 0222 loss_train: 0.7114 acc_train: 0.8214 loss_val: 0.7398 acc_val: 0.8233 time: 0.0849s\n",
      "Epoch: 0223 loss_train: 0.7528 acc_train: 0.8000 loss_val: 0.7388 acc_val: 0.8233 time: 0.0799s\n",
      "Epoch: 0224 loss_train: 0.7364 acc_train: 0.8286 loss_val: 0.7377 acc_val: 0.8233 time: 0.0852s\n",
      "Epoch: 0225 loss_train: 0.7023 acc_train: 0.8571 loss_val: 0.7366 acc_val: 0.8233 time: 0.0700s\n",
      "Epoch: 0226 loss_train: 0.6874 acc_train: 0.7929 loss_val: 0.7357 acc_val: 0.8233 time: 0.0636s\n",
      "Epoch: 0227 loss_train: 0.7090 acc_train: 0.8143 loss_val: 0.7347 acc_val: 0.8267 time: 0.0751s\n",
      "Epoch: 0228 loss_train: 0.7161 acc_train: 0.7857 loss_val: 0.7339 acc_val: 0.8267 time: 0.0702s\n",
      "Epoch: 0229 loss_train: 0.6895 acc_train: 0.8071 loss_val: 0.7329 acc_val: 0.8233 time: 0.0844s\n",
      "Epoch: 0230 loss_train: 0.7414 acc_train: 0.8143 loss_val: 0.7320 acc_val: 0.8233 time: 0.0797s\n",
      "Epoch: 0231 loss_train: 0.7856 acc_train: 0.7857 loss_val: 0.7313 acc_val: 0.8267 time: 0.0852s\n",
      "Epoch: 0232 loss_train: 0.7405 acc_train: 0.8143 loss_val: 0.7310 acc_val: 0.8267 time: 0.0702s\n",
      "Epoch: 0233 loss_train: 0.7171 acc_train: 0.8214 loss_val: 0.7306 acc_val: 0.8267 time: 0.0748s\n",
      "Epoch: 0234 loss_train: 0.8433 acc_train: 0.7571 loss_val: 0.7300 acc_val: 0.8267 time: 0.0790s\n",
      "Epoch: 0235 loss_train: 0.7340 acc_train: 0.7929 loss_val: 0.7294 acc_val: 0.8267 time: 0.0846s\n",
      "Epoch: 0236 loss_train: 0.7236 acc_train: 0.8214 loss_val: 0.7288 acc_val: 0.8267 time: 0.0847s\n",
      "Epoch: 0237 loss_train: 0.6451 acc_train: 0.8143 loss_val: 0.7283 acc_val: 0.8267 time: 0.0799s\n",
      "Epoch: 0238 loss_train: 0.6775 acc_train: 0.8071 loss_val: 0.7277 acc_val: 0.8267 time: 0.0647s\n",
      "Epoch: 0239 loss_train: 0.8594 acc_train: 0.7643 loss_val: 0.7273 acc_val: 0.8233 time: 0.0802s\n",
      "Epoch: 0240 loss_train: 0.7386 acc_train: 0.8143 loss_val: 0.7268 acc_val: 0.8233 time: 0.0850s\n",
      "Epoch: 0241 loss_train: 0.6925 acc_train: 0.8071 loss_val: 0.7263 acc_val: 0.8233 time: 0.0799s\n",
      "Epoch: 0242 loss_train: 0.6575 acc_train: 0.8143 loss_val: 0.7257 acc_val: 0.8267 time: 0.0851s\n",
      "Epoch: 0243 loss_train: 0.6589 acc_train: 0.8000 loss_val: 0.7254 acc_val: 0.8267 time: 0.0649s\n",
      "Epoch: 0244 loss_train: 0.6708 acc_train: 0.7857 loss_val: 0.7250 acc_val: 0.8200 time: 0.0696s\n",
      "Epoch: 0245 loss_train: 0.7433 acc_train: 0.7714 loss_val: 0.7249 acc_val: 0.8200 time: 0.0693s\n",
      "Epoch: 0246 loss_train: 0.5921 acc_train: 0.8571 loss_val: 0.7247 acc_val: 0.8200 time: 0.0652s\n",
      "Epoch: 0247 loss_train: 0.8198 acc_train: 0.7643 loss_val: 0.7244 acc_val: 0.8200 time: 0.0646s\n",
      "Epoch: 0248 loss_train: 0.7760 acc_train: 0.7571 loss_val: 0.7245 acc_val: 0.8167 time: 0.0703s\n",
      "Epoch: 0249 loss_train: 0.7839 acc_train: 0.7500 loss_val: 0.7247 acc_val: 0.8133 time: 0.0800s\n",
      "Epoch: 0250 loss_train: 0.6080 acc_train: 0.8214 loss_val: 0.7248 acc_val: 0.8100 time: 0.0797s\n",
      "Epoch: 0251 loss_train: 0.7131 acc_train: 0.8143 loss_val: 0.7245 acc_val: 0.8100 time: 0.0802s\n",
      "Epoch: 0252 loss_train: 0.7242 acc_train: 0.8000 loss_val: 0.7242 acc_val: 0.8100 time: 0.0702s\n",
      "Epoch: 0253 loss_train: 0.7363 acc_train: 0.7857 loss_val: 0.7236 acc_val: 0.8100 time: 0.0695s\n",
      "Epoch: 0254 loss_train: 0.7161 acc_train: 0.8214 loss_val: 0.7230 acc_val: 0.8100 time: 0.0752s\n",
      "Epoch: 0255 loss_train: 0.6540 acc_train: 0.8143 loss_val: 0.7224 acc_val: 0.8100 time: 0.0700s\n",
      "Epoch: 0256 loss_train: 0.7053 acc_train: 0.8000 loss_val: 0.7222 acc_val: 0.8100 time: 0.0701s\n",
      "Epoch: 0257 loss_train: 0.6477 acc_train: 0.8143 loss_val: 0.7218 acc_val: 0.8100 time: 0.0799s\n",
      "Epoch: 0258 loss_train: 0.8490 acc_train: 0.7286 loss_val: 0.7209 acc_val: 0.8100 time: 0.0854s\n",
      "Epoch: 0259 loss_train: 0.6959 acc_train: 0.7714 loss_val: 0.7202 acc_val: 0.8100 time: 0.0802s\n",
      "Epoch: 0260 loss_train: 0.6706 acc_train: 0.8786 loss_val: 0.7195 acc_val: 0.8100 time: 0.0850s\n",
      "Epoch: 0261 loss_train: 0.7130 acc_train: 0.8214 loss_val: 0.7188 acc_val: 0.8100 time: 0.0798s\n",
      "Epoch: 0262 loss_train: 0.6218 acc_train: 0.8214 loss_val: 0.7178 acc_val: 0.8167 time: 0.0853s\n",
      "Epoch: 0263 loss_train: 0.6958 acc_train: 0.8429 loss_val: 0.7170 acc_val: 0.8167 time: 0.0846s\n",
      "Epoch: 0264 loss_train: 0.7872 acc_train: 0.7429 loss_val: 0.7163 acc_val: 0.8167 time: 0.0689s\n",
      "Epoch: 0265 loss_train: 0.7591 acc_train: 0.8071 loss_val: 0.7158 acc_val: 0.8167 time: 0.0652s\n",
      "Epoch: 0266 loss_train: 0.7222 acc_train: 0.8286 loss_val: 0.7154 acc_val: 0.8167 time: 0.0699s\n",
      "Epoch: 0267 loss_train: 0.6587 acc_train: 0.8286 loss_val: 0.7150 acc_val: 0.8200 time: 0.0699s\n",
      "Epoch: 0268 loss_train: 0.6284 acc_train: 0.8286 loss_val: 0.7144 acc_val: 0.8200 time: 0.0704s\n",
      "Epoch: 0269 loss_train: 0.7522 acc_train: 0.7643 loss_val: 0.7138 acc_val: 0.8200 time: 0.0797s\n",
      "Epoch: 0270 loss_train: 0.7675 acc_train: 0.7500 loss_val: 0.7130 acc_val: 0.8200 time: 0.0751s\n",
      "Epoch: 0271 loss_train: 0.7005 acc_train: 0.8143 loss_val: 0.7121 acc_val: 0.8167 time: 0.0798s\n",
      "Epoch: 0272 loss_train: 0.6662 acc_train: 0.8071 loss_val: 0.7113 acc_val: 0.8167 time: 0.0738s\n",
      "Epoch: 0273 loss_train: 0.6811 acc_train: 0.8286 loss_val: 0.7104 acc_val: 0.8167 time: 0.0698s\n",
      "Epoch: 0274 loss_train: 0.6577 acc_train: 0.7786 loss_val: 0.7095 acc_val: 0.8167 time: 0.0604s\n",
      "Epoch: 0275 loss_train: 0.6928 acc_train: 0.8143 loss_val: 0.7086 acc_val: 0.8167 time: 0.0701s\n",
      "Epoch: 0276 loss_train: 0.6877 acc_train: 0.8357 loss_val: 0.7077 acc_val: 0.8167 time: 0.0650s\n",
      "Epoch: 0277 loss_train: 0.6264 acc_train: 0.8714 loss_val: 0.7068 acc_val: 0.8167 time: 0.0703s\n",
      "Epoch: 0278 loss_train: 0.7367 acc_train: 0.7786 loss_val: 0.7058 acc_val: 0.8200 time: 0.0856s\n",
      "Epoch: 0279 loss_train: 0.7029 acc_train: 0.7714 loss_val: 0.7048 acc_val: 0.8200 time: 0.0855s\n",
      "Epoch: 0280 loss_train: 0.7096 acc_train: 0.7929 loss_val: 0.7040 acc_val: 0.8200 time: 0.0804s\n",
      "Epoch: 0281 loss_train: 0.6544 acc_train: 0.7786 loss_val: 0.7033 acc_val: 0.8200 time: 0.0747s\n",
      "Epoch: 0282 loss_train: 0.6170 acc_train: 0.8214 loss_val: 0.7031 acc_val: 0.8200 time: 0.0851s\n",
      "Epoch: 0283 loss_train: 0.6753 acc_train: 0.8214 loss_val: 0.7027 acc_val: 0.8200 time: 0.0799s\n",
      "Epoch: 0284 loss_train: 0.7846 acc_train: 0.7857 loss_val: 0.7027 acc_val: 0.8200 time: 0.0900s\n",
      "Epoch: 0285 loss_train: 0.7001 acc_train: 0.8429 loss_val: 0.7026 acc_val: 0.8200 time: 0.0848s\n",
      "Epoch: 0286 loss_train: 0.6625 acc_train: 0.8500 loss_val: 0.7026 acc_val: 0.8200 time: 0.0633s\n",
      "Epoch: 0287 loss_train: 0.6293 acc_train: 0.8000 loss_val: 0.7027 acc_val: 0.8200 time: 0.0698s\n",
      "Epoch: 0288 loss_train: 0.6819 acc_train: 0.7929 loss_val: 0.7031 acc_val: 0.8200 time: 0.0701s\n",
      "Epoch: 0289 loss_train: 0.5954 acc_train: 0.8429 loss_val: 0.7033 acc_val: 0.8200 time: 0.0697s\n",
      "Epoch: 0290 loss_train: 0.6124 acc_train: 0.8571 loss_val: 0.7036 acc_val: 0.8200 time: 0.0797s\n",
      "Epoch: 0291 loss_train: 0.6777 acc_train: 0.8357 loss_val: 0.7042 acc_val: 0.8200 time: 0.0797s\n",
      "Epoch: 0292 loss_train: 0.7821 acc_train: 0.7429 loss_val: 0.7048 acc_val: 0.8200 time: 0.0798s\n",
      "Epoch: 0293 loss_train: 0.6139 acc_train: 0.8214 loss_val: 0.7050 acc_val: 0.8200 time: 0.0710s\n",
      "Epoch: 0294 loss_train: 0.6853 acc_train: 0.8286 loss_val: 0.7055 acc_val: 0.8233 time: 0.0645s\n",
      "Epoch: 0295 loss_train: 0.6695 acc_train: 0.8357 loss_val: 0.7059 acc_val: 0.8233 time: 0.0702s\n",
      "Epoch: 0296 loss_train: 0.5903 acc_train: 0.8143 loss_val: 0.7065 acc_val: 0.8267 time: 0.0702s\n",
      "Epoch: 0297 loss_train: 0.7119 acc_train: 0.7786 loss_val: 0.7068 acc_val: 0.8267 time: 0.0749s\n",
      "Epoch: 0298 loss_train: 0.6722 acc_train: 0.8357 loss_val: 0.7070 acc_val: 0.8233 time: 0.0781s\n",
      "Epoch: 0299 loss_train: 0.6331 acc_train: 0.8429 loss_val: 0.7071 acc_val: 0.8233 time: 0.0828s\n",
      "Epoch: 0300 loss_train: 0.6364 acc_train: 0.8214 loss_val: 0.7070 acc_val: 0.8267 time: 0.0874s\n",
      "Epoch: 0301 loss_train: 0.6059 acc_train: 0.8429 loss_val: 0.7067 acc_val: 0.8267 time: 0.0595s\n",
      "Epoch: 0302 loss_train: 0.6463 acc_train: 0.7857 loss_val: 0.7065 acc_val: 0.8267 time: 0.0699s\n",
      "Epoch: 0303 loss_train: 0.7374 acc_train: 0.7857 loss_val: 0.7059 acc_val: 0.8267 time: 0.0850s\n",
      "Epoch: 0304 loss_train: 0.7353 acc_train: 0.8000 loss_val: 0.7054 acc_val: 0.8267 time: 0.0799s\n",
      "Epoch: 0305 loss_train: 0.6255 acc_train: 0.8000 loss_val: 0.7049 acc_val: 0.8267 time: 0.0900s\n",
      "Epoch: 0306 loss_train: 0.5716 acc_train: 0.8429 loss_val: 0.7045 acc_val: 0.8267 time: 0.0800s\n",
      "Epoch: 0307 loss_train: 0.6471 acc_train: 0.8429 loss_val: 0.7041 acc_val: 0.8267 time: 0.0702s\n",
      "Epoch: 0308 loss_train: 0.6681 acc_train: 0.8071 loss_val: 0.7035 acc_val: 0.8267 time: 0.0700s\n",
      "Epoch: 0309 loss_train: 0.6840 acc_train: 0.7929 loss_val: 0.7029 acc_val: 0.8233 time: 0.0801s\n",
      "Epoch: 0310 loss_train: 0.7326 acc_train: 0.7571 loss_val: 0.7019 acc_val: 0.8233 time: 0.0899s\n",
      "Epoch: 0311 loss_train: 0.6938 acc_train: 0.7714 loss_val: 0.7009 acc_val: 0.8233 time: 0.0901s\n",
      "Epoch: 0312 loss_train: 0.6962 acc_train: 0.8000 loss_val: 0.7002 acc_val: 0.8233 time: 0.0800s\n",
      "Epoch: 0313 loss_train: 0.7108 acc_train: 0.7857 loss_val: 0.6995 acc_val: 0.8233 time: 0.0700s\n",
      "Epoch: 0314 loss_train: 0.5884 acc_train: 0.8286 loss_val: 0.6990 acc_val: 0.8233 time: 0.0701s\n",
      "Epoch: 0315 loss_train: 0.5993 acc_train: 0.8643 loss_val: 0.6986 acc_val: 0.8233 time: 0.0772s\n",
      "Epoch: 0316 loss_train: 0.7073 acc_train: 0.8000 loss_val: 0.6980 acc_val: 0.8200 time: 0.0801s\n",
      "Epoch: 0317 loss_train: 0.7212 acc_train: 0.7571 loss_val: 0.6973 acc_val: 0.8200 time: 0.0908s\n",
      "Epoch: 0318 loss_train: 0.6003 acc_train: 0.8500 loss_val: 0.6967 acc_val: 0.8200 time: 0.0791s\n",
      "Epoch: 0319 loss_train: 0.7319 acc_train: 0.8143 loss_val: 0.6963 acc_val: 0.8200 time: 0.0651s\n",
      "Epoch: 0320 loss_train: 0.7695 acc_train: 0.7643 loss_val: 0.6959 acc_val: 0.8200 time: 0.0798s\n",
      "Epoch: 0321 loss_train: 0.5662 acc_train: 0.8429 loss_val: 0.6954 acc_val: 0.8233 time: 0.0900s\n",
      "Epoch: 0322 loss_train: 0.7042 acc_train: 0.7643 loss_val: 0.6949 acc_val: 0.8233 time: 0.0848s\n",
      "Epoch: 0323 loss_train: 0.6223 acc_train: 0.8643 loss_val: 0.6942 acc_val: 0.8233 time: 0.0804s\n",
      "Epoch: 0324 loss_train: 0.6493 acc_train: 0.8214 loss_val: 0.6935 acc_val: 0.8233 time: 0.0702s\n",
      "Epoch: 0325 loss_train: 0.6363 acc_train: 0.8071 loss_val: 0.6930 acc_val: 0.8267 time: 0.0647s\n",
      "Epoch: 0326 loss_train: 0.6646 acc_train: 0.8357 loss_val: 0.6927 acc_val: 0.8267 time: 0.0792s\n",
      "Epoch: 0327 loss_train: 0.5364 acc_train: 0.8857 loss_val: 0.6923 acc_val: 0.8300 time: 0.0828s\n",
      "Epoch: 0328 loss_train: 0.6656 acc_train: 0.8143 loss_val: 0.6921 acc_val: 0.8300 time: 0.0800s\n",
      "Epoch: 0329 loss_train: 0.5877 acc_train: 0.8500 loss_val: 0.6920 acc_val: 0.8300 time: 0.0798s\n",
      "Epoch: 0330 loss_train: 0.6509 acc_train: 0.8214 loss_val: 0.6917 acc_val: 0.8300 time: 0.0649s\n",
      "Epoch: 0331 loss_train: 0.6318 acc_train: 0.8571 loss_val: 0.6911 acc_val: 0.8267 time: 0.0698s\n",
      "Epoch: 0332 loss_train: 0.7221 acc_train: 0.7857 loss_val: 0.6909 acc_val: 0.8267 time: 0.0908s\n",
      "Epoch: 0333 loss_train: 0.7470 acc_train: 0.7714 loss_val: 0.6911 acc_val: 0.8300 time: 0.0849s\n",
      "Epoch: 0334 loss_train: 0.7164 acc_train: 0.8143 loss_val: 0.6913 acc_val: 0.8333 time: 0.0798s\n",
      "Epoch: 0335 loss_train: 0.6746 acc_train: 0.8000 loss_val: 0.6915 acc_val: 0.8300 time: 0.0803s\n",
      "Epoch: 0336 loss_train: 0.6583 acc_train: 0.8143 loss_val: 0.6914 acc_val: 0.8333 time: 0.0701s\n",
      "Epoch: 0337 loss_train: 0.5946 acc_train: 0.8429 loss_val: 0.6914 acc_val: 0.8300 time: 0.0695s\n",
      "Epoch: 0338 loss_train: 0.6793 acc_train: 0.8071 loss_val: 0.6914 acc_val: 0.8267 time: 0.0703s\n",
      "Epoch: 0339 loss_train: 0.5877 acc_train: 0.8357 loss_val: 0.6912 acc_val: 0.8267 time: 0.0649s\n",
      "Epoch: 0340 loss_train: 0.7307 acc_train: 0.7786 loss_val: 0.6911 acc_val: 0.8267 time: 0.0651s\n",
      "Epoch: 0341 loss_train: 0.7547 acc_train: 0.7857 loss_val: 0.6909 acc_val: 0.8267 time: 0.0709s\n",
      "Epoch: 0342 loss_train: 0.7112 acc_train: 0.8071 loss_val: 0.6905 acc_val: 0.8267 time: 0.0673s\n",
      "Epoch: 0343 loss_train: 0.7048 acc_train: 0.7929 loss_val: 0.6901 acc_val: 0.8267 time: 0.0762s\n",
      "Epoch: 0344 loss_train: 0.7301 acc_train: 0.8071 loss_val: 0.6896 acc_val: 0.8267 time: 0.0851s\n",
      "Epoch: 0345 loss_train: 0.6071 acc_train: 0.8214 loss_val: 0.6892 acc_val: 0.8267 time: 0.0804s\n",
      "Epoch: 0346 loss_train: 0.6628 acc_train: 0.7929 loss_val: 0.6888 acc_val: 0.8267 time: 0.0849s\n",
      "Epoch: 0347 loss_train: 0.6402 acc_train: 0.8286 loss_val: 0.6886 acc_val: 0.8233 time: 0.0733s\n",
      "Epoch: 0348 loss_train: 0.5932 acc_train: 0.8429 loss_val: 0.6883 acc_val: 0.8233 time: 0.0718s\n",
      "Epoch: 0349 loss_train: 0.5740 acc_train: 0.8571 loss_val: 0.6879 acc_val: 0.8233 time: 0.0669s\n",
      "Epoch: 0350 loss_train: 0.6448 acc_train: 0.8429 loss_val: 0.6875 acc_val: 0.8233 time: 0.0598s\n",
      "Epoch: 0351 loss_train: 0.6513 acc_train: 0.8071 loss_val: 0.6871 acc_val: 0.8233 time: 0.0804s\n",
      "Epoch: 0352 loss_train: 0.6257 acc_train: 0.8357 loss_val: 0.6869 acc_val: 0.8233 time: 0.0652s\n",
      "Epoch: 0353 loss_train: 0.5223 acc_train: 0.8429 loss_val: 0.6867 acc_val: 0.8233 time: 0.0599s\n",
      "Epoch: 0354 loss_train: 0.6609 acc_train: 0.8571 loss_val: 0.6865 acc_val: 0.8200 time: 0.0801s\n",
      "Epoch: 0355 loss_train: 0.5894 acc_train: 0.8214 loss_val: 0.6867 acc_val: 0.8200 time: 0.0650s\n",
      "Epoch: 0356 loss_train: 0.7086 acc_train: 0.8000 loss_val: 0.6867 acc_val: 0.8200 time: 0.0710s\n",
      "Epoch: 0357 loss_train: 0.6514 acc_train: 0.8143 loss_val: 0.6868 acc_val: 0.8200 time: 0.0793s\n",
      "Epoch: 0358 loss_train: 0.6773 acc_train: 0.7714 loss_val: 0.6868 acc_val: 0.8200 time: 0.0597s\n",
      "Epoch: 0359 loss_train: 0.6167 acc_train: 0.8143 loss_val: 0.6870 acc_val: 0.8233 time: 0.0704s\n",
      "Epoch: 0360 loss_train: 0.6065 acc_train: 0.8571 loss_val: 0.6869 acc_val: 0.8233 time: 0.0797s\n",
      "Epoch: 0361 loss_train: 0.6897 acc_train: 0.7857 loss_val: 0.6871 acc_val: 0.8233 time: 0.0602s\n",
      "Epoch: 0362 loss_train: 0.7012 acc_train: 0.7929 loss_val: 0.6874 acc_val: 0.8233 time: 0.0600s\n",
      "Epoch: 0363 loss_train: 0.6896 acc_train: 0.8071 loss_val: 0.6877 acc_val: 0.8233 time: 0.0715s\n",
      "Epoch: 0364 loss_train: 0.7226 acc_train: 0.7714 loss_val: 0.6879 acc_val: 0.8233 time: 0.0685s\n",
      "Epoch: 0365 loss_train: 0.5756 acc_train: 0.8714 loss_val: 0.6880 acc_val: 0.8233 time: 0.0679s\n",
      "Epoch: 0366 loss_train: 0.6407 acc_train: 0.8286 loss_val: 0.6880 acc_val: 0.8233 time: 0.0700s\n",
      "Epoch: 0367 loss_train: 0.7614 acc_train: 0.7286 loss_val: 0.6877 acc_val: 0.8233 time: 0.0704s\n",
      "Epoch: 0368 loss_train: 0.6124 acc_train: 0.8286 loss_val: 0.6872 acc_val: 0.8233 time: 0.0646s\n",
      "Epoch: 0369 loss_train: 0.6088 acc_train: 0.8214 loss_val: 0.6864 acc_val: 0.8233 time: 0.0702s\n",
      "Epoch: 0370 loss_train: 0.6269 acc_train: 0.8500 loss_val: 0.6858 acc_val: 0.8233 time: 0.0700s\n",
      "Epoch: 0371 loss_train: 0.6935 acc_train: 0.8071 loss_val: 0.6853 acc_val: 0.8233 time: 0.0698s\n",
      "Epoch: 0372 loss_train: 0.6679 acc_train: 0.8500 loss_val: 0.6848 acc_val: 0.8233 time: 0.0699s\n",
      "Epoch: 0373 loss_train: 0.6427 acc_train: 0.8000 loss_val: 0.6843 acc_val: 0.8233 time: 0.0723s\n",
      "Epoch: 0374 loss_train: 0.5999 acc_train: 0.8429 loss_val: 0.6837 acc_val: 0.8233 time: 0.0679s\n",
      "Epoch: 0375 loss_train: 0.6891 acc_train: 0.8214 loss_val: 0.6831 acc_val: 0.8233 time: 0.0699s\n",
      "Epoch: 0376 loss_train: 0.7298 acc_train: 0.7286 loss_val: 0.6821 acc_val: 0.8233 time: 0.0698s\n",
      "Epoch: 0377 loss_train: 0.5901 acc_train: 0.8357 loss_val: 0.6814 acc_val: 0.8233 time: 0.0699s\n",
      "Epoch: 0378 loss_train: 0.6252 acc_train: 0.8214 loss_val: 0.6809 acc_val: 0.8233 time: 0.0771s\n",
      "Epoch: 0379 loss_train: 0.6301 acc_train: 0.8214 loss_val: 0.6803 acc_val: 0.8233 time: 0.0631s\n",
      "Epoch: 0380 loss_train: 0.6441 acc_train: 0.8143 loss_val: 0.6797 acc_val: 0.8233 time: 0.0648s\n",
      "Epoch: 0381 loss_train: 0.6942 acc_train: 0.8000 loss_val: 0.6788 acc_val: 0.8233 time: 0.0753s\n",
      "Epoch: 0382 loss_train: 0.6663 acc_train: 0.8071 loss_val: 0.6778 acc_val: 0.8233 time: 0.0649s\n",
      "Epoch: 0383 loss_train: 0.6071 acc_train: 0.8429 loss_val: 0.6766 acc_val: 0.8267 time: 0.0650s\n",
      "Epoch: 0384 loss_train: 0.6060 acc_train: 0.8214 loss_val: 0.6754 acc_val: 0.8267 time: 0.0719s\n",
      "Epoch: 0385 loss_train: 0.6482 acc_train: 0.8214 loss_val: 0.6746 acc_val: 0.8233 time: 0.0652s\n",
      "Epoch: 0386 loss_train: 0.5648 acc_train: 0.8571 loss_val: 0.6741 acc_val: 0.8233 time: 0.0706s\n",
      "Epoch: 0387 loss_train: 0.5962 acc_train: 0.8286 loss_val: 0.6735 acc_val: 0.8233 time: 0.0692s\n",
      "Epoch: 0388 loss_train: 0.6460 acc_train: 0.8571 loss_val: 0.6731 acc_val: 0.8267 time: 0.0599s\n",
      "Epoch: 0389 loss_train: 0.6644 acc_train: 0.7857 loss_val: 0.6727 acc_val: 0.8267 time: 0.0698s\n",
      "Epoch: 0390 loss_train: 0.6074 acc_train: 0.8286 loss_val: 0.6725 acc_val: 0.8267 time: 0.0748s\n",
      "Epoch: 0391 loss_train: 0.6374 acc_train: 0.8143 loss_val: 0.6727 acc_val: 0.8267 time: 0.0601s\n",
      "Epoch: 0392 loss_train: 0.6535 acc_train: 0.8071 loss_val: 0.6729 acc_val: 0.8267 time: 0.0699s\n",
      "Epoch: 0393 loss_train: 0.6639 acc_train: 0.8571 loss_val: 0.6730 acc_val: 0.8267 time: 0.0702s\n",
      "Epoch: 0394 loss_train: 0.6375 acc_train: 0.8143 loss_val: 0.6732 acc_val: 0.8267 time: 0.0700s\n",
      "Epoch: 0395 loss_train: 0.7145 acc_train: 0.7786 loss_val: 0.6736 acc_val: 0.8267 time: 0.0698s\n",
      "Epoch: 0396 loss_train: 0.5599 acc_train: 0.8500 loss_val: 0.6741 acc_val: 0.8267 time: 0.0701s\n",
      "Epoch: 0397 loss_train: 0.6042 acc_train: 0.8286 loss_val: 0.6745 acc_val: 0.8267 time: 0.0701s\n",
      "Epoch: 0398 loss_train: 0.6012 acc_train: 0.8500 loss_val: 0.6748 acc_val: 0.8267 time: 0.0648s\n",
      "Epoch: 0399 loss_train: 0.6703 acc_train: 0.8000 loss_val: 0.6752 acc_val: 0.8267 time: 0.0698s\n",
      "Epoch: 0400 loss_train: 0.7104 acc_train: 0.7714 loss_val: 0.6754 acc_val: 0.8267 time: 0.0703s\n",
      "Epoch: 0401 loss_train: 0.6427 acc_train: 0.8429 loss_val: 0.6754 acc_val: 0.8233 time: 0.0720s\n",
      "Epoch: 0402 loss_train: 0.6754 acc_train: 0.8143 loss_val: 0.6755 acc_val: 0.8233 time: 0.0651s\n",
      "Epoch: 0403 loss_train: 0.5967 acc_train: 0.8571 loss_val: 0.6756 acc_val: 0.8233 time: 0.0710s\n",
      "Epoch: 0404 loss_train: 0.6495 acc_train: 0.8000 loss_val: 0.6755 acc_val: 0.8233 time: 0.0649s\n",
      "Epoch: 0405 loss_train: 0.5994 acc_train: 0.8071 loss_val: 0.6752 acc_val: 0.8233 time: 0.0603s\n",
      "Epoch: 0406 loss_train: 0.5977 acc_train: 0.8214 loss_val: 0.6751 acc_val: 0.8233 time: 0.0718s\n",
      "Epoch: 0407 loss_train: 0.6196 acc_train: 0.8286 loss_val: 0.6747 acc_val: 0.8233 time: 0.0678s\n",
      "Epoch: 0408 loss_train: 0.6147 acc_train: 0.8000 loss_val: 0.6744 acc_val: 0.8267 time: 0.0597s\n",
      "Epoch: 0409 loss_train: 0.6926 acc_train: 0.7714 loss_val: 0.6741 acc_val: 0.8233 time: 0.0626s\n",
      "Epoch: 0410 loss_train: 0.6438 acc_train: 0.7786 loss_val: 0.6738 acc_val: 0.8233 time: 0.0770s\n",
      "Epoch: 0411 loss_train: 0.6365 acc_train: 0.8071 loss_val: 0.6735 acc_val: 0.8233 time: 0.0646s\n",
      "Epoch: 0412 loss_train: 0.6491 acc_train: 0.8143 loss_val: 0.6729 acc_val: 0.8200 time: 0.0661s\n",
      "Epoch: 0413 loss_train: 0.6591 acc_train: 0.8357 loss_val: 0.6725 acc_val: 0.8200 time: 0.0731s\n",
      "Epoch: 0414 loss_train: 0.7353 acc_train: 0.8000 loss_val: 0.6723 acc_val: 0.8200 time: 0.0647s\n",
      "Epoch: 0415 loss_train: 0.5474 acc_train: 0.8500 loss_val: 0.6722 acc_val: 0.8200 time: 0.0704s\n",
      "Epoch: 0416 loss_train: 0.6519 acc_train: 0.7786 loss_val: 0.6723 acc_val: 0.8200 time: 0.0697s\n",
      "Epoch: 0417 loss_train: 0.6642 acc_train: 0.7929 loss_val: 0.6728 acc_val: 0.8200 time: 0.0601s\n",
      "Epoch: 0418 loss_train: 0.5401 acc_train: 0.8857 loss_val: 0.6731 acc_val: 0.8200 time: 0.0750s\n",
      "Epoch: 0419 loss_train: 0.6742 acc_train: 0.8357 loss_val: 0.6736 acc_val: 0.8200 time: 0.0749s\n",
      "Epoch: 0420 loss_train: 0.6879 acc_train: 0.7857 loss_val: 0.6743 acc_val: 0.8233 time: 0.0665s\n",
      "Epoch: 0421 loss_train: 0.6120 acc_train: 0.8286 loss_val: 0.6750 acc_val: 0.8233 time: 0.0626s\n",
      "Epoch: 0422 loss_train: 0.5566 acc_train: 0.8571 loss_val: 0.6755 acc_val: 0.8233 time: 0.0730s\n",
      "Epoch: 0423 loss_train: 0.6710 acc_train: 0.8143 loss_val: 0.6761 acc_val: 0.8233 time: 0.0668s\n",
      "Epoch: 0424 loss_train: 0.7041 acc_train: 0.8000 loss_val: 0.6767 acc_val: 0.8233 time: 0.0661s\n",
      "Epoch: 0425 loss_train: 0.6567 acc_train: 0.8071 loss_val: 0.6773 acc_val: 0.8233 time: 0.0711s\n",
      "Epoch: 0426 loss_train: 0.6778 acc_train: 0.7929 loss_val: 0.6776 acc_val: 0.8233 time: 0.0653s\n",
      "Epoch: 0427 loss_train: 0.5736 acc_train: 0.8500 loss_val: 0.6779 acc_val: 0.8233 time: 0.0627s\n",
      "Epoch: 0428 loss_train: 0.5580 acc_train: 0.8571 loss_val: 0.6780 acc_val: 0.8233 time: 0.0800s\n",
      "Epoch: 0429 loss_train: 0.7004 acc_train: 0.7857 loss_val: 0.6780 acc_val: 0.8233 time: 0.0603s\n",
      "Epoch: 0430 loss_train: 0.6653 acc_train: 0.8286 loss_val: 0.6777 acc_val: 0.8233 time: 0.0653s\n",
      "Epoch: 0431 loss_train: 0.6935 acc_train: 0.7929 loss_val: 0.6778 acc_val: 0.8200 time: 0.0746s\n",
      "Epoch: 0432 loss_train: 0.5534 acc_train: 0.8643 loss_val: 0.6779 acc_val: 0.8200 time: 0.0599s\n",
      "Epoch: 0433 loss_train: 0.6583 acc_train: 0.8000 loss_val: 0.6781 acc_val: 0.8200 time: 0.0650s\n",
      "Epoch: 0434 loss_train: 0.5650 acc_train: 0.8429 loss_val: 0.6783 acc_val: 0.8200 time: 0.0749s\n",
      "Epoch: 0435 loss_train: 0.5676 acc_train: 0.8643 loss_val: 0.6788 acc_val: 0.8200 time: 0.0598s\n",
      "Epoch: 0436 loss_train: 0.5744 acc_train: 0.8571 loss_val: 0.6789 acc_val: 0.8200 time: 0.0593s\n",
      "Epoch: 0437 loss_train: 0.6018 acc_train: 0.8357 loss_val: 0.6785 acc_val: 0.8200 time: 0.0717s\n",
      "Epoch: 0438 loss_train: 0.5998 acc_train: 0.8357 loss_val: 0.6782 acc_val: 0.8200 time: 0.0643s\n",
      "Epoch: 0439 loss_train: 0.6174 acc_train: 0.8286 loss_val: 0.6779 acc_val: 0.8200 time: 0.0603s\n",
      "Epoch: 0440 loss_train: 0.5883 acc_train: 0.8286 loss_val: 0.6775 acc_val: 0.8200 time: 0.0699s\n",
      "Epoch: 0441 loss_train: 0.7146 acc_train: 0.7929 loss_val: 0.6773 acc_val: 0.8200 time: 0.0700s\n",
      "Epoch: 0442 loss_train: 0.6321 acc_train: 0.7929 loss_val: 0.6772 acc_val: 0.8200 time: 0.0601s\n",
      "Epoch: 0443 loss_train: 0.6327 acc_train: 0.7857 loss_val: 0.6773 acc_val: 0.8200 time: 0.0598s\n",
      "Epoch: 0444 loss_train: 0.6220 acc_train: 0.8143 loss_val: 0.6773 acc_val: 0.8200 time: 0.0703s\n",
      "Epoch: 0445 loss_train: 0.6754 acc_train: 0.7714 loss_val: 0.6773 acc_val: 0.8200 time: 0.0701s\n",
      "Epoch: 0446 loss_train: 0.6702 acc_train: 0.7714 loss_val: 0.6775 acc_val: 0.8167 time: 0.0602s\n",
      "Epoch: 0447 loss_train: 0.6398 acc_train: 0.8214 loss_val: 0.6777 acc_val: 0.8167 time: 0.0699s\n",
      "Epoch: 0448 loss_train: 0.5858 acc_train: 0.8500 loss_val: 0.6780 acc_val: 0.8167 time: 0.0652s\n",
      "Epoch: 0449 loss_train: 0.7154 acc_train: 0.8000 loss_val: 0.6784 acc_val: 0.8167 time: 0.0698s\n",
      "Epoch: 0450 loss_train: 0.6198 acc_train: 0.8214 loss_val: 0.6791 acc_val: 0.8167 time: 0.0701s\n",
      "Epoch: 0451 loss_train: 0.6112 acc_train: 0.8214 loss_val: 0.6796 acc_val: 0.8167 time: 0.0598s\n",
      "Epoch: 0452 loss_train: 0.6118 acc_train: 0.8071 loss_val: 0.6799 acc_val: 0.8167 time: 0.0699s\n",
      "Epoch: 0453 loss_train: 0.7550 acc_train: 0.7786 loss_val: 0.6801 acc_val: 0.8133 time: 0.0651s\n",
      "Epoch: 0454 loss_train: 0.6519 acc_train: 0.7929 loss_val: 0.6801 acc_val: 0.8133 time: 0.0598s\n",
      "Epoch: 0455 loss_train: 0.6400 acc_train: 0.8357 loss_val: 0.6800 acc_val: 0.8133 time: 0.0602s\n",
      "Epoch: 0456 loss_train: 0.5517 acc_train: 0.8429 loss_val: 0.6798 acc_val: 0.8133 time: 0.0798s\n",
      "Epoch: 0457 loss_train: 0.5895 acc_train: 0.8643 loss_val: 0.6795 acc_val: 0.8133 time: 0.0700s\n",
      "Epoch: 0458 loss_train: 0.5938 acc_train: 0.8214 loss_val: 0.6791 acc_val: 0.8133 time: 0.0597s\n",
      "Epoch: 0459 loss_train: 0.6510 acc_train: 0.8143 loss_val: 0.6789 acc_val: 0.8133 time: 0.0650s\n",
      "Epoch: 0460 loss_train: 0.7239 acc_train: 0.7571 loss_val: 0.6784 acc_val: 0.8167 time: 0.0701s\n",
      "Epoch: 0461 loss_train: 0.6751 acc_train: 0.7571 loss_val: 0.6778 acc_val: 0.8167 time: 0.0592s\n",
      "Epoch: 0462 loss_train: 0.6765 acc_train: 0.7857 loss_val: 0.6773 acc_val: 0.8167 time: 0.0699s\n",
      "Epoch: 0463 loss_train: 0.5823 acc_train: 0.8643 loss_val: 0.6770 acc_val: 0.8167 time: 0.0648s\n",
      "Epoch: 0464 loss_train: 0.6088 acc_train: 0.8143 loss_val: 0.6765 acc_val: 0.8167 time: 0.0644s\n",
      "Epoch: 0465 loss_train: 0.5592 acc_train: 0.8429 loss_val: 0.6759 acc_val: 0.8200 time: 0.0697s\n",
      "Epoch: 0466 loss_train: 0.6030 acc_train: 0.8429 loss_val: 0.6751 acc_val: 0.8200 time: 0.0699s\n",
      "Epoch: 0467 loss_train: 0.6571 acc_train: 0.7643 loss_val: 0.6746 acc_val: 0.8200 time: 0.0699s\n",
      "Epoch: 0468 loss_train: 0.6966 acc_train: 0.7643 loss_val: 0.6741 acc_val: 0.8200 time: 0.0599s\n",
      "Epoch: 0469 loss_train: 0.7455 acc_train: 0.7214 loss_val: 0.6735 acc_val: 0.8233 time: 0.0599s\n",
      "Epoch: 0470 loss_train: 0.6943 acc_train: 0.7786 loss_val: 0.6734 acc_val: 0.8233 time: 0.0805s\n",
      "Epoch: 0471 loss_train: 0.6491 acc_train: 0.8071 loss_val: 0.6733 acc_val: 0.8233 time: 0.0591s\n",
      "Epoch: 0472 loss_train: 0.7298 acc_train: 0.7714 loss_val: 0.6735 acc_val: 0.8233 time: 0.0600s\n",
      "Epoch: 0473 loss_train: 0.6844 acc_train: 0.7643 loss_val: 0.6739 acc_val: 0.8233 time: 0.0700s\n",
      "Epoch: 0474 loss_train: 0.6445 acc_train: 0.8214 loss_val: 0.6742 acc_val: 0.8200 time: 0.0652s\n",
      "Epoch: 0475 loss_train: 0.6631 acc_train: 0.8000 loss_val: 0.6743 acc_val: 0.8233 time: 0.0651s\n",
      "Epoch: 0476 loss_train: 0.6683 acc_train: 0.8143 loss_val: 0.6746 acc_val: 0.8233 time: 0.0703s\n",
      "Epoch: 0477 loss_train: 0.4902 acc_train: 0.8429 loss_val: 0.6745 acc_val: 0.8267 time: 0.0601s\n",
      "Epoch: 0478 loss_train: 0.7114 acc_train: 0.7714 loss_val: 0.6745 acc_val: 0.8267 time: 0.0596s\n",
      "Epoch: 0479 loss_train: 0.5747 acc_train: 0.8286 loss_val: 0.6742 acc_val: 0.8267 time: 0.0702s\n",
      "Epoch: 0480 loss_train: 0.6248 acc_train: 0.8214 loss_val: 0.6740 acc_val: 0.8267 time: 0.0698s\n",
      "Epoch: 0481 loss_train: 0.5851 acc_train: 0.8071 loss_val: 0.6739 acc_val: 0.8300 time: 0.0600s\n",
      "Epoch: 0482 loss_train: 0.6764 acc_train: 0.7929 loss_val: 0.6737 acc_val: 0.8300 time: 0.0708s\n",
      "Epoch: 0483 loss_train: 0.6595 acc_train: 0.8143 loss_val: 0.6735 acc_val: 0.8300 time: 0.0694s\n",
      "Epoch: 0484 loss_train: 0.6196 acc_train: 0.8286 loss_val: 0.6733 acc_val: 0.8300 time: 0.0598s\n",
      "Epoch: 0485 loss_train: 0.6486 acc_train: 0.8500 loss_val: 0.6729 acc_val: 0.8267 time: 0.0701s\n",
      "Epoch: 0486 loss_train: 0.6531 acc_train: 0.8071 loss_val: 0.6727 acc_val: 0.8267 time: 0.0704s\n",
      "Epoch: 0487 loss_train: 0.6694 acc_train: 0.7929 loss_val: 0.6721 acc_val: 0.8267 time: 0.0596s\n",
      "Epoch: 0488 loss_train: 0.5685 acc_train: 0.8357 loss_val: 0.6716 acc_val: 0.8267 time: 0.0647s\n",
      "Epoch: 0489 loss_train: 0.5787 acc_train: 0.8571 loss_val: 0.6712 acc_val: 0.8267 time: 0.0748s\n",
      "Epoch: 0490 loss_train: 0.6254 acc_train: 0.8214 loss_val: 0.6708 acc_val: 0.8267 time: 0.0653s\n",
      "Epoch: 0491 loss_train: 0.5165 acc_train: 0.8500 loss_val: 0.6703 acc_val: 0.8267 time: 0.0594s\n",
      "Epoch: 0492 loss_train: 0.6820 acc_train: 0.7714 loss_val: 0.6697 acc_val: 0.8267 time: 0.0798s\n",
      "Epoch: 0493 loss_train: 0.6333 acc_train: 0.8000 loss_val: 0.6693 acc_val: 0.8267 time: 0.0600s\n",
      "Epoch: 0494 loss_train: 0.6321 acc_train: 0.8071 loss_val: 0.6687 acc_val: 0.8267 time: 0.0643s\n",
      "Epoch: 0495 loss_train: 0.6075 acc_train: 0.8571 loss_val: 0.6684 acc_val: 0.8267 time: 0.0703s\n",
      "Epoch: 0496 loss_train: 0.5902 acc_train: 0.8500 loss_val: 0.6685 acc_val: 0.8267 time: 0.0699s\n",
      "Epoch: 0497 loss_train: 0.6021 acc_train: 0.8500 loss_val: 0.6685 acc_val: 0.8267 time: 0.0675s\n",
      "Epoch: 0498 loss_train: 0.5453 acc_train: 0.8500 loss_val: 0.6683 acc_val: 0.8233 time: 0.0616s\n",
      "Epoch: 0499 loss_train: 0.5774 acc_train: 0.8500 loss_val: 0.6683 acc_val: 0.8233 time: 0.0681s\n",
      "Epoch: 0500 loss_train: 0.5939 acc_train: 0.8429 loss_val: 0.6684 acc_val: 0.8233 time: 0.0603s\n",
      "Epoch: 0501 loss_train: 0.6814 acc_train: 0.8000 loss_val: 0.6686 acc_val: 0.8233 time: 0.0599s\n",
      "Epoch: 0502 loss_train: 0.5746 acc_train: 0.8571 loss_val: 0.6689 acc_val: 0.8267 time: 0.0701s\n",
      "Epoch: 0503 loss_train: 0.6428 acc_train: 0.8286 loss_val: 0.6690 acc_val: 0.8233 time: 0.0718s\n",
      "Epoch: 0504 loss_train: 0.4926 acc_train: 0.8929 loss_val: 0.6691 acc_val: 0.8133 time: 0.0598s\n",
      "Epoch: 0505 loss_train: 0.6071 acc_train: 0.8500 loss_val: 0.6691 acc_val: 0.8133 time: 0.0699s\n",
      "Epoch: 0506 loss_train: 0.6688 acc_train: 0.8000 loss_val: 0.6689 acc_val: 0.8133 time: 0.0697s\n",
      "Epoch: 0507 loss_train: 0.6440 acc_train: 0.8000 loss_val: 0.6687 acc_val: 0.8133 time: 0.0738s\n",
      "Epoch: 0508 loss_train: 0.5760 acc_train: 0.8357 loss_val: 0.6686 acc_val: 0.8133 time: 0.0659s\n",
      "Epoch: 0509 loss_train: 0.6792 acc_train: 0.7643 loss_val: 0.6684 acc_val: 0.8133 time: 0.0700s\n",
      "Epoch: 0510 loss_train: 0.6800 acc_train: 0.7929 loss_val: 0.6686 acc_val: 0.8133 time: 0.0703s\n",
      "Epoch: 0511 loss_train: 0.6099 acc_train: 0.8357 loss_val: 0.6683 acc_val: 0.8133 time: 0.0755s\n",
      "Epoch: 0512 loss_train: 0.5942 acc_train: 0.8286 loss_val: 0.6681 acc_val: 0.8133 time: 0.0896s\n",
      "Epoch: 0513 loss_train: 0.5192 acc_train: 0.8571 loss_val: 0.6678 acc_val: 0.8133 time: 0.0861s\n",
      "Epoch: 0514 loss_train: 0.5640 acc_train: 0.8357 loss_val: 0.6675 acc_val: 0.8100 time: 0.0790s\n",
      "Epoch: 0515 loss_train: 0.6268 acc_train: 0.8500 loss_val: 0.6671 acc_val: 0.8100 time: 0.0803s\n",
      "Epoch: 0516 loss_train: 0.6447 acc_train: 0.7929 loss_val: 0.6671 acc_val: 0.8100 time: 0.0678s\n",
      "Epoch: 0517 loss_train: 0.6373 acc_train: 0.8286 loss_val: 0.6671 acc_val: 0.8167 time: 0.0592s\n",
      "Epoch: 0518 loss_train: 0.6850 acc_train: 0.8071 loss_val: 0.6670 acc_val: 0.8167 time: 0.0692s\n",
      "Epoch: 0519 loss_train: 0.6260 acc_train: 0.8429 loss_val: 0.6669 acc_val: 0.8133 time: 0.0708s\n",
      "Epoch: 0520 loss_train: 0.5803 acc_train: 0.8357 loss_val: 0.6668 acc_val: 0.8133 time: 0.0695s\n",
      "Epoch: 0521 loss_train: 0.6555 acc_train: 0.7857 loss_val: 0.6666 acc_val: 0.8133 time: 0.0700s\n",
      "Epoch: 0522 loss_train: 0.6804 acc_train: 0.8143 loss_val: 0.6661 acc_val: 0.8133 time: 0.0600s\n",
      "Epoch: 0523 loss_train: 0.6627 acc_train: 0.8000 loss_val: 0.6655 acc_val: 0.8133 time: 0.0752s\n",
      "Epoch: 0524 loss_train: 0.5724 acc_train: 0.8429 loss_val: 0.6649 acc_val: 0.8100 time: 0.0719s\n",
      "Epoch: 0525 loss_train: 0.6278 acc_train: 0.8000 loss_val: 0.6644 acc_val: 0.8167 time: 0.0598s\n",
      "Epoch: 0526 loss_train: 0.7314 acc_train: 0.7286 loss_val: 0.6637 acc_val: 0.8167 time: 0.0648s\n",
      "Epoch: 0527 loss_train: 0.6855 acc_train: 0.8000 loss_val: 0.6629 acc_val: 0.8167 time: 0.0599s\n",
      "Epoch: 0528 loss_train: 0.5423 acc_train: 0.8571 loss_val: 0.6623 acc_val: 0.8133 time: 0.0825s\n",
      "Epoch: 0529 loss_train: 0.5916 acc_train: 0.8071 loss_val: 0.6617 acc_val: 0.8133 time: 0.0705s\n",
      "Epoch: 0530 loss_train: 0.6119 acc_train: 0.8286 loss_val: 0.6614 acc_val: 0.8167 time: 0.0633s\n",
      "Epoch: 0531 loss_train: 0.6243 acc_train: 0.8286 loss_val: 0.6610 acc_val: 0.8133 time: 0.0641s\n",
      "Epoch: 0532 loss_train: 0.5320 acc_train: 0.8643 loss_val: 0.6607 acc_val: 0.8200 time: 0.0759s\n",
      "Epoch: 0533 loss_train: 0.6676 acc_train: 0.8071 loss_val: 0.6604 acc_val: 0.8233 time: 0.0700s\n",
      "Epoch: 0534 loss_train: 0.6625 acc_train: 0.8214 loss_val: 0.6602 acc_val: 0.8233 time: 0.0681s\n",
      "Epoch: 0535 loss_train: 0.6077 acc_train: 0.8429 loss_val: 0.6602 acc_val: 0.8233 time: 0.0698s\n",
      "Epoch: 0536 loss_train: 0.6411 acc_train: 0.8071 loss_val: 0.6601 acc_val: 0.8233 time: 0.0598s\n",
      "Epoch: 0537 loss_train: 0.5221 acc_train: 0.8571 loss_val: 0.6600 acc_val: 0.8233 time: 0.0685s\n",
      "Epoch: 0538 loss_train: 0.5974 acc_train: 0.8071 loss_val: 0.6598 acc_val: 0.8233 time: 0.0704s\n",
      "Epoch: 0539 loss_train: 0.6075 acc_train: 0.8286 loss_val: 0.6597 acc_val: 0.8233 time: 0.0698s\n",
      "Epoch: 0540 loss_train: 0.6368 acc_train: 0.8000 loss_val: 0.6598 acc_val: 0.8200 time: 0.0702s\n",
      "Epoch: 0541 loss_train: 0.5919 acc_train: 0.8143 loss_val: 0.6599 acc_val: 0.8233 time: 0.0699s\n",
      "Epoch: 0542 loss_train: 0.6470 acc_train: 0.8000 loss_val: 0.6602 acc_val: 0.8233 time: 0.0748s\n",
      "Epoch: 0543 loss_train: 0.6351 acc_train: 0.8357 loss_val: 0.6610 acc_val: 0.8233 time: 0.0604s\n",
      "Epoch: 0544 loss_train: 0.6500 acc_train: 0.7786 loss_val: 0.6617 acc_val: 0.8233 time: 0.0797s\n",
      "Epoch: 0545 loss_train: 0.6113 acc_train: 0.7857 loss_val: 0.6628 acc_val: 0.8200 time: 0.0603s\n",
      "Epoch: 0546 loss_train: 0.5164 acc_train: 0.8500 loss_val: 0.6637 acc_val: 0.8233 time: 0.0600s\n",
      "Epoch: 0547 loss_train: 0.6018 acc_train: 0.8429 loss_val: 0.6647 acc_val: 0.8200 time: 0.0803s\n",
      "Epoch: 0548 loss_train: 0.6135 acc_train: 0.8071 loss_val: 0.6661 acc_val: 0.8200 time: 0.0646s\n",
      "Epoch: 0549 loss_train: 0.6422 acc_train: 0.7929 loss_val: 0.6673 acc_val: 0.8200 time: 0.0699s\n",
      "Epoch: 0550 loss_train: 0.6749 acc_train: 0.7786 loss_val: 0.6687 acc_val: 0.8167 time: 0.0801s\n",
      "Epoch: 0551 loss_train: 0.5799 acc_train: 0.8286 loss_val: 0.6697 acc_val: 0.8167 time: 0.0753s\n",
      "Epoch: 0552 loss_train: 0.6126 acc_train: 0.8143 loss_val: 0.6706 acc_val: 0.8133 time: 0.0751s\n",
      "Epoch: 0553 loss_train: 0.6051 acc_train: 0.8071 loss_val: 0.6712 acc_val: 0.8133 time: 0.0752s\n",
      "Epoch: 0554 loss_train: 0.6392 acc_train: 0.8143 loss_val: 0.6715 acc_val: 0.8133 time: 0.0653s\n",
      "Epoch: 0555 loss_train: 0.6963 acc_train: 0.7857 loss_val: 0.6718 acc_val: 0.8133 time: 0.0686s\n",
      "Epoch: 0556 loss_train: 0.6060 acc_train: 0.8571 loss_val: 0.6715 acc_val: 0.8100 time: 0.0701s\n",
      "Epoch: 0557 loss_train: 0.6507 acc_train: 0.8357 loss_val: 0.6712 acc_val: 0.8100 time: 0.0802s\n",
      "Epoch: 0558 loss_train: 0.5967 acc_train: 0.8143 loss_val: 0.6709 acc_val: 0.8100 time: 0.0799s\n",
      "Epoch: 0559 loss_train: 0.6270 acc_train: 0.8286 loss_val: 0.6705 acc_val: 0.8100 time: 0.0800s\n",
      "Epoch: 0560 loss_train: 0.6229 acc_train: 0.8214 loss_val: 0.6701 acc_val: 0.8100 time: 0.0648s\n",
      "Epoch: 0561 loss_train: 0.6353 acc_train: 0.8214 loss_val: 0.6697 acc_val: 0.8100 time: 0.0700s\n",
      "Epoch: 0562 loss_train: 0.6132 acc_train: 0.8357 loss_val: 0.6692 acc_val: 0.8100 time: 0.0852s\n",
      "Epoch: 0563 loss_train: 0.6835 acc_train: 0.7786 loss_val: 0.6686 acc_val: 0.8100 time: 0.0850s\n",
      "Epoch: 0564 loss_train: 0.6574 acc_train: 0.7929 loss_val: 0.6682 acc_val: 0.8067 time: 0.0799s\n",
      "Epoch: 0565 loss_train: 0.5322 acc_train: 0.8714 loss_val: 0.6678 acc_val: 0.8067 time: 0.0795s\n",
      "Epoch: 0566 loss_train: 0.5670 acc_train: 0.8643 loss_val: 0.6673 acc_val: 0.8067 time: 0.0853s\n",
      "Epoch: 0567 loss_train: 0.6622 acc_train: 0.8214 loss_val: 0.6668 acc_val: 0.8133 time: 0.0850s\n",
      "Epoch: 0568 loss_train: 0.5680 acc_train: 0.8571 loss_val: 0.6661 acc_val: 0.8100 time: 0.0854s\n",
      "Epoch: 0569 loss_train: 0.6604 acc_train: 0.7857 loss_val: 0.6652 acc_val: 0.8100 time: 0.0847s\n",
      "Epoch: 0570 loss_train: 0.6117 acc_train: 0.8071 loss_val: 0.6644 acc_val: 0.8133 time: 0.0748s\n",
      "Epoch: 0571 loss_train: 0.6214 acc_train: 0.8429 loss_val: 0.6634 acc_val: 0.8133 time: 0.0753s\n",
      "Epoch: 0572 loss_train: 0.5467 acc_train: 0.8571 loss_val: 0.6624 acc_val: 0.8133 time: 0.0701s\n",
      "Epoch: 0573 loss_train: 0.5949 acc_train: 0.8286 loss_val: 0.6617 acc_val: 0.8133 time: 0.0600s\n",
      "Epoch: 0574 loss_train: 0.5013 acc_train: 0.8643 loss_val: 0.6612 acc_val: 0.8133 time: 0.0801s\n",
      "Epoch: 0575 loss_train: 0.6789 acc_train: 0.8143 loss_val: 0.6608 acc_val: 0.8133 time: 0.0802s\n",
      "Epoch: 0576 loss_train: 0.5765 acc_train: 0.8286 loss_val: 0.6606 acc_val: 0.8167 time: 0.0797s\n",
      "Epoch: 0577 loss_train: 0.5917 acc_train: 0.8143 loss_val: 0.6607 acc_val: 0.8167 time: 0.0701s\n",
      "Epoch: 0578 loss_train: 0.5645 acc_train: 0.8357 loss_val: 0.6610 acc_val: 0.8167 time: 0.0704s\n",
      "Epoch: 0579 loss_train: 0.5732 acc_train: 0.8429 loss_val: 0.6615 acc_val: 0.8167 time: 0.0697s\n",
      "Epoch: 0580 loss_train: 0.6316 acc_train: 0.8071 loss_val: 0.6620 acc_val: 0.8167 time: 0.0700s\n",
      "Epoch: 0581 loss_train: 0.5795 acc_train: 0.8286 loss_val: 0.6624 acc_val: 0.8167 time: 0.0702s\n",
      "Epoch: 0582 loss_train: 0.6339 acc_train: 0.8000 loss_val: 0.6629 acc_val: 0.8133 time: 0.0600s\n",
      "Epoch: 0583 loss_train: 0.6592 acc_train: 0.7786 loss_val: 0.6631 acc_val: 0.8133 time: 0.0806s\n",
      "Epoch: 0584 loss_train: 0.5766 acc_train: 0.8357 loss_val: 0.6633 acc_val: 0.8133 time: 0.0844s\n",
      "Epoch: 0585 loss_train: 0.5983 acc_train: 0.8071 loss_val: 0.6635 acc_val: 0.8100 time: 0.0854s\n",
      "Epoch: 0586 loss_train: 0.6714 acc_train: 0.7929 loss_val: 0.6637 acc_val: 0.8100 time: 0.0650s\n",
      "Epoch: 0587 loss_train: 0.5365 acc_train: 0.8714 loss_val: 0.6638 acc_val: 0.8100 time: 0.0698s\n",
      "Epoch: 0588 loss_train: 0.5689 acc_train: 0.8429 loss_val: 0.6639 acc_val: 0.8100 time: 0.0702s\n",
      "Epoch: 0589 loss_train: 0.5760 acc_train: 0.8714 loss_val: 0.6637 acc_val: 0.8067 time: 0.0665s\n",
      "Epoch: 0590 loss_train: 0.5538 acc_train: 0.8357 loss_val: 0.6637 acc_val: 0.8067 time: 0.0700s\n",
      "Epoch: 0591 loss_train: 0.6202 acc_train: 0.8357 loss_val: 0.6636 acc_val: 0.8067 time: 0.0650s\n",
      "Epoch: 0592 loss_train: 0.7131 acc_train: 0.7857 loss_val: 0.6637 acc_val: 0.8067 time: 0.0697s\n",
      "Epoch: 0593 loss_train: 0.5151 acc_train: 0.8571 loss_val: 0.6638 acc_val: 0.8067 time: 0.0803s\n",
      "Epoch: 0594 loss_train: 0.7066 acc_train: 0.7714 loss_val: 0.6642 acc_val: 0.8067 time: 0.0850s\n",
      "Epoch: 0595 loss_train: 0.5612 acc_train: 0.8429 loss_val: 0.6649 acc_val: 0.8100 time: 0.0798s\n",
      "Epoch: 0596 loss_train: 0.5850 acc_train: 0.8786 loss_val: 0.6656 acc_val: 0.8133 time: 0.0849s\n",
      "Epoch: 0597 loss_train: 0.6823 acc_train: 0.7857 loss_val: 0.6663 acc_val: 0.8133 time: 0.0649s\n",
      "Epoch: 0598 loss_train: 0.7727 acc_train: 0.7500 loss_val: 0.6671 acc_val: 0.8133 time: 0.0642s\n",
      "Epoch: 0599 loss_train: 0.5158 acc_train: 0.8571 loss_val: 0.6676 acc_val: 0.8133 time: 0.0804s\n",
      "Epoch: 0600 loss_train: 0.5579 acc_train: 0.8429 loss_val: 0.6686 acc_val: 0.8133 time: 0.0796s\n",
      "Epoch: 0601 loss_train: 0.6684 acc_train: 0.8071 loss_val: 0.6692 acc_val: 0.8133 time: 0.0801s\n",
      "Epoch: 0602 loss_train: 0.5378 acc_train: 0.8143 loss_val: 0.6700 acc_val: 0.8133 time: 0.0701s\n",
      "Epoch: 0603 loss_train: 0.6543 acc_train: 0.7929 loss_val: 0.6708 acc_val: 0.8133 time: 0.0651s\n",
      "Epoch: 0604 loss_train: 0.5935 acc_train: 0.8214 loss_val: 0.6711 acc_val: 0.8133 time: 0.0721s\n",
      "Epoch: 0605 loss_train: 0.6166 acc_train: 0.8429 loss_val: 0.6716 acc_val: 0.8167 time: 0.0847s\n",
      "Epoch: 0606 loss_train: 0.6498 acc_train: 0.8214 loss_val: 0.6717 acc_val: 0.8167 time: 0.0842s\n",
      "Epoch: 0607 loss_train: 0.5545 acc_train: 0.8571 loss_val: 0.6713 acc_val: 0.8167 time: 0.0853s\n",
      "Epoch: 0608 loss_train: 0.6481 acc_train: 0.7857 loss_val: 0.6706 acc_val: 0.8167 time: 0.0799s\n",
      "Epoch: 0609 loss_train: 0.5436 acc_train: 0.8643 loss_val: 0.6698 acc_val: 0.8167 time: 0.0811s\n",
      "Epoch: 0610 loss_train: 0.7459 acc_train: 0.7714 loss_val: 0.6686 acc_val: 0.8167 time: 0.0859s\n",
      "Epoch: 0611 loss_train: 0.6772 acc_train: 0.7857 loss_val: 0.6671 acc_val: 0.8167 time: 0.0778s\n",
      "Epoch: 0612 loss_train: 0.6195 acc_train: 0.7929 loss_val: 0.6657 acc_val: 0.8167 time: 0.0652s\n",
      "Epoch: 0613 loss_train: 0.5707 acc_train: 0.8429 loss_val: 0.6646 acc_val: 0.8167 time: 0.0648s\n",
      "Epoch: 0614 loss_train: 0.5991 acc_train: 0.8429 loss_val: 0.6638 acc_val: 0.8167 time: 0.0708s\n",
      "Epoch: 0615 loss_train: 0.6347 acc_train: 0.7786 loss_val: 0.6631 acc_val: 0.8167 time: 0.0684s\n",
      "Epoch: 0616 loss_train: 0.5510 acc_train: 0.8500 loss_val: 0.6625 acc_val: 0.8133 time: 0.0655s\n",
      "Epoch: 0617 loss_train: 0.6126 acc_train: 0.8000 loss_val: 0.6618 acc_val: 0.8133 time: 0.0701s\n",
      "Epoch: 0618 loss_train: 0.4915 acc_train: 0.8786 loss_val: 0.6611 acc_val: 0.8133 time: 0.0736s\n",
      "Epoch: 0619 loss_train: 0.6546 acc_train: 0.8286 loss_val: 0.6609 acc_val: 0.8133 time: 0.0665s\n",
      "Epoch: 0620 loss_train: 0.6653 acc_train: 0.8143 loss_val: 0.6609 acc_val: 0.8133 time: 0.0697s\n",
      "Epoch: 0621 loss_train: 0.6519 acc_train: 0.8071 loss_val: 0.6609 acc_val: 0.8133 time: 0.0701s\n",
      "Epoch: 0622 loss_train: 0.6163 acc_train: 0.8071 loss_val: 0.6609 acc_val: 0.8100 time: 0.0648s\n",
      "Epoch: 0623 loss_train: 0.6644 acc_train: 0.7929 loss_val: 0.6610 acc_val: 0.8100 time: 0.0702s\n",
      "Epoch: 0624 loss_train: 0.6389 acc_train: 0.8357 loss_val: 0.6614 acc_val: 0.8100 time: 0.0699s\n",
      "Epoch: 0625 loss_train: 0.5874 acc_train: 0.8429 loss_val: 0.6617 acc_val: 0.8100 time: 0.0650s\n",
      "Epoch: 0626 loss_train: 0.5924 acc_train: 0.7857 loss_val: 0.6615 acc_val: 0.8100 time: 0.0697s\n",
      "Epoch: 0627 loss_train: 0.5800 acc_train: 0.8071 loss_val: 0.6610 acc_val: 0.8100 time: 0.0651s\n",
      "Epoch: 0628 loss_train: 0.6118 acc_train: 0.8214 loss_val: 0.6607 acc_val: 0.8100 time: 0.0652s\n",
      "Epoch: 0629 loss_train: 0.5796 acc_train: 0.8286 loss_val: 0.6607 acc_val: 0.8100 time: 0.0748s\n",
      "Epoch: 0630 loss_train: 0.6116 acc_train: 0.8500 loss_val: 0.6605 acc_val: 0.8067 time: 0.0648s\n",
      "Epoch: 0631 loss_train: 0.6027 acc_train: 0.8286 loss_val: 0.6604 acc_val: 0.8067 time: 0.0702s\n",
      "Epoch: 0632 loss_train: 0.6572 acc_train: 0.7643 loss_val: 0.6603 acc_val: 0.8067 time: 0.0752s\n",
      "Epoch: 0633 loss_train: 0.6630 acc_train: 0.8214 loss_val: 0.6601 acc_val: 0.8100 time: 0.0649s\n",
      "Epoch: 0634 loss_train: 0.5719 acc_train: 0.8214 loss_val: 0.6597 acc_val: 0.8100 time: 0.0698s\n",
      "Epoch: 0635 loss_train: 0.6310 acc_train: 0.7929 loss_val: 0.6597 acc_val: 0.8100 time: 0.0698s\n",
      "Epoch: 0636 loss_train: 0.6639 acc_train: 0.7714 loss_val: 0.6597 acc_val: 0.8133 time: 0.0798s\n",
      "Epoch: 0637 loss_train: 0.6378 acc_train: 0.8071 loss_val: 0.6598 acc_val: 0.8167 time: 0.0702s\n",
      "Epoch: 0638 loss_train: 0.6230 acc_train: 0.8214 loss_val: 0.6598 acc_val: 0.8167 time: 0.0798s\n",
      "Epoch: 0639 loss_train: 0.6983 acc_train: 0.8214 loss_val: 0.6595 acc_val: 0.8167 time: 0.0704s\n",
      "Epoch: 0640 loss_train: 0.6644 acc_train: 0.7714 loss_val: 0.6591 acc_val: 0.8167 time: 0.0596s\n",
      "Epoch: 0641 loss_train: 0.6121 acc_train: 0.8143 loss_val: 0.6590 acc_val: 0.8167 time: 0.0698s\n",
      "Epoch: 0642 loss_train: 0.6253 acc_train: 0.8357 loss_val: 0.6588 acc_val: 0.8167 time: 0.0801s\n",
      "Epoch: 0643 loss_train: 0.6629 acc_train: 0.7786 loss_val: 0.6585 acc_val: 0.8167 time: 0.0796s\n",
      "Epoch: 0644 loss_train: 0.5929 acc_train: 0.8429 loss_val: 0.6581 acc_val: 0.8167 time: 0.0703s\n",
      "Epoch: 0645 loss_train: 0.6445 acc_train: 0.7929 loss_val: 0.6577 acc_val: 0.8133 time: 0.0702s\n",
      "Epoch: 0646 loss_train: 0.6108 acc_train: 0.8357 loss_val: 0.6575 acc_val: 0.8133 time: 0.0702s\n",
      "Epoch: 0647 loss_train: 0.5989 acc_train: 0.8214 loss_val: 0.6574 acc_val: 0.8133 time: 0.0702s\n",
      "Epoch: 0648 loss_train: 0.6352 acc_train: 0.8214 loss_val: 0.6571 acc_val: 0.8133 time: 0.0799s\n",
      "Epoch: 0649 loss_train: 0.5337 acc_train: 0.8571 loss_val: 0.6568 acc_val: 0.8133 time: 0.0641s\n",
      "Epoch: 0650 loss_train: 0.5543 acc_train: 0.8357 loss_val: 0.6563 acc_val: 0.8133 time: 0.0748s\n",
      "Epoch: 0651 loss_train: 0.6361 acc_train: 0.8071 loss_val: 0.6561 acc_val: 0.8133 time: 0.0734s\n",
      "Epoch: 0652 loss_train: 0.5206 acc_train: 0.8786 loss_val: 0.6561 acc_val: 0.8133 time: 0.0756s\n",
      "Epoch: 0653 loss_train: 0.6355 acc_train: 0.8357 loss_val: 0.6565 acc_val: 0.8133 time: 0.0699s\n",
      "Epoch: 0654 loss_train: 0.7291 acc_train: 0.7643 loss_val: 0.6568 acc_val: 0.8167 time: 0.0650s\n",
      "Epoch: 0655 loss_train: 0.6104 acc_train: 0.8000 loss_val: 0.6569 acc_val: 0.8167 time: 0.0699s\n",
      "Epoch: 0656 loss_train: 0.6936 acc_train: 0.8071 loss_val: 0.6569 acc_val: 0.8167 time: 0.0753s\n",
      "Epoch: 0657 loss_train: 0.5790 acc_train: 0.8429 loss_val: 0.6568 acc_val: 0.8133 time: 0.0698s\n",
      "Epoch: 0658 loss_train: 0.5809 acc_train: 0.8357 loss_val: 0.6569 acc_val: 0.8133 time: 0.0751s\n",
      "Epoch: 0659 loss_train: 0.5598 acc_train: 0.8286 loss_val: 0.6567 acc_val: 0.8100 time: 0.0748s\n",
      "Epoch: 0660 loss_train: 0.6003 acc_train: 0.8500 loss_val: 0.6564 acc_val: 0.8100 time: 0.0648s\n",
      "Epoch: 0661 loss_train: 0.5368 acc_train: 0.8357 loss_val: 0.6562 acc_val: 0.8100 time: 0.0653s\n",
      "Epoch: 0662 loss_train: 0.6657 acc_train: 0.8143 loss_val: 0.6560 acc_val: 0.8100 time: 0.0699s\n",
      "Epoch: 0663 loss_train: 0.5712 acc_train: 0.8357 loss_val: 0.6556 acc_val: 0.8100 time: 0.0662s\n",
      "Epoch: 0664 loss_train: 0.6245 acc_train: 0.8000 loss_val: 0.6551 acc_val: 0.8100 time: 0.0640s\n",
      "Epoch: 0665 loss_train: 0.6300 acc_train: 0.8214 loss_val: 0.6547 acc_val: 0.8133 time: 0.0700s\n",
      "Epoch: 0666 loss_train: 0.6219 acc_train: 0.8071 loss_val: 0.6547 acc_val: 0.8133 time: 0.0649s\n",
      "Epoch: 0667 loss_train: 0.6209 acc_train: 0.8214 loss_val: 0.6548 acc_val: 0.8167 time: 0.0702s\n",
      "Epoch: 0668 loss_train: 0.4903 acc_train: 0.8429 loss_val: 0.6547 acc_val: 0.8233 time: 0.0751s\n",
      "Epoch: 0669 loss_train: 0.6243 acc_train: 0.7857 loss_val: 0.6551 acc_val: 0.8133 time: 0.0649s\n",
      "Epoch: 0670 loss_train: 0.6077 acc_train: 0.8214 loss_val: 0.6553 acc_val: 0.8200 time: 0.0648s\n",
      "Epoch: 0671 loss_train: 0.6252 acc_train: 0.8143 loss_val: 0.6554 acc_val: 0.8200 time: 0.0732s\n",
      "Epoch: 0672 loss_train: 0.5177 acc_train: 0.8429 loss_val: 0.6556 acc_val: 0.8167 time: 0.0697s\n",
      "Epoch: 0673 loss_train: 0.6743 acc_train: 0.7786 loss_val: 0.6558 acc_val: 0.8167 time: 0.0654s\n",
      "Epoch: 0674 loss_train: 0.6426 acc_train: 0.7857 loss_val: 0.6556 acc_val: 0.8167 time: 0.0699s\n",
      "Epoch: 0675 loss_train: 0.5924 acc_train: 0.8500 loss_val: 0.6556 acc_val: 0.8167 time: 0.0701s\n",
      "Epoch: 0676 loss_train: 0.6771 acc_train: 0.7714 loss_val: 0.6557 acc_val: 0.8167 time: 0.0750s\n",
      "Epoch: 0677 loss_train: 0.5970 acc_train: 0.8000 loss_val: 0.6557 acc_val: 0.8200 time: 0.0799s\n",
      "Epoch: 0678 loss_train: 0.6392 acc_train: 0.7857 loss_val: 0.6556 acc_val: 0.8200 time: 0.0602s\n",
      "Epoch: 0679 loss_train: 0.6099 acc_train: 0.8500 loss_val: 0.6554 acc_val: 0.8133 time: 0.0654s\n",
      "Epoch: 0680 loss_train: 0.6374 acc_train: 0.7857 loss_val: 0.6552 acc_val: 0.8133 time: 0.0698s\n",
      "Epoch: 0681 loss_train: 0.5370 acc_train: 0.9000 loss_val: 0.6552 acc_val: 0.8200 time: 0.0705s\n",
      "Epoch: 0682 loss_train: 0.6144 acc_train: 0.8071 loss_val: 0.6555 acc_val: 0.8167 time: 0.0697s\n",
      "Epoch: 0683 loss_train: 0.6094 acc_train: 0.8214 loss_val: 0.6558 acc_val: 0.8167 time: 0.0800s\n",
      "Epoch: 0684 loss_train: 0.6539 acc_train: 0.7929 loss_val: 0.6560 acc_val: 0.8167 time: 0.0696s\n",
      "Epoch: 0685 loss_train: 0.6407 acc_train: 0.7786 loss_val: 0.6560 acc_val: 0.8167 time: 0.0704s\n",
      "Epoch: 0686 loss_train: 0.5621 acc_train: 0.8357 loss_val: 0.6556 acc_val: 0.8200 time: 0.0697s\n",
      "Epoch: 0687 loss_train: 0.6142 acc_train: 0.7929 loss_val: 0.6550 acc_val: 0.8200 time: 0.0601s\n",
      "Epoch: 0688 loss_train: 0.5876 acc_train: 0.8214 loss_val: 0.6544 acc_val: 0.8200 time: 0.0600s\n",
      "Epoch: 0689 loss_train: 0.6856 acc_train: 0.7857 loss_val: 0.6543 acc_val: 0.8233 time: 0.0703s\n",
      "Epoch: 0690 loss_train: 0.4693 acc_train: 0.8786 loss_val: 0.6541 acc_val: 0.8233 time: 0.0651s\n",
      "Epoch: 0691 loss_train: 0.6166 acc_train: 0.8286 loss_val: 0.6542 acc_val: 0.8233 time: 0.0647s\n",
      "Epoch: 0692 loss_train: 0.6994 acc_train: 0.7571 loss_val: 0.6543 acc_val: 0.8233 time: 0.0747s\n",
      "Epoch: 0693 loss_train: 0.6397 acc_train: 0.8000 loss_val: 0.6544 acc_val: 0.8233 time: 0.0599s\n",
      "Epoch: 0694 loss_train: 0.5473 acc_train: 0.8714 loss_val: 0.6542 acc_val: 0.8233 time: 0.0648s\n",
      "Epoch: 0695 loss_train: 0.6605 acc_train: 0.8071 loss_val: 0.6538 acc_val: 0.8233 time: 0.0750s\n",
      "Epoch: 0696 loss_train: 0.5394 acc_train: 0.8429 loss_val: 0.6535 acc_val: 0.8233 time: 0.0649s\n",
      "Epoch: 0697 loss_train: 0.5678 acc_train: 0.8286 loss_val: 0.6536 acc_val: 0.8233 time: 0.0698s\n",
      "Epoch: 0698 loss_train: 0.6574 acc_train: 0.7857 loss_val: 0.6536 acc_val: 0.8233 time: 0.0698s\n",
      "Epoch: 0699 loss_train: 0.6591 acc_train: 0.8143 loss_val: 0.6537 acc_val: 0.8167 time: 0.0599s\n",
      "Epoch: 0700 loss_train: 0.5794 acc_train: 0.7857 loss_val: 0.6538 acc_val: 0.8167 time: 0.0614s\n",
      "Epoch: 0701 loss_train: 0.5162 acc_train: 0.8429 loss_val: 0.6539 acc_val: 0.8167 time: 0.0801s\n",
      "Epoch: 0702 loss_train: 0.6224 acc_train: 0.8071 loss_val: 0.6542 acc_val: 0.8167 time: 0.0650s\n",
      "Epoch: 0703 loss_train: 0.5940 acc_train: 0.8357 loss_val: 0.6542 acc_val: 0.8167 time: 0.0652s\n",
      "Epoch: 0704 loss_train: 0.7196 acc_train: 0.7429 loss_val: 0.6543 acc_val: 0.8167 time: 0.0702s\n",
      "Epoch: 0705 loss_train: 0.5424 acc_train: 0.8357 loss_val: 0.6540 acc_val: 0.8167 time: 0.0607s\n",
      "Epoch: 0706 loss_train: 0.6126 acc_train: 0.7929 loss_val: 0.6542 acc_val: 0.8167 time: 0.0649s\n",
      "Epoch: 0707 loss_train: 0.6077 acc_train: 0.8357 loss_val: 0.6542 acc_val: 0.8200 time: 0.0699s\n",
      "Epoch: 0708 loss_train: 0.6088 acc_train: 0.8143 loss_val: 0.6544 acc_val: 0.8233 time: 0.0698s\n",
      "Epoch: 0709 loss_train: 0.6835 acc_train: 0.8000 loss_val: 0.6545 acc_val: 0.8233 time: 0.0653s\n",
      "Epoch: 0710 loss_train: 0.6097 acc_train: 0.8286 loss_val: 0.6552 acc_val: 0.8233 time: 0.0704s\n",
      "Epoch: 0711 loss_train: 0.5088 acc_train: 0.8500 loss_val: 0.6559 acc_val: 0.8233 time: 0.0601s\n",
      "Epoch: 0712 loss_train: 0.5360 acc_train: 0.8357 loss_val: 0.6565 acc_val: 0.8233 time: 0.0602s\n",
      "Epoch: 0713 loss_train: 0.6576 acc_train: 0.7786 loss_val: 0.6569 acc_val: 0.8233 time: 0.0800s\n",
      "Epoch: 0714 loss_train: 0.6351 acc_train: 0.8000 loss_val: 0.6575 acc_val: 0.8233 time: 0.0599s\n",
      "Epoch: 0715 loss_train: 0.5480 acc_train: 0.8571 loss_val: 0.6581 acc_val: 0.8233 time: 0.0648s\n",
      "Epoch: 0716 loss_train: 0.5508 acc_train: 0.8429 loss_val: 0.6591 acc_val: 0.8233 time: 0.0753s\n",
      "Epoch: 0717 loss_train: 0.5127 acc_train: 0.8429 loss_val: 0.6599 acc_val: 0.8167 time: 0.0646s\n",
      "Epoch: 0718 loss_train: 0.5988 acc_train: 0.8071 loss_val: 0.6606 acc_val: 0.8167 time: 0.0651s\n",
      "Epoch: 0719 loss_train: 0.5742 acc_train: 0.8214 loss_val: 0.6608 acc_val: 0.8167 time: 0.0739s\n",
      "Epoch: 0720 loss_train: 0.6026 acc_train: 0.8143 loss_val: 0.6609 acc_val: 0.8167 time: 0.0647s\n",
      "Epoch: 0721 loss_train: 0.5445 acc_train: 0.8429 loss_val: 0.6607 acc_val: 0.8133 time: 0.0649s\n",
      "Epoch: 0722 loss_train: 0.6387 acc_train: 0.8214 loss_val: 0.6605 acc_val: 0.8133 time: 0.0700s\n",
      "Epoch: 0723 loss_train: 0.5707 acc_train: 0.8143 loss_val: 0.6600 acc_val: 0.8133 time: 0.0680s\n",
      "Epoch: 0724 loss_train: 0.5913 acc_train: 0.8357 loss_val: 0.6592 acc_val: 0.8133 time: 0.0649s\n",
      "Epoch: 0725 loss_train: 0.7311 acc_train: 0.7429 loss_val: 0.6583 acc_val: 0.8133 time: 0.0751s\n",
      "Epoch: 0726 loss_train: 0.7099 acc_train: 0.7429 loss_val: 0.6573 acc_val: 0.8133 time: 0.0602s\n",
      "Epoch: 0727 loss_train: 0.6034 acc_train: 0.8214 loss_val: 0.6567 acc_val: 0.8133 time: 0.0650s\n",
      "Epoch: 0728 loss_train: 0.6222 acc_train: 0.8357 loss_val: 0.6561 acc_val: 0.8133 time: 0.0701s\n",
      "Epoch: 0729 loss_train: 0.5883 acc_train: 0.8214 loss_val: 0.6553 acc_val: 0.8133 time: 0.0645s\n",
      "Epoch: 0730 loss_train: 0.6139 acc_train: 0.8214 loss_val: 0.6549 acc_val: 0.8133 time: 0.0648s\n",
      "Epoch: 0731 loss_train: 0.4971 acc_train: 0.8857 loss_val: 0.6550 acc_val: 0.8133 time: 0.0696s\n",
      "Epoch: 0732 loss_train: 0.5843 acc_train: 0.8571 loss_val: 0.6548 acc_val: 0.8133 time: 0.0650s\n",
      "Epoch: 0733 loss_train: 0.6084 acc_train: 0.8143 loss_val: 0.6546 acc_val: 0.8100 time: 0.0653s\n",
      "Epoch: 0734 loss_train: 0.5297 acc_train: 0.8643 loss_val: 0.6541 acc_val: 0.8067 time: 0.0699s\n",
      "Epoch: 0735 loss_train: 0.5645 acc_train: 0.8429 loss_val: 0.6538 acc_val: 0.8067 time: 0.0702s\n",
      "Epoch: 0736 loss_train: 0.6190 acc_train: 0.8286 loss_val: 0.6536 acc_val: 0.8067 time: 0.0601s\n",
      "Epoch: 0737 loss_train: 0.6484 acc_train: 0.8000 loss_val: 0.6531 acc_val: 0.8067 time: 0.0701s\n",
      "Epoch: 0738 loss_train: 0.5604 acc_train: 0.8571 loss_val: 0.6524 acc_val: 0.8067 time: 0.0601s\n",
      "Epoch: 0739 loss_train: 0.5811 acc_train: 0.8071 loss_val: 0.6516 acc_val: 0.8067 time: 0.0707s\n",
      "Epoch: 0740 loss_train: 0.7110 acc_train: 0.7286 loss_val: 0.6512 acc_val: 0.8067 time: 0.0599s\n",
      "Epoch: 0741 loss_train: 0.6564 acc_train: 0.8143 loss_val: 0.6511 acc_val: 0.8067 time: 0.0700s\n",
      "Epoch: 0742 loss_train: 0.5768 acc_train: 0.8429 loss_val: 0.6509 acc_val: 0.8067 time: 0.0698s\n",
      "Epoch: 0743 loss_train: 0.5473 acc_train: 0.8643 loss_val: 0.6504 acc_val: 0.8067 time: 0.0701s\n",
      "Epoch: 0744 loss_train: 0.5624 acc_train: 0.8214 loss_val: 0.6498 acc_val: 0.8133 time: 0.0696s\n",
      "Epoch: 0745 loss_train: 0.6202 acc_train: 0.8071 loss_val: 0.6492 acc_val: 0.8133 time: 0.0648s\n",
      "Epoch: 0746 loss_train: 0.6338 acc_train: 0.8071 loss_val: 0.6487 acc_val: 0.8133 time: 0.0651s\n",
      "Epoch: 0747 loss_train: 0.5385 acc_train: 0.8643 loss_val: 0.6483 acc_val: 0.8133 time: 0.0675s\n",
      "Epoch: 0748 loss_train: 0.6290 acc_train: 0.8357 loss_val: 0.6484 acc_val: 0.8133 time: 0.0595s\n",
      "Epoch: 0749 loss_train: 0.5793 acc_train: 0.8286 loss_val: 0.6481 acc_val: 0.8133 time: 0.0600s\n",
      "Epoch: 0750 loss_train: 0.6570 acc_train: 0.7857 loss_val: 0.6483 acc_val: 0.8133 time: 0.0697s\n",
      "Epoch: 0751 loss_train: 0.6601 acc_train: 0.7786 loss_val: 0.6490 acc_val: 0.8133 time: 0.0700s\n",
      "Epoch: 0752 loss_train: 0.5579 acc_train: 0.8500 loss_val: 0.6494 acc_val: 0.8133 time: 0.0600s\n",
      "Epoch: 0753 loss_train: 0.6761 acc_train: 0.8071 loss_val: 0.6500 acc_val: 0.8133 time: 0.0735s\n",
      "Epoch: 0754 loss_train: 0.5573 acc_train: 0.8429 loss_val: 0.6504 acc_val: 0.8133 time: 0.0667s\n",
      "Epoch: 0755 loss_train: 0.6760 acc_train: 0.8000 loss_val: 0.6509 acc_val: 0.8133 time: 0.0698s\n",
      "Epoch: 0756 loss_train: 0.5931 acc_train: 0.8214 loss_val: 0.6513 acc_val: 0.8133 time: 0.0700s\n",
      "Epoch: 0757 loss_train: 0.5225 acc_train: 0.8500 loss_val: 0.6515 acc_val: 0.8133 time: 0.0703s\n",
      "Epoch: 0758 loss_train: 0.5754 acc_train: 0.8357 loss_val: 0.6520 acc_val: 0.8133 time: 0.0698s\n",
      "Epoch: 0759 loss_train: 0.5066 acc_train: 0.8571 loss_val: 0.6525 acc_val: 0.8133 time: 0.0747s\n",
      "Epoch: 0760 loss_train: 0.5150 acc_train: 0.8357 loss_val: 0.6527 acc_val: 0.8133 time: 0.0653s\n",
      "Epoch: 0761 loss_train: 0.5319 acc_train: 0.8643 loss_val: 0.6530 acc_val: 0.8133 time: 0.0599s\n",
      "Epoch: 0762 loss_train: 0.5574 acc_train: 0.8357 loss_val: 0.6532 acc_val: 0.8133 time: 0.0697s\n",
      "Epoch: 0763 loss_train: 0.5902 acc_train: 0.8429 loss_val: 0.6534 acc_val: 0.8133 time: 0.0726s\n",
      "Epoch: 0764 loss_train: 0.6357 acc_train: 0.8143 loss_val: 0.6537 acc_val: 0.8133 time: 0.0650s\n",
      "Epoch: 0765 loss_train: 0.6663 acc_train: 0.8214 loss_val: 0.6541 acc_val: 0.8133 time: 0.0660s\n",
      "Epoch: 0766 loss_train: 0.5685 acc_train: 0.8357 loss_val: 0.6546 acc_val: 0.8167 time: 0.0700s\n",
      "Epoch: 0767 loss_train: 0.5067 acc_train: 0.8571 loss_val: 0.6551 acc_val: 0.8200 time: 0.0650s\n",
      "Epoch: 0768 loss_train: 0.6050 acc_train: 0.8214 loss_val: 0.6552 acc_val: 0.8200 time: 0.0603s\n",
      "Epoch: 0769 loss_train: 0.5513 acc_train: 0.8500 loss_val: 0.6554 acc_val: 0.8200 time: 0.0702s\n",
      "Epoch: 0770 loss_train: 0.5476 acc_train: 0.8071 loss_val: 0.6556 acc_val: 0.8200 time: 0.0698s\n",
      "Epoch: 0771 loss_train: 0.6430 acc_train: 0.8286 loss_val: 0.6559 acc_val: 0.8200 time: 0.0703s\n",
      "Epoch: 0772 loss_train: 0.5551 acc_train: 0.8143 loss_val: 0.6563 acc_val: 0.8200 time: 0.0749s\n",
      "Epoch: 0773 loss_train: 0.6923 acc_train: 0.8000 loss_val: 0.6566 acc_val: 0.8200 time: 0.0650s\n",
      "Epoch: 0774 loss_train: 0.5526 acc_train: 0.8500 loss_val: 0.6570 acc_val: 0.8200 time: 0.0651s\n",
      "Epoch: 0775 loss_train: 0.6408 acc_train: 0.8071 loss_val: 0.6577 acc_val: 0.8233 time: 0.0747s\n",
      "Epoch: 0776 loss_train: 0.4933 acc_train: 0.8786 loss_val: 0.6580 acc_val: 0.8233 time: 0.0658s\n",
      "Epoch: 0777 loss_train: 0.6422 acc_train: 0.8286 loss_val: 0.6581 acc_val: 0.8267 time: 0.0652s\n",
      "Epoch: 0778 loss_train: 0.5722 acc_train: 0.8214 loss_val: 0.6586 acc_val: 0.8267 time: 0.0702s\n",
      "Epoch: 0779 loss_train: 0.6185 acc_train: 0.7857 loss_val: 0.6592 acc_val: 0.8267 time: 0.0699s\n",
      "Epoch: 0780 loss_train: 0.5897 acc_train: 0.8071 loss_val: 0.6596 acc_val: 0.8267 time: 0.0649s\n",
      "Epoch: 0781 loss_train: 0.6510 acc_train: 0.8143 loss_val: 0.6598 acc_val: 0.8267 time: 0.0701s\n",
      "Epoch: 0782 loss_train: 0.5893 acc_train: 0.8143 loss_val: 0.6599 acc_val: 0.8267 time: 0.0723s\n",
      "Epoch: 0783 loss_train: 0.6362 acc_train: 0.8286 loss_val: 0.6602 acc_val: 0.8267 time: 0.0675s\n",
      "Epoch: 0784 loss_train: 0.6463 acc_train: 0.8143 loss_val: 0.6603 acc_val: 0.8267 time: 0.0602s\n",
      "Epoch: 0785 loss_train: 0.6489 acc_train: 0.7643 loss_val: 0.6606 acc_val: 0.8267 time: 0.0718s\n",
      "Epoch: 0786 loss_train: 0.6253 acc_train: 0.8143 loss_val: 0.6615 acc_val: 0.8267 time: 0.0683s\n",
      "Epoch: 0787 loss_train: 0.5952 acc_train: 0.8071 loss_val: 0.6622 acc_val: 0.8300 time: 0.0701s\n",
      "Epoch: 0788 loss_train: 0.6562 acc_train: 0.8071 loss_val: 0.6629 acc_val: 0.8300 time: 0.0649s\n",
      "Epoch: 0789 loss_train: 0.6989 acc_train: 0.7929 loss_val: 0.6636 acc_val: 0.8300 time: 0.0702s\n",
      "Epoch: 0790 loss_train: 0.6612 acc_train: 0.8000 loss_val: 0.6639 acc_val: 0.8300 time: 0.0700s\n",
      "Epoch: 0791 loss_train: 0.6519 acc_train: 0.8000 loss_val: 0.6642 acc_val: 0.8300 time: 0.0699s\n",
      "Epoch: 0792 loss_train: 0.6303 acc_train: 0.8071 loss_val: 0.6645 acc_val: 0.8300 time: 0.0701s\n",
      "Epoch: 0793 loss_train: 0.6054 acc_train: 0.8000 loss_val: 0.6647 acc_val: 0.8300 time: 0.0764s\n",
      "Epoch: 0794 loss_train: 0.5367 acc_train: 0.8357 loss_val: 0.6651 acc_val: 0.8267 time: 0.0636s\n",
      "Epoch: 0795 loss_train: 0.5591 acc_train: 0.8357 loss_val: 0.6654 acc_val: 0.8267 time: 0.0700s\n",
      "Epoch: 0796 loss_train: 0.6181 acc_train: 0.7857 loss_val: 0.6656 acc_val: 0.8267 time: 0.0752s\n",
      "Epoch: 0797 loss_train: 0.6109 acc_train: 0.8500 loss_val: 0.6658 acc_val: 0.8267 time: 0.0645s\n",
      "Epoch: 0798 loss_train: 0.6317 acc_train: 0.8143 loss_val: 0.6661 acc_val: 0.8233 time: 0.0701s\n",
      "Epoch: 0799 loss_train: 0.6006 acc_train: 0.8571 loss_val: 0.6663 acc_val: 0.8233 time: 0.0749s\n",
      "Epoch: 0800 loss_train: 0.4983 acc_train: 0.9071 loss_val: 0.6664 acc_val: 0.8233 time: 0.0651s\n",
      "Epoch: 0801 loss_train: 0.6755 acc_train: 0.7857 loss_val: 0.6661 acc_val: 0.8233 time: 0.0649s\n",
      "Epoch: 0802 loss_train: 0.6101 acc_train: 0.8429 loss_val: 0.6659 acc_val: 0.8233 time: 0.0800s\n",
      "Epoch: 0803 loss_train: 0.6197 acc_train: 0.8214 loss_val: 0.6660 acc_val: 0.8200 time: 0.0672s\n",
      "Epoch: 0804 loss_train: 0.6161 acc_train: 0.8071 loss_val: 0.6658 acc_val: 0.8200 time: 0.0597s\n",
      "Epoch: 0805 loss_train: 0.4940 acc_train: 0.8571 loss_val: 0.6658 acc_val: 0.8200 time: 0.0801s\n",
      "Epoch: 0806 loss_train: 0.5992 acc_train: 0.7929 loss_val: 0.6655 acc_val: 0.8233 time: 0.0650s\n",
      "Epoch: 0807 loss_train: 0.6076 acc_train: 0.8143 loss_val: 0.6652 acc_val: 0.8200 time: 0.0599s\n",
      "Epoch: 0808 loss_train: 0.6554 acc_train: 0.8071 loss_val: 0.6651 acc_val: 0.8200 time: 0.0599s\n",
      "Epoch: 0809 loss_train: 0.5497 acc_train: 0.8571 loss_val: 0.6653 acc_val: 0.8167 time: 0.0698s\n",
      "Epoch: 0810 loss_train: 0.5195 acc_train: 0.8643 loss_val: 0.6652 acc_val: 0.8167 time: 0.0601s\n",
      "Epoch: 0811 loss_train: 0.4956 acc_train: 0.8643 loss_val: 0.6646 acc_val: 0.8167 time: 0.0653s\n",
      "Epoch: 0812 loss_train: 0.6667 acc_train: 0.7786 loss_val: 0.6639 acc_val: 0.8167 time: 0.0647s\n",
      "Epoch: 0813 loss_train: 0.5708 acc_train: 0.8357 loss_val: 0.6633 acc_val: 0.8167 time: 0.0749s\n",
      "Epoch: 0814 loss_train: 0.5822 acc_train: 0.8143 loss_val: 0.6630 acc_val: 0.8200 time: 0.0599s\n",
      "Epoch: 0815 loss_train: 0.5781 acc_train: 0.8214 loss_val: 0.6625 acc_val: 0.8200 time: 0.0700s\n",
      "Epoch: 0816 loss_train: 0.6633 acc_train: 0.7786 loss_val: 0.6617 acc_val: 0.8167 time: 0.0699s\n",
      "Epoch: 0817 loss_train: 0.5675 acc_train: 0.8571 loss_val: 0.6611 acc_val: 0.8200 time: 0.0652s\n",
      "Epoch: 0818 loss_train: 0.6359 acc_train: 0.7857 loss_val: 0.6605 acc_val: 0.8200 time: 0.0651s\n",
      "Epoch: 0819 loss_train: 0.5844 acc_train: 0.8357 loss_val: 0.6602 acc_val: 0.8200 time: 0.0700s\n",
      "Epoch: 0820 loss_train: 0.6715 acc_train: 0.8143 loss_val: 0.6601 acc_val: 0.8200 time: 0.0599s\n",
      "Epoch: 0821 loss_train: 0.5513 acc_train: 0.8214 loss_val: 0.6599 acc_val: 0.8200 time: 0.0646s\n",
      "Epoch: 0822 loss_train: 0.7042 acc_train: 0.8071 loss_val: 0.6595 acc_val: 0.8200 time: 0.0703s\n",
      "Epoch: 0823 loss_train: 0.5195 acc_train: 0.8500 loss_val: 0.6589 acc_val: 0.8200 time: 0.0698s\n",
      "Epoch: 0824 loss_train: 0.7061 acc_train: 0.7500 loss_val: 0.6587 acc_val: 0.8233 time: 0.0613s\n",
      "Epoch: 0825 loss_train: 0.6562 acc_train: 0.8357 loss_val: 0.6584 acc_val: 0.8233 time: 0.0704s\n",
      "Epoch: 0826 loss_train: 0.6045 acc_train: 0.8429 loss_val: 0.6583 acc_val: 0.8200 time: 0.0646s\n",
      "Epoch: 0827 loss_train: 0.6788 acc_train: 0.7714 loss_val: 0.6581 acc_val: 0.8200 time: 0.0600s\n",
      "Epoch: 0828 loss_train: 0.7031 acc_train: 0.8286 loss_val: 0.6580 acc_val: 0.8167 time: 0.0701s\n",
      "Epoch: 0829 loss_train: 0.6624 acc_train: 0.7714 loss_val: 0.6579 acc_val: 0.8133 time: 0.0699s\n",
      "Epoch: 0830 loss_train: 0.6593 acc_train: 0.8000 loss_val: 0.6573 acc_val: 0.8133 time: 0.0652s\n",
      "Epoch: 0831 loss_train: 0.6263 acc_train: 0.7929 loss_val: 0.6569 acc_val: 0.8167 time: 0.0705s\n",
      "Epoch: 0832 loss_train: 0.6852 acc_train: 0.7571 loss_val: 0.6567 acc_val: 0.8167 time: 0.0698s\n",
      "Epoch: 0833 loss_train: 0.4965 acc_train: 0.8500 loss_val: 0.6565 acc_val: 0.8167 time: 0.0600s\n",
      "Epoch: 0834 loss_train: 0.6709 acc_train: 0.7643 loss_val: 0.6562 acc_val: 0.8167 time: 0.0645s\n",
      "Epoch: 0835 loss_train: 0.5929 acc_train: 0.8429 loss_val: 0.6558 acc_val: 0.8200 time: 0.0751s\n",
      "Epoch: 0836 loss_train: 0.5837 acc_train: 0.8286 loss_val: 0.6558 acc_val: 0.8233 time: 0.0700s\n",
      "Epoch: 0837 loss_train: 0.5935 acc_train: 0.8143 loss_val: 0.6558 acc_val: 0.8233 time: 0.0649s\n",
      "Epoch: 0838 loss_train: 0.5130 acc_train: 0.8500 loss_val: 0.6559 acc_val: 0.8233 time: 0.0648s\n",
      "Epoch: 0839 loss_train: 0.6377 acc_train: 0.8214 loss_val: 0.6559 acc_val: 0.8233 time: 0.0696s\n",
      "Epoch: 0840 loss_train: 0.6191 acc_train: 0.8143 loss_val: 0.6560 acc_val: 0.8233 time: 0.0700s\n",
      "Epoch: 0841 loss_train: 0.6423 acc_train: 0.8286 loss_val: 0.6558 acc_val: 0.8267 time: 0.0603s\n",
      "Epoch: 0842 loss_train: 0.5479 acc_train: 0.8429 loss_val: 0.6556 acc_val: 0.8267 time: 0.0651s\n",
      "Epoch: 0843 loss_train: 0.5967 acc_train: 0.8071 loss_val: 0.6555 acc_val: 0.8233 time: 0.0747s\n",
      "Epoch: 0844 loss_train: 0.6600 acc_train: 0.7929 loss_val: 0.6555 acc_val: 0.8233 time: 0.0650s\n",
      "Epoch: 0845 loss_train: 0.5205 acc_train: 0.8571 loss_val: 0.6553 acc_val: 0.8233 time: 0.0649s\n",
      "Epoch: 0846 loss_train: 0.5934 acc_train: 0.8357 loss_val: 0.6550 acc_val: 0.8233 time: 0.0752s\n",
      "Epoch: 0847 loss_train: 0.4987 acc_train: 0.8786 loss_val: 0.6547 acc_val: 0.8233 time: 0.0697s\n",
      "Epoch: 0848 loss_train: 0.6772 acc_train: 0.7857 loss_val: 0.6543 acc_val: 0.8233 time: 0.0699s\n",
      "Epoch: 0849 loss_train: 0.6571 acc_train: 0.7857 loss_val: 0.6539 acc_val: 0.8200 time: 0.0704s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 65.6500s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(args.epochs):\n",
    "    loss_values.append(train(epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == args.patience:\n",
    "        break\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:44:45.895788700Z",
     "start_time": "2023-10-25T03:43:40.235879900Z"
    }
   },
   "id": "8881dd3aecfcd007"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构建测试流程"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2129cec99abd5bc"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.data.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:44:45.915927700Z",
     "start_time": "2023-10-25T03:44:45.895788700Z"
    }
   },
   "id": "6bd248f703d4eb31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 开始测试"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5492d5dc7f413725"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.6648 accuracy= 0.8410\n"
     ]
    }
   ],
   "source": [
    "compute_test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:44:45.935787500Z",
     "start_time": "2023-10-25T03:44:45.905529700Z"
    }
   },
   "id": "a99692d5863ab906"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
