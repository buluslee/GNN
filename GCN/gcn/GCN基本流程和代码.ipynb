{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 简述\n",
    "这是图神经网络的最基础的模型GCN，本代码是对由Thomas Kipf和Max Welling在2016年提出的。他们的文章标题为“Semi-Supervised Classification with Graph Convolutional Networks”开源的代码进行一个详解\n",
    "## 数据集\n",
    "本代码使用的是cora数据集，\n",
    "Cora数据集是自然语言处理和图神经网络研究中经常使用的一个数据集，主要用于文献分类任务。以下是对Cora数据集的简介：\n",
    "内容：Cora数据集包含2,708个科学出版物，这些出版物属于7个类别。\n",
    "特点：\n",
    "节点：每个节点代表一篇科学出版物。\n",
    "边：如果一篇文章引用了另一篇文章，它们之间就有一条边。\n",
    "特征：每篇出版物都有一个词向量，该向量基于出版物中的词的出现或不出现。原始的Cora数据集使用了一个1433维的二进制词向量来表示每个文档。\n",
    "标签：数据集中的出版物分为7个类别，包括：“Case_Based”、“Genetic_Algorithms”、“Neural_Networks”、“Probabilistic_Methods”、“Reinforcement_Learning”、“Rule_Learning”和“Theory”。\n",
    "## 关键点\n",
    "1.理解图结构的形式\n",
    "2.如何使用邻接矩阵实现其图结构形式\n",
    "3.GCN卷积是如何实现节点特征更新的\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f805a49dbc64c6c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据预处理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "392f300a1bdee6c2"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:17.514332300Z",
     "start_time": "2023-10-25T03:16:16.063085800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['31336' '0' '0' ... '0' '0' 'Neural_Networks']\n",
      " ['1061127' '0' '0' ... '0' '0' 'Rule_Learning']\n",
      " ['1106406' '0' '0' ... '0' '0' 'Reinforcement_Learning']\n",
      " ...\n",
      " ['1128978' '0' '0' ... '0' '0' 'Genetic_Algorithms']\n",
      " ['117328' '0' '0' ... '0' '0' 'Case_Based']\n",
      " ['24043' '0' '0' ... '0' '0' 'Neural_Networks']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "##以数组形式提取cora。content\n",
    "idx_features_labels = np.genfromtxt('C:\\\\Users\\\\Wei Zhou\\\\Desktop\\\\test\\\\图神经网络几个算法\\\\gcn\\\\data\\\\cora\\\\cora.content',dtype=np.dtype(str))\n",
    "print(idx_features_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "##提取节点特征\n",
    "features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)  #sp.csr_matrix建立稀疏矩阵\n",
    "print(features.A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.571699900Z",
     "start_time": "2023-10-25T03:16:17.514332300Z"
    }
   },
   "id": "970e1e24e45624f8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "## 引入one hot 编码\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.583656100Z",
     "start_time": "2023-10-25T03:16:18.571699900Z"
    }
   },
   "id": "a7bca49ae3f81b4c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "## 对标签进行one hot 编码\n",
    "labels = encode_onehot(idx_features_labels[:, -1]) \n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.631739200Z",
     "start_time": "2023-10-25T03:16:18.583656100Z"
    }
   },
   "id": "99b716e9879922d2"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f46b0f574a9577fb"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  31336 1061127 1106406 ... 1128978  117328   24043]\n"
     ]
    }
   ],
   "source": [
    "## 取节点的索引\n",
    "idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "print(idx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.656864Z",
     "start_time": "2023-10-25T03:16:18.601657600Z"
    }
   },
   "id": "99abef1b4b903b03"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{31336: 0, 1061127: 1, 1106406: 2, 13195: 3, 37879: 4, 1126012: 5, 1107140: 6, 1102850: 7, 31349: 8, 1106418: 9, 1123188: 10, 1128990: 11, 109323: 12, 217139: 13, 31353: 14, 32083: 15, 1126029: 16, 1118017: 17, 49482: 18, 753265: 19, 249858: 20, 1113739: 21, 48766: 22, 646195: 23, 1126050: 24, 59626: 25, 340299: 26, 354004: 27, 242637: 28, 1106492: 29, 74975: 30, 1152272: 31, 100701: 32, 66982: 33, 13960: 34, 13966: 35, 66990: 36, 182093: 37, 182094: 38, 13972: 39, 13982: 40, 16819: 41, 273152: 42, 237521: 43, 1153703: 44, 32872: 45, 284025: 46, 218666: 47, 16843: 48, 1153724: 49, 1153728: 50, 158098: 51, 8699: 52, 1134865: 53, 28456: 54, 248425: 55, 1112319: 56, 28471: 57, 175548: 58, 696345: 59, 28485: 60, 1139195: 61, 35778: 62, 28491: 63, 310530: 64, 1153784: 65, 1481: 66, 1153786: 67, 13212: 68, 1111614: 69, 5055: 70, 4329: 71, 330148: 72, 1105062: 73, 4330: 74, 5062: 75, 4335: 76, 158812: 77, 40124: 78, 1103610: 79, 688361: 80, 302545: 81, 20534: 82, 1031453: 83, 5086: 84, 193742: 85, 58268: 86, 424: 87, 40151: 88, 636098: 89, 260121: 90, 950052: 91, 434: 92, 1131270: 93, 1131274: 94, 1131277: 95, 1110947: 96, 662279: 97, 1139928: 98, 153063: 99, 134199: 100, 641956: 101, 20584: 102, 1130567: 103, 171225: 104, 714879: 105, 37998: 106, 50336: 107, 50337: 108, 15429: 109, 23448: 110, 1122574: 111, 1110998: 112, 853150: 113, 15431: 114, 646286: 115, 1152307: 116, 1115291: 117, 1106547: 118, 68463: 119, 59715: 120, 69198: 121, 7272: 122, 163235: 123, 7276: 124, 34315: 125, 644843: 126, 7297: 127, 628815: 128, 35061: 129, 68495: 130, 1136310: 131, 18313: 132, 34355: 133, 45212: 134, 1153091: 135, 8703: 136, 126920: 137, 126927: 138, 595157: 139, 140005: 140, 1117476: 141, 59798: 142, 219446: 143, 44514: 144, 287787: 145, 157401: 146, 1154500: 147, 682666: 148, 399173: 149, 198866: 150, 51834: 151, 200630: 152, 782486: 153, 1136393: 154, 137849: 155, 1153811: 156, 24966: 157, 11148: 158, 51866: 159, 24974: 160, 137868: 161, 28542: 162, 35: 163, 116021: 164, 348305: 165, 10430: 166, 39403: 167, 40: 168, 282700: 169, 1105116: 170, 35854: 171, 63477: 172, 124064: 173, 1120431: 174, 949318: 175, 649944: 176, 63486: 177, 1153866: 178, 1140040: 179, 1112426: 180, 239800: 181, 1131314: 182, 1153891: 183, 1129835: 184, 310653: 185, 1130600: 186, 1111733: 187, 210871: 188, 210872: 189, 1132083: 190, 132806: 191, 12631: 192, 12638: 193, 38771: 194, 232605: 195, 232606: 196, 1107312: 197, 1114605: 198, 68505: 199, 133553: 200, 144408: 201, 23502: 202, 1108050: 203, 23507: 204, 83826: 205, 133563: 206, 85299: 207, 49660: 208, 593060: 209, 341188: 210, 714975: 211, 1115375: 212, 95435: 213, 145176: 214, 1113934: 215, 1132809: 216, 22835: 217, 1153148: 218, 41714: 219, 1118245: 220, 1152436: 221, 1153166: 222, 1153169: 223, 38000: 224, 1152448: 225, 1137140: 226, 30895: 227, 5966: 228, 1136422: 229, 27174: 230, 1128407: 231, 1124844: 232, 1153195: 233, 1113995: 234, 1136442: 235, 8821: 236, 46079: 237, 119761: 238, 1111052: 239, 315789: 240, 1108841: 241, 1135746: 242, 100935: 243, 353541: 244, 60682: 245, 253762: 246, 8872: 247, 714260: 248, 137956: 249, 35922: 250, 2354: 251, 168410: 252, 346292: 253, 1153933: 254, 1119751: 255, 17798: 256, 400356: 257, 10531: 258, 1110390: 259, 714289: 260, 733167: 261, 81714: 262, 428610: 263, 552469: 264, 164885: 265, 81722: 266, 111866: 267, 194617: 268, 93318: 269, 134307: 270, 203646: 271, 367312: 272, 650814: 273, 93320: 274, 134315: 275, 134316: 276, 976334: 277, 1095507: 278, 134320: 279, 662416: 280, 194645: 281, 1131421: 282, 161221: 283, 38839: 284, 38846: 285, 133615: 286, 1112574: 287, 521207: 288, 3828: 289, 593105: 290, 390693: 291, 642847: 292, 1122704: 293, 4584: 294, 7419: 295, 30901: 296, 1115456: 297, 7432: 298, 573553: 299, 1022969: 300, 143801: 301, 612306: 302, 417017: 303, 396412: 304, 1107455: 305, 91975: 306, 180187: 307, 27203: 308, 1152508: 309, 69392: 310, 1118332: 311, 189577: 312, 1114777: 313, 75969: 314, 1132922: 315, 1153254: 316, 1117618: 317, 6767: 318, 27241: 319, 27246: 320, 95589: 321, 6771: 322, 86840: 323, 108962: 324, 6786: 325, 108963: 326, 108974: 327, 1117653: 328, 1152569: 329, 1132968: 330, 370366: 331, 108983: 332, 399339: 333, 64319: 334, 1110426: 335, 1102407: 336, 1127812: 337, 1128542: 338, 65057: 339, 159084: 340, 159085: 341, 65074: 342, 33895: 343, 2440: 344, 1717: 345, 249421: 346, 3187: 347, 591016: 348, 1110494: 349, 29492: 350, 400473: 351, 644334: 352, 949511: 353, 205192: 354, 763009: 355, 169280: 356, 1120643: 357, 645088: 358, 5348: 359, 124296: 360, 1121398: 361, 950305: 362, 567018: 363, 52000: 364, 52003: 365, 52007: 366, 58540: 367, 436796: 368, 948846: 369, 8213: 370, 671293: 371, 1131550: 372, 899119: 373, 1105394: 374, 85452: 375, 1112686: 376, 69418: 377, 8224: 378, 145315: 379, 575077: 380, 20850: 381, 44017: 382, 1135125: 383, 286562: 384, 1123553: 385, 1135137: 386, 325314: 387, 662572: 388, 159897: 389, 1130856: 390, 96335: 391, 755082: 392, 1123576: 393, 1103979: 394, 593260: 395, 601567: 396, 1119140: 397, 189655: 398, 31769: 399, 1107567: 400, 88356: 401, 1033: 402, 1034: 403, 1106849: 404, 16470: 405, 35343: 406, 16471: 407, 1154074: 408, 16476: 409, 23774: 410, 16485: 411, 136665: 412, 94953: 413, 9708: 414, 38205: 415, 645897: 416, 216877: 417, 18619: 418, 559804: 419, 6898: 420, 166420: 421, 787016: 422, 73146: 423, 1136634: 424, 1111230: 425, 3218: 426, 3229: 427, 193347: 428, 84020: 429, 3231: 430, 52847: 431, 193352: 432, 193354: 433, 1110531: 434, 686532: 435, 711598: 436, 1063773: 437, 3243: 438, 78994: 439, 181782: 440, 284414: 441, 114189: 442, 686559: 443, 253971: 444, 1106103: 445, 1114125: 446, 75318: 447, 45599: 448, 97892: 449, 446271: 450, 1106112: 451, 280876: 452, 12182: 453, 175909: 454, 64484: 455, 6125: 456, 1120713: 457, 1114153: 458, 12197: 459, 248823: 460, 919885: 461, 94229: 462, 1120731: 463, 23069: 464, 6151: 465, 6155: 466, 23070: 467, 644448: 468, 1112723: 469, 31097: 470, 6169: 471, 1106172: 472, 6170: 473, 211875: 474, 1109017: 475, 5454: 476, 6184: 477, 10796: 478, 10798: 479, 1120777: 480, 86258: 481, 154134: 482, 6196: 483, 20920: 484, 20923: 485, 22386: 486, 1131639: 487, 77515: 488, 93555: 489, 17201: 490, 644494: 491, 17208: 492, 1125082: 493, 1131647: 494, 74698: 495, 13652: 496, 20942: 497, 390894: 498, 390896: 499, 1125092: 500, 13656: 501, 1116347: 502, 13658: 503, 114966: 504, 120013: 505, 1117089: 506, 57948: 507, 334153: 508, 160732: 509, 1154103: 510, 12946: 511, 1104787: 512, 17242: 513, 321861: 514, 189721: 515, 1119211: 516, 12960: 517, 95718: 518, 6910: 519, 180373: 520, 6917: 521, 358884: 522, 887: 523, 180399: 524, 358894: 525, 1154169: 526, 120084: 527, 1120019: 528, 1152711: 529, 1154176: 530, 424540: 531, 1118546: 532, 643003: 533, 112099: 534, 1104007: 535, 1120049: 536, 175256: 537, 45605: 538, 15889: 539, 35490: 540, 221302: 541, 562123: 542, 1104031: 543, 1129442: 544, 1129443: 545, 1137466: 546, 328370: 547, 1103315: 548, 12210: 549, 1104055: 550, 64519: 551, 114: 552, 1109873: 553, 128: 554, 12238: 555, 1112099: 556, 18774: 557, 18777: 558, 130: 559, 23116: 560, 948299: 561, 6209: 562, 197054: 563, 6210: 564, 6213: 565, 6214: 566, 6216: 567, 6217: 568, 2653: 569, 2658: 570, 753047: 571, 188318: 572, 74700: 573, 67415: 574, 6220: 575, 2665: 576, 28957: 577, 143323: 578, 340075: 579, 1949: 580, 1953: 581, 1955: 582, 1959: 583, 390922: 584, 22431: 585, 1113541: 586, 1132418: 587, 628500: 588, 648106: 589, 1104809: 590, 4804: 591, 648112: 592, 33301: 593, 33303: 594, 267824: 595, 1138970: 596, 13717: 597, 1131719: 598, 1120866: 599, 1106287: 600, 755217: 601, 647408: 602, 1116410: 603, 1132459: 604, 1105574: 605, 1133196: 606, 307336: 607, 906: 608, 1131745: 609, 1131748: 610, 910: 611, 943: 612, 31927: 613, 101261: 614, 101263: 615, 31932: 616, 779960: 617, 1135358: 618, 1154230: 619, 1135368: 620, 28227: 621, 32688: 622, 189856: 623, 27510: 624, 27514: 625, 1154276: 626, 27530: 627, 1152821: 628, 28265: 629, 103430: 630, 27543: 631, 39126: 632, 28278: 633, 39131: 634, 10169: 635, 28287: 636, 1129518: 637, 1272: 638, 194223: 639, 10177: 640, 18811: 641, 18812: 642, 73327: 643, 1117942: 644, 15984: 645, 202522: 646, 1152858: 647, 1152859: 648, 10183: 649, 81350: 650, 259126: 651, 13024: 652, 1120170: 653, 46452: 654, 26850: 655, 18832: 656, 18833: 657, 82098: 658, 103482: 659, 158614: 660, 46468: 661, 71904: 662, 80656: 663, 29708: 664, 1128839: 665, 1128846: 666, 12330: 667, 240321: 668, 1128853: 669, 219976: 670, 38480: 671, 12350: 672, 1104191: 673, 7022: 674, 63931: 675, 68224: 676, 1110768: 677, 384428: 678, 1107041: 679, 1114352: 680, 1107062: 681, 288: 682, 1107067: 683, 91581: 684, 39904: 685, 6334: 686, 123825: 687, 23258: 688, 66805: 689, 6346: 690, 55968: 691, 368431: 692, 179702: 693, 1140547: 694, 1114388: 695, 90888: 696, 510715: 697, 33412: 698, 188471: 699, 1152143: 700, 1120962: 701, 1125258: 702, 648232: 703, 143476: 704, 1152150: 705, 1117249: 706, 25413: 707, 1152162: 708, 241821: 709, 350362: 710, 1116530: 711, 61069: 712, 1110000: 713, 646809: 714, 1105698: 715, 1152194: 716, 198653: 717, 1116569: 718, 77758: 719, 854434: 720, 1128151: 721, 1123867: 722, 191404: 723, 1116594: 724, 126793: 725, 43639: 726, 44368: 727, 97390: 728, 87915: 729, 131117: 730, 8581: 731, 27606: 732, 1115886: 733, 184157: 734, 8594: 735, 1152904: 736, 1120211: 737, 28350: 738, 1152910: 739, 27627: 740, 649731: 741, 308920: 742, 289780: 743, 289781: 744, 19621: 745, 1129608: 746, 1365: 747, 103543: 748, 28387: 749, 28389: 750, 43698: 751, 54550: 752, 1129621: 753, 46536: 754, 1129629: 755, 294126: 756, 568857: 757, 447224: 758, 38537: 759, 1152975: 760, 34979: 761, 1104261: 762, 139865: 763, 56709: 764, 1128945: 765, 19697: 766, 107177: 767, 1131165: 768, 1128959: 769, 152219: 770, 184918: 771, 16008: 772, 1122425: 773, 928873: 774, 206259: 775, 714748: 776, 1131189: 777, 217115: 778, 560936: 779, 1131198: 780, 1128985: 781, 466170: 782, 429805: 783, 561674: 784, 654177: 785, 95225: 786, 37884: 787, 37888: 788, 1128997: 789, 545647: 790, 42207: 791, 42209: 792, 82920: 793, 128202: 794, 128203: 795, 1134056: 796, 1102873: 797, 42221: 798, 1107171: 799, 1133338: 800, 67633: 801, 375825: 802, 48781: 803, 75674: 804, 289088: 805, 1152244: 806, 13917: 807, 75695: 808, 34257: 809, 1117348: 810, 574710: 811, 34263: 812, 1128204: 813, 34266: 814, 1128208: 815, 1116629: 816, 110162: 817, 110163: 818, 110164: 819, 628751: 820, 708945: 821, 1123926: 822, 1152277: 823, 77826: 824, 77829: 825, 8617: 826, 242663: 827, 8619: 828, 628764: 829, 628766: 830, 1125393: 831, 66986: 832, 646913: 833, 578309: 834, 18251: 835, 1152290: 836, 954315: 837, 212107: 838, 578337: 839, 907845: 840, 1127530: 841, 1128267: 842, 28412: 843, 594387: 844, 1127541: 845, 44455: 846, 45188: 847, 45189: 848, 62607: 849, 1127551: 850, 1123991: 851, 1127558: 852, 105057: 853, 1128291: 854, 1127566: 855, 1154459: 856, 218682: 857, 28447: 858, 1153736: 859, 62634: 860, 211432: 861, 112378: 862, 1113035: 863, 1118848: 864, 137790: 865, 217984: 866, 949217: 867, 28473: 868, 1104300: 869, 1105033: 870, 11093: 871, 696342: 872, 696343: 873, 696346: 874, 28487: 875, 5038: 876, 195150: 877, 62676: 878, 13213: 879, 576973: 880, 35797: 881, 134128: 882, 166825: 883, 175576: 884, 509379: 885, 1113084: 886, 53942: 887, 642621: 888, 1131236: 889, 1112369: 890, 446610: 891, 644093: 892, 411092: 893, 642641: 894, 408885: 895, 1131258: 896, 1131267: 897, 13269: 898, 1104379: 899, 1114502: 900, 1107215: 901, 83725: 902, 84459: 903, 642681: 904, 445938: 905, 1103676: 906, 1130568: 907, 1153003: 908, 51045: 909, 12576: 910, 144330: 911, 105865: 912, 51052: 913, 746058: 914, 1153014: 915, 641976: 916, 561789: 917, 1130586: 918, 368605: 919, 1133428: 920, 1113828: 921, 129042: 922, 129045: 923, 6539: 924, 1153031: 925, 1122580: 926, 1132706: 927, 1152308: 928, 105899: 929, 50354: 930, 1121867: 931, 1113852: 932, 1153056: 933, 94641: 934, 1153065: 935, 1133469: 936, 35070: 937, 576257: 938, 368657: 939, 1129018: 940, 263069: 941, 1129027: 942, 1152358: 943, 1125467: 944, 1125469: 945, 72101: 946, 40922: 947, 1153097: 948, 1109439: 949, 423463: 950, 128383: 951, 683360: 952, 1129040: 953, 52515: 954, 41666: 955, 1128319: 956, 1152379: 957, 1136342: 958, 1125492: 959, 1108728: 960, 265203: 961, 628888: 962, 1127619: 963, 56112: 964, 56115: 965, 56119: 966, 89547: 967, 51831: 968, 91038: 969, 96847: 970, 521855: 971, 594483: 972, 1119623: 973, 96851: 974, 1136397: 975, 158172: 976, 1127657: 977, 131315: 978, 131318: 979, 289945: 980, 62718: 981, 229635: 982, 56167: 983, 1119654: 984, 51879: 985, 10435: 986, 137873: 987, 168332: 988, 330208: 989, 689152: 990, 1120444: 991, 1153877: 992, 111770: 993, 1153879: 994, 108047: 995, 1131300: 996, 362926: 997, 129896: 998, 129897: 999, 59045: 1000, 1153889: 1001, 239810: 1002, 20601: 1003, 20602: 1004, 416964: 1005, 38722: 1006, 72908: 1007, 116081: 1008, 1153897: 1009, 116084: 1010, 116087: 1011, 1113182: 1012, 1131330: 1013, 582139: 1014, 561809: 1015, 14062: 1016, 1104449: 1017, 39474: 1018, 27895: 1019, 167670: 1020, 1131345: 1021, 1131348: 1022, 14083: 1023, 1103737: 1024, 65650: 1025, 93273: 1026, 65653: 1027, 5194: 1028, 14090: 1029, 1131360: 1030, 1130634: 1031, 976284: 1032, 1130637: 1033, 593022: 1034, 1131374: 1035, 975567: 1036, 133550: 1037, 145134: 1038, 1130653: 1039, 1130657: 1040, 1104495: 1041, 133566: 1042, 133567: 1043, 1122642: 1044, 1114629: 1045, 91852: 1046, 91853: 1047, 376704: 1048, 1153101: 1049, 32276: 1050, 1130678: 1051, 83847: 1052, 8079: 1053, 593068: 1054, 285675: 1055, 1130680: 1056, 1106630: 1057, 278394: 1058, 285687: 1059, 69284: 1060, 6639: 1061, 14807: 1062, 152483: 1063, 683404: 1064, 593091: 1065, 1117501: 1066, 99023: 1067, 99025: 1068, 513189: 1069, 1152421: 1070, 1153150: 1071, 99030: 1072, 1105932: 1073, 1153160: 1074, 1106671: 1075, 531348: 1076, 577086: 1077, 531351: 1078, 25702: 1079, 87482: 1080, 135765: 1081, 135766: 1082, 1132864: 1083, 22886: 1084, 1118286: 1085, 162664: 1086, 1109542: 1087, 1116835: 1088, 1116839: 1089, 1103016: 1090, 1128425: 1091, 1116842: 1092, 1136446: 1093, 1136447: 1094, 27199: 1095, 1125597: 1096, 1132887: 1097, 593813: 1098, 594543: 1099, 917493: 1100, 1128430: 1101, 51909: 1102, 1108834: 1103, 1128437: 1104, 989397: 1105, 97645: 1106, 8832: 1107, 1103031: 1108, 346243: 1109, 1119708: 1110, 36620: 1111, 25772: 1112, 640617: 1113, 8865: 1114, 950986: 1115, 35905: 1116, 8875: 1117, 25791: 1118, 100961: 1119, 738941: 1120, 64271: 1121, 3084: 1122, 3085: 1123, 28649: 1124, 3095: 1125, 3097: 1126, 1153943: 1127, 1121254: 1128, 74427: 1129, 231249: 1130, 1105221: 1131, 28674: 1132, 1129907: 1133, 650807: 1134, 348437: 1135, 1688: 1136, 33013: 1137, 38829: 1138, 307015: 1139, 127033: 1140, 310742: 1141, 1694: 1142, 650834: 1143, 1131420: 1144, 193918: 1145, 85324: 1146, 642827: 1147, 38845: 1148, 193931: 1149, 193932: 1150, 4553: 1151, 1116146: 1152, 85352: 1153, 261040: 1154, 145215: 1155, 646412: 1156, 1131464: 1157, 1131466: 1158, 574264: 1159, 458439: 1160, 57764: 1161, 646440: 1162, 1111899: 1163, 521252: 1164, 1115471: 1165, 1123493: 1166, 601462: 1167, 421481: 1168, 385572: 1169, 30934: 1170, 84695: 1171, 189566: 1172, 69397: 1173, 6741: 1174, 177998: 1175, 395725: 1176, 61417: 1177, 54129: 1178, 1118347: 1179, 1106764: 1180, 102406: 1181, 75972: 1182, 95579: 1183, 54132: 1184, 27243: 1185, 1153262: 1186, 1153264: 1187, 30973: 1188, 1129208: 1189, 1106771: 1190, 27249: 1191, 95586: 1192, 95588: 1193, 255233: 1194, 6775: 1195, 129287: 1196, 27250: 1197, 19231: 1198, 1153275: 1199, 1132948: 1200, 1106789: 1201, 95597: 1202, 6784: 1203, 682815: 1204, 1153280: 1205, 148170: 1206, 263279: 1207, 1116922: 1208, 1152564: 1209, 1118388: 1210, 851968: 1211, 3101: 1212, 1129243: 1213, 170798: 1214, 3112: 1215, 503877: 1216, 17821: 1217, 503883: 1218, 561238: 1219, 1110438: 1220, 575795: 1221, 1116974: 1222, 272720: 1223, 415693: 1224, 18582: 1225, 11325: 1226, 11326: 1227, 1103162: 1228, 1111186: 1229, 578645: 1230, 578646: 1231, 578649: 1232, 1121313: 1233, 11335: 1234, 1102442: 1235, 11339: 1236, 52784: 1237, 11342: 1238, 1130080: 1239, 3191: 1240, 3192: 1241, 400455: 1242, 1135899: 1243, 591017: 1244, 751408: 1245, 1140230: 1246, 1140231: 1247, 1106052: 1248, 70970: 1249, 67245: 1250, 67246: 1251, 205196: 1252, 135130: 1253, 123556: 1254, 645084: 1255, 1786: 1256, 66556: 1257, 1130808: 1258, 4649: 1259, 582343: 1260, 395075: 1261, 582349: 1262, 20833: 1263, 1131549: 1264, 58552: 1265, 85449: 1266, 49811: 1267, 77438: 1268, 4660: 1269, 66594: 1270, 66596: 1271, 314459: 1272, 1116268: 1273, 1103960: 1274, 49843: 1275, 1103969: 1276, 593240: 1277, 207395: 1278, 593248: 1279, 943087: 1280, 7532: 1281, 7537: 1282, 25181: 1283, 25184: 1284, 16437: 1285, 1103985: 1286, 6814: 1287, 6818: 1288, 1154042: 1289, 23738: 1290, 1107558: 1291, 137359: 1292, 16451: 1293, 318071: 1294, 232860: 1295, 1107572: 1296, 49895: 1297, 16474: 1298, 1154076: 1299, 626999: 1300, 137380: 1301, 1119178: 1302, 33904: 1303, 1119180: 1304, 33907: 1305, 174418: 1306, 70281: 1307, 73119: 1308, 9716: 1309, 174425: 1310, 416455: 1311, 18615: 1312, 127940: 1313, 1152663: 1314, 675649: 1315, 1117760: 1316, 1138091: 1317, 1152673: 1318, 321004: 1319, 139547: 1320, 45533: 1321, 3217: 1322, 1111240: 1323, 523574: 1324, 1110515: 1325, 73162: 1326, 52835: 1327, 3220: 1328, 3223: 1329, 1129367: 1330, 1129368: 1331, 1129369: 1332, 84021: 1333, 1127913: 1334, 3233: 1335, 3235: 1336, 3236: 1337, 562067: 1338, 3240: 1339, 92065: 1340, 213246: 1341, 911198: 1342, 12158: 1343, 20178: 1344, 20179: 1345, 80491: 1346, 561364: 1347, 20180: 1348, 245955: 1349, 1102548: 1350, 1817: 1351, 31043: 1352, 1102550: 1353, 20193: 1354, 1110579: 1355, 213279: 1356, 1133010: 1357, 157761: 1358, 31055: 1359, 12194: 1360, 1133028: 1361, 578780: 1362, 12198: 1363, 12199: 1364, 90655: 1365, 6130: 1366, 337766: 1367, 112787: 1368, 1133047: 1369, 1105428: 1370, 785678: 1371, 644441: 1372, 672064: 1373, 41216: 1374, 1105433: 1375, 1113459: 1376, 55770: 1377, 6163: 1378, 259701: 1379, 259702: 1380, 1131607: 1381, 430329: 1382, 643734: 1383, 643735: 1384, 656048: 1385, 1131611: 1386, 617575: 1387, 1105450: 1388, 15076: 1389, 10793: 1390, 1117049: 1391, 647315: 1392, 33231: 1393, 1116328: 1394, 1104749: 1395, 594025: 1396, 315266: 1397, 643777: 1398, 1130927: 1399, 1132385: 1400, 1108329: 1401, 1130929: 1402, 1104769: 1403, 594047: 1404, 1130931: 1405, 1130934: 1406, 141868: 1407, 593329: 1408, 144701: 1409, 574462: 1410, 60170: 1411, 120039: 1412, 502574: 1413, 293974: 1414, 1119216: 1415, 1108363: 1416, 191216: 1417, 469504: 1418, 358866: 1419, 1116397: 1420, 191222: 1421, 36145: 1422, 1115677: 1423, 577331: 1424, 31863: 1425, 566488: 1426, 358887: 1427, 6935: 1428, 6939: 1429, 197783: 1430, 34708: 1431, 1107674: 1432, 248119: 1433, 318187: 1434, 1152714: 1435, 1154173: 1436, 300071: 1437, 1120020: 1438, 423816: 1439, 1106966: 1440, 148341: 1441, 136766: 1442, 325497: 1443, 136767: 1444, 136768: 1445, 409255: 1446, 1152740: 1447, 1117833: 1448, 309476: 1449, 1120059: 1450, 80515: 1451, 65212: 1452, 15892: 1453, 1120084: 1454, 576691: 1455, 148399: 1456, 175291: 1457, 1112071: 1458, 117: 1459, 157805: 1460, 300806: 1461, 31105: 1462, 154982: 1463, 141160: 1464, 112813: 1465, 98693: 1466, 98698: 1467, 192734: 1468, 12247: 1469, 1109891: 1470, 141171: 1471, 312409: 1472, 608190: 1473, 608191: 1474, 55801: 1475, 1136791: 1476, 815073: 1477, 1114222: 1478, 173884: 1479, 1102646: 1480, 63832: 1481, 211906: 1482, 83449: 1483, 2654: 1484, 815096: 1485, 277263: 1486, 1105505: 1487, 48550: 1488, 83461: 1489, 48555: 1490, 6238: 1491, 636500: 1492, 340078: 1493, 1113534: 1494, 578898: 1495, 1951: 1496, 1952: 1497, 1956: 1498, 636511: 1499, 463825: 1500, 1121569: 1501, 1105531: 1502, 14428: 1503, 14429: 1504, 74749: 1505, 14430: 1506, 14431: 1507, 1132434: 1508, 648121: 1509, 582511: 1510, 688849: 1511, 1997: 1512, 1131728: 1513, 1106298: 1514, 86359: 1515, 647413: 1516, 1120880: 1517, 1131734: 1518, 562940: 1519, 230879: 1520, 1104851: 1521, 1152075: 1522, 58758: 1523, 230884: 1524, 34082: 1525, 1132486: 1526, 39890: 1527, 66782: 1528, 218410: 1529, 647447: 1530, 1117184: 1531, 66794: 1532, 227178: 1533, 936: 1534, 940: 1535, 575292: 1536, 941: 1537, 1109185: 1538, 85688: 1539, 28202: 1540, 50807: 1541, 379288: 1542, 1154229: 1543, 1109199: 1544, 118682: 1545, 153598: 1546, 1154251: 1547, 62417: 1548, 1125909: 1549, 79809: 1550, 739280: 1551, 70441: 1552, 70442: 1553, 70444: 1554, 79817: 1555, 129558: 1556, 892139: 1557, 576725: 1558, 28254: 1559, 1246: 1560, 237376: 1561, 27531: 1562, 397488: 1563, 42847: 1564, 42848: 1565, 155736: 1566, 155738: 1567, 39124: 1568, 39127: 1569, 39130: 1570, 1153577: 1571, 335733: 1572, 28290: 1573, 18815: 1574, 1136814: 1575, 1120169: 1576, 82087: 1577, 178209: 1578, 139738: 1579, 82090: 1580, 18834: 1581, 39165: 1582, 190698: 1583, 1125992: 1584, 1109957: 1585, 46470: 1586, 46476: 1587, 1129570: 1588, 1071981: 1589, 1129573: 1590, 39199: 1591, 12337: 1592, 29723: 1593, 694759: 1594, 46491: 1595, 1128856: 1596, 1107010: 1597, 643199: 1598, 1104182: 1599, 12347: 1600, 63915: 1601, 519353: 1602, 608292: 1603, 1121603: 1604, 1130356: 1605, 12359: 1606, 192850: 1607, 7032: 1608, 1128881: 1609, 140569: 1610, 1114331: 1611, 7041: 1612, 561581: 1613, 561582: 1614, 192870: 1615, 1113614: 1616, 1102761: 1617, 116528: 1618, 561595: 1619, 94416: 1620, 5600: 1621, 1000012: 1622, 1114364: 1623, 1121659: 1624, 66809: 1625, 6343: 1626, 212777: 1627, 583318: 1628, 709518: 1629, 350319: 1630, 116553: 1631, 170338: 1632, 179706: 1633, 1112929: 1634, 656231: 1635, 14531: 1636, 1106370: 1637, 1109208: 1638, 1114398: 1639, 95188: 1640, 510718: 1641, 208345: 1642, 6378: 1643, 22563: 1644, 10981: 1645, 110041: 1646, 14549: 1647, 95198: 1648, 6385: 1649, 575331: 1650, 568045: 1651, 1136110: 1652, 1131828: 1653, 67584: 1654, 243274: 1655, 135464: 1656, 1105672: 1657, 93755: 1658, 756061: 1659, 522338: 1660, 219239: 1661, 61073: 1662, 262178: 1663, 686015: 1664, 1110024: 1665, 613409: 1666, 686030: 1667, 227286: 1668, 45061: 1669, 646836: 1670, 1108551: 1671, 13885: 1672, 1104999: 1673, 566653: 1674, 1127430: 1675, 299197: 1676, 1135455: 1677, 97377: 1678, 592826: 1679, 566664: 1680, 633030: 1681, 633031: 1682, 686061: 1683, 592830: 1684, 573964: 1685, 1155073: 1686, 17476: 1687, 17477: 1688, 190706: 1689, 28336: 1690, 573978: 1691, 1107861: 1692, 17488: 1693, 1128198: 1694, 1108597: 1695, 103515: 1696, 27623: 1697, 200480: 1698, 103529: 1699, 649730: 1700, 39210: 1701, 46501: 1702, 27632: 1703, 649739: 1704, 1119471: 1705, 103531: 1706, 470511: 1707, 509233: 1708, 236759: 1709, 237489: 1710, 1152944: 1711, 1118764: 1712, 643221: 1713, 212097: 1714, 608326: 1715, 643239: 1716, 1131116: 1717, 202639: 1718, 141324: 1719, 294145: 1720, 1128927: 1721, 561610: 1722, 561611: 1723, 147870: 1724, 248395: 1725, 1128935: 1726, 241133: 1727, 141342: 1728, 141347: 1729, 1128946: 1730, 1131164: 1731, 12439: 1732, 1131167: 1733, 1129683: 1734, 359067: 1735, 117315: 1736, 117316: 1737, 144212: 1738, 1106401: 1739, 1134022: 1740, 13193: 1741, 1131192: 1742, 1107136: 1743, 1131195: 1744, 1128982: 1745, 121792: 1746, 653441: 1747, 385251: 1748, 1126011: 1749, 1134031: 1750, 642593: 1751, 1115166: 1752, 737204: 1753, 118079: 1754, 1122460: 1755, 1114442: 1756, 589923: 1757, 1121739: 1758, 626574: 1759, 1126037: 1760, 645452: 1761, 753264: 1762, 1126044: 1763, 74920: 1764, 74921: 1765, 1105718: 1766, 48764: 1767, 48768: 1768, 1113742: 1769, 74937: 1770, 575402: 1771, 168958: 1772, 78508: 1773, 289085: 1774, 78511: 1775, 308232: 1776, 682508: 1777, 75691: 1778, 75693: 1779, 75694: 1780, 155158: 1781, 1105764: 1782, 1152259: 1783, 579008: 1784, 1128201: 1785, 1133390: 1786, 1118083: 1787, 78549: 1788, 604073: 1789, 595056: 1790, 1118092: 1791, 1125386: 1792, 78552: 1793, 78555: 1794, 78557: 1795, 646900: 1796, 595063: 1797, 648369: 1798, 1128227: 1799, 89416: 1800, 578306: 1801, 683294: 1802, 440815: 1803, 126867: 1804, 126868: 1805, 72056: 1806, 1119505: 1807, 1128256: 1808, 1108656: 1809, 71336: 1810, 1109392: 1811, 40886: 1812, 1115959: 1813, 578347: 1814, 284023: 1815, 345340: 1816, 621555: 1817, 118873: 1818, 8687: 1819, 226698: 1820, 578365: 1821, 1135589: 1822, 8696: 1823, 1118823: 1824, 411005: 1825, 509315: 1826, 171954: 1827, 230300: 1828, 1105011: 1829, 1121057: 1830, 592973: 1831, 592975: 1832, 48066: 1833, 248431: 1834, 1121063: 1835, 592986: 1836, 48075: 1837, 289885: 1838, 592993: 1839, 592996: 1840, 28489: 1841, 590022: 1842, 111676: 1843, 13205: 1844, 13208: 1845, 102938: 1846, 102939: 1847, 416867: 1848, 72805: 1849, 574009: 1850, 294239: 1851, 1131223: 1852, 77108: 1853, 5064: 1854, 5069: 1855, 1131230: 1856, 40125: 1857, 1123215: 1858, 20526: 1859, 20528: 1860, 77112: 1861, 107251: 1862, 107252: 1863, 5075: 1864, 126128: 1865, 734406: 1866, 40131: 1867, 703953: 1868, 40135: 1869, 1131257: 1870, 1123239: 1871, 1129778: 1872, 662250: 1873, 711994: 1874, 273949: 1875, 1131266: 1876, 1130539: 1877, 377303: 1878, 179180: 1879, 1129798: 1880, 1114512: 1881, 1110950: 1882, 12558: 1883, 853114: 1884, 853115: 1885, 853116: 1886, 853118: 1887, 1114526: 1888, 212930: 1889, 206371: 1890, 105856: 1891, 463: 1892, 20592: 1893, 51049: 1894, 20593: 1895, 83746: 1896, 124734: 1897, 106590: 1898, 1133417: 1899, 1125402: 1900, 1153024: 1901, 853155: 1902, 1118120: 1903, 1105810: 1904, 1113831: 1905, 646289: 1906, 1106546: 1907, 31479: 1908, 31483: 1909, 31489: 1910, 94639: 1911, 631015: 1912, 645571: 1913, 1106568: 1914, 430711: 1915, 7296: 1916, 1132731: 1917, 1153064: 1918, 93923: 1919, 1134197: 1920, 87363: 1921, 395540: 1922, 395547: 1923, 50381: 1924, 1129015: 1925, 126909: 1926, 143676: 1927, 395553: 1928, 752684: 1929, 1129021: 1930, 19045: 1931, 631052: 1932, 126912: 1933, 116790: 1934, 5869: 1935, 579108: 1936, 683355: 1937, 1105877: 1938, 59772: 1939, 243483: 1940, 126926: 1941, 155277: 1942, 1128314: 1943, 1105887: 1944, 1110209: 1945, 307656: 1946, 199571: 1947, 1152394: 1948, 60560: 1949, 595193: 1950, 990075: 1951, 119686: 1952, 1154520: 1953, 28504: 1954, 1154524: 1955, 1154525: 1956, 1129096: 1957, 1128369: 1958, 96845: 1959, 380341: 1960, 8766: 1961, 1110256: 1962, 55403: 1963, 389715: 1964, 1153816: 1965, 131317: 1966, 260979: 1967, 264556: 1968, 35852: 1969, 1119671: 1970, 1153853: 1971, 1112417: 1972, 1153860: 1973, 1153861: 1974, 35863: 1975, 1121176: 1976, 1131301: 1977, 1131305: 1978, 1105148: 1979, 134219: 1980, 671052: 1981, 1131312: 1982, 156794: 1983, 1153896: 1984, 1153899: 1985, 167656: 1986, 239829: 1987, 1104435: 1988, 187260: 1989, 231198: 1990, 1131334: 1991, 1131335: 1992, 142268: 1993, 504: 1994, 506: 1995, 228990: 1996, 228992: 1997, 1132073: 1998, 654326: 1999, 1116044: 2000, 1131359: 2001, 643485: 2002, 654339: 2003, 1107319: 2004, 132821: 2005, 360028: 2006, 214472: 2007, 646334: 2008, 653628: 2009, 1107325: 2010, 166989: 2011, 1111788: 2012, 151708: 2013, 118259: 2014, 32260: 2015, 137130: 2016, 92589: 2017, 118260: 2018, 124828: 2019, 141596: 2020, 197452: 2021, 646357: 2022, 1153106: 2023, 30817: 2024, 642798: 2025, 1130676: 2026, 1107355: 2027, 1118209: 2028, 987188: 2029, 87417: 2030, 23545: 2031, 23546: 2032, 1113926: 2033, 94713: 2034, 1107367: 2035, 987197: 2036, 521183: 2037, 1114664: 2038, 69296: 2039, 51180: 2040, 43165: 2041, 1132815: 2042, 1107385: 2043, 100197: 2044, 520471: 2045, 215912: 2046, 61312: 2047, 1129106: 2048, 43186: 2049, 1129111: 2050, 41732: 2051, 22869: 2052, 9513: 2053, 9515: 2054, 119712: 2055, 270456: 2056, 5959: 2057, 576362: 2058, 1153183: 2059, 22874: 2060, 22875: 2061, 22876: 2062, 1124837: 2063, 1132857: 2064, 594511: 2065, 22883: 2066, 238401: 2067, 1136449: 2068, 714208: 2069, 9559: 2070, 135798: 2071, 1152490: 2072, 1109566: 2073, 1103038: 2074, 177115: 2075, 523394: 2076, 1128453: 2077, 1109581: 2078, 101660: 2079, 101662: 2080, 9581: 2081, 9586: 2082, 1135750: 2083, 51934: 2084, 762980: 2085, 1153900: 2086, 593859: 2087, 714256: 2088, 8874: 2089, 25794: 2090, 75121: 2091, 28632: 2092, 1153922: 2093, 1119742: 2094, 63549: 2095, 1138619: 2096, 1102364: 2097, 28640: 2098, 28641: 2099, 409725: 2100, 292277: 2101, 606479: 2102, 1153942: 2103, 1153945: 2104, 1153946: 2105, 709113: 2106, 194609: 2107, 90470: 2108, 820661: 2109, 820662: 2110, 1105231: 2111, 73712: 2112, 54844: 2113, 684972: 2114, 134314: 2115, 735303: 2116, 824245: 2117, 195361: 2118, 529165: 2119, 1131414: 2120, 617378: 2121, 1120563: 2122, 47570: 2123, 684986: 2124, 735311: 2125, 187354: 2126, 1132157: 2127, 58436: 2128, 278403: 2129, 58453: 2130, 58454: 2131, 206524: 2132, 593104: 2133, 133628: 2134, 46887: 2135, 49720: 2136, 1131471: 2137, 643597: 2138, 1107418: 2139, 1129994: 2140, 573535: 2141, 814836: 2142, 1119004: 2143, 1134320: 2144, 1116181: 2145, 1108167: 2146, 1108169: 2147, 49753: 2148, 57773: 2149, 7430: 2150, 521251: 2151, 593155: 2152, 642894: 2153, 1126315: 2154, 1108175: 2155, 1059953: 2156, 521269: 2157, 1118302: 2158, 1130780: 2159, 1134346: 2160, 1134348: 2161, 1135082: 2162, 899085: 2163, 124952: 2164, 240791: 2165, 189571: 2166, 189572: 2167, 1126350: 2168, 189574: 2169, 177993: 2170, 27230: 2171, 1119078: 2172, 128540: 2173, 308529: 2174, 54131: 2175, 75983: 2176, 15670: 2177, 33818: 2178, 95594: 2179, 6782: 2180, 33823: 2181, 25805: 2182, 1153287: 2183, 596075: 2184, 817774: 2185, 18532: 2186, 18536: 2187, 235670: 2188, 235678: 2189, 235679: 2190, 739707: 2191, 17811: 2192, 503871: 2193, 235683: 2194, 1128531: 2195, 594649: 2196, 1128536: 2197, 1102400: 2198, 593921: 2199, 486840: 2200, 1127810: 2201, 503893: 2202, 399370: 2203, 387795: 2204, 220420: 2205, 593942: 2206, 8961: 2207, 645016: 2208, 481073: 2209, 11337: 2210, 578650: 2211, 1130069: 2212, 1127851: 2213, 124224: 2214, 37483: 2215, 578669: 2216, 1127863: 2217, 1135894: 2218, 645046: 2219, 22229: 2220, 149669: 2221, 365294: 2222, 169279: 2223, 1138755: 2224, 323128: 2225, 22241: 2226, 156977: 2227, 763010: 2228, 1120650: 2229, 1105344: 2230, 59244: 2231, 286500: 2232, 567005: 2233, 644361: 2234, 644363: 2235, 154023: 2236, 286513: 2237, 459206: 2238, 671269: 2239, 1105360: 2240, 1112650: 2241, 632796: 2242, 47682: 2243, 47683: 2244, 47684: 2245, 4637: 2246, 642920: 2247, 634902: 2248, 459213: 2249, 459214: 2250, 634904: 2251, 459216: 2252, 20821: 2253, 178718: 2254, 1108209: 2255, 1112665: 2256, 1104647: 2257, 1140289: 2258, 66563: 2259, 67292: 2260, 66564: 2261, 154047: 2262, 642930: 2263, 654519: 2264, 178727: 2265, 1135108: 2266, 593201: 2267, 162075: 2268, 593209: 2269, 107569: 2270, 1123530: 2271, 1135115: 2272, 1132285: 2273, 1131557: 2274, 162080: 2275, 3932: 2276, 593210: 2277, 118424: 2278, 1135122: 2279, 634938: 2280, 1131565: 2281, 20857: 2282, 118435: 2283, 118436: 2284, 643695: 2285, 1130847: 2286, 1111978: 2287, 1154012: 2288, 1108258: 2289, 49844: 2290, 49847: 2291, 189620: 2292, 189623: 2293, 1108267: 2294, 1050679: 2295, 634975: 2296, 1114838: 2297, 577227: 2298, 28026: 2299, 601561: 2300, 24476: 2301, 1026: 2302, 95642: 2303, 270600: 2304, 145384: 2305, 16461: 2306, 35335: 2307, 1138027: 2308, 1035: 2309, 1114864: 2310, 1154068: 2311, 449841: 2312, 1154071: 2313, 1106854: 2314, 210309: 2315, 801170: 2316, 251756: 2317, 645870: 2318, 144679: 2319, 1138043: 2320, 86923: 2321, 342802: 2322, 1152633: 2323, 711527: 2324, 684372: 2325, 216878: 2326, 62274: 2327, 72406: 2328, 101811: 2329, 246618: 2330, 1136631: 2331, 1152676: 2332, 235776: 2333, 57119: 2334, 119956: 2335, 948147: 2336, 739816: 2337, 3222: 2338, 1117786: 2339, 1110520: 2340, 36802: 2341, 3232: 2342, 3237: 2343, 1111265: 2344, 695284: 2345, 37541: 2346, 1110546: 2347, 71736: 2348, 1135955: 2349, 12155: 2350, 258259: 2351, 1114118: 2352, 606647: 2353, 12165: 2354, 1110563: 2355, 12169: 2356, 1133004: 2357, 1133008: 2358, 1102567: 2359, 12195: 2360, 28851: 2361, 644427: 2362, 1113438: 2363, 1121459: 2364, 689439: 2365, 633585: 2366, 31083: 2367, 6152: 2368, 1119987: 2369, 1114184: 2370, 82664: 2371, 82666: 2372, 672070: 2373, 672071: 2374, 632874: 2375, 1114192: 2376, 644470: 2377, 5462: 2378, 594011: 2379, 20924: 2380, 1131634: 2381, 1120786: 2382, 1112767: 2383, 180301: 2384, 160705: 2385, 628458: 2386, 628459: 2387, 1130915: 2388, 1116336: 2389, 390889: 2390, 57922: 2391, 594039: 2392, 13654: 2393, 57932: 2394, 73972: 2395, 198443: 2396, 60159: 2397, 101143: 2398, 101145: 2399, 763181: 2400, 44121: 2401, 593328: 2402, 259772: 2403, 189708: 2404, 60169: 2405, 24530: 2406, 467383: 2407, 20972: 2408, 13686: 2409, 152731: 2410, 118558: 2411, 118559: 2412, 1154123: 2413, 1154124: 2414, 1126503: 2415, 40583: 2416, 95719: 2417, 693143: 2418, 36131: 2419, 1123689: 2420, 6913: 2421, 256106: 2422, 36140: 2423, 1115670: 2424, 1108389: 2425, 6923: 2426, 6925: 2427, 36162: 2428, 62329: 2429, 36167: 2430, 6941: 2431, 245288: 2432, 62333: 2433, 189774: 2434, 1133846: 2435, 167205: 2436, 62347: 2437, 267003: 2438, 1114992: 2439, 1112026: 2440, 1119295: 2441, 1111304: 2442, 964248: 2443, 45603: 2444, 1109830: 2445, 1152761: 2446, 62389: 2447, 444191: 2448, 263482: 2449, 263486: 2450, 263498: 2451, 675756: 2452, 1125895: 2453, 627024: 2454, 12211: 2455, 643069: 2456, 1112075: 2457, 884094: 2458, 120817: 2459, 1110628: 2460, 18770: 2461, 18773: 2462, 173863: 2463, 1130243: 2464, 1102625: 2465, 63812: 2466, 18781: 2467, 18785: 2468, 1129494: 2469, 578845: 2470, 68115: 2471, 293271: 2472, 63835: 2473, 1919: 2474, 164: 2475, 293285: 2476, 12275: 2477, 1103383: 2478, 1114239: 2479, 6215: 2480, 288107: 2481, 385067: 2482, 1121537: 2483, 1103394: 2484, 6224: 2485, 2663: 2486, 104840: 2487, 632935: 2488, 1106236: 2489, 375605: 2490, 1132406: 2491, 28964: 2492, 308003: 2493, 47839: 2494, 753070: 2495, 563613: 2496, 1132416: 2497, 2695: 2498, 2696: 2499, 2698: 2500, 1105530: 2501, 1113551: 2502, 688824: 2503, 1138968: 2504, 1120858: 2505, 40605: 2506, 1132443: 2507, 1999: 2508, 33325: 2509, 644577: 2510, 66751: 2511, 594119: 2512, 1132461: 2513, 1115701: 2514, 1131741: 2515, 270085: 2516, 1136040: 2517, 1131752: 2518, 1131754: 2519, 4878: 2520, 1123756: 2521, 1135345: 2522, 1107728: 2523, 1154232: 2524, 1154233: 2525, 17363: 2526, 1213: 2527, 149139: 2528, 28230: 2529, 50838: 2530, 1125906: 2531, 32698: 2532, 754594: 2533, 1133930: 2534, 1115790: 2535, 28249: 2536, 1237: 2537, 684531: 2538, 238099: 2539, 131042: 2540, 444240: 2541, 1112106: 2542, 27535: 2543, 28267: 2544, 1120138: 2545, 1117920: 2546, 1125944: 2547, 1118658: 2548, 263553: 2549, 1125953: 2550, 114308: 2551, 630817: 2552, 687401: 2553, 594900: 2554, 10174: 2555, 73323: 2556, 46431: 2557, 202520: 2558, 15987: 2559, 10186: 2560, 294030: 2561, 675847: 2562, 190697: 2563, 576795: 2564, 1125993: 2565, 519318: 2566, 1120197: 2567, 1152896: 2568, 1122304: 2569, 2702: 2570, 1129572: 2571, 1112194: 2572, 29738: 2573, 1128868: 2574, 633721: 2575, 630890: 2576, 1123068: 2577, 561568: 2578, 733534: 2579, 1102751: 2580, 1114336: 2581, 1123087: 2582, 6311: 2583, 116512: 2584, 6318: 2585, 7047: 2586, 1123093: 2587, 1103499: 2588, 151430: 2589, 431206: 2590, 372862: 2591, 561593: 2592, 1106330: 2593, 1105603: 2594, 1132505: 2595, 74821: 2596, 6344: 2597, 116545: 2598, 733576: 2599, 1112911: 2600, 1105622: 2601, 1102794: 2602, 262108: 2603, 116552: 2604, 41417: 2605, 1140543: 2606, 14529: 2607, 1117219: 2608, 1107095: 2609, 1140548: 2610, 523010: 2611, 42156: 2612, 262121: 2613, 22564: 2614, 14545: 2615, 22566: 2616, 1106388: 2617, 429781: 2618, 335042: 2619, 219218: 2620, 610529: 2621, 250566: 2622, 1104946: 2623, 195792: 2624, 1152179: 2625, 89308: 2626, 350373: 2627, 628667: 2628, 628668: 2629, 102061: 2630, 430574: 2631, 1107808: 2632, 1110028: 2633, 45052: 2634, 89335: 2635, 252715: 2636, 4983: 2637, 646837: 2638, 1139009: 2639, 252725: 2640, 593544: 2641, 299195: 2642, 593559: 2643, 1108570: 2644, 272345: 2645, 593560: 2646, 70520: 2647, 131122: 2648, 8591: 2649, 217852: 2650, 264347: 2651, 7867: 2652, 27612: 2653, 1152917: 2654, 28359: 2655, 103528: 2656, 46500: 2657, 27631: 2658, 289779: 2659, 103537: 2660, 633081: 2661, 255628: 2662, 397590: 2663, 1129610: 2664, 50980: 2665, 28385: 2666, 427606: 2667, 616336: 2668, 1120252: 2669, 1152958: 2670, 1152959: 2671, 1385: 2672, 254923: 2673, 34961: 2674, 46547: 2675, 13136: 2676, 1131137: 2677, 233106: 2678, 561613: 2679, 1131149: 2680, 1104258: 2681, 1152991: 2682, 447250: 2683, 115188: 2684, 102879: 2685, 1131150: 2686, 56708: 2687, 1128943: 2688, 134060: 2689, 102884: 2690, 1131163: 2691, 4274: 2692, 1131172: 2693, 767763: 2694, 152226: 2695, 152227: 2696, 626530: 2697, 626531: 2698, 1131180: 2699, 1130454: 2700, 1131184: 2701, 1128974: 2702, 1128975: 2703, 1128977: 2704, 1128978: 2705, 117328: 2706, 24043: 2707}\n"
     ]
    }
   ],
   "source": [
    "#构建节点索引\n",
    "idx_map = {j: i for i, j in enumerate(idx)} \n",
    "print(idx_map)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.656864Z",
     "start_time": "2023-10-25T03:16:18.616301900Z"
    }
   },
   "id": "fa035ebec9c31a24"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     35    1033]\n",
      " [     35  103482]\n",
      " [     35  103515]\n",
      " ...\n",
      " [ 853118 1140289]\n",
      " [ 853155  853118]\n",
      " [ 954315 1155073]]\n"
     ]
    }
   ],
   "source": [
    "##读取边的数据\n",
    "edges_unordered = np.genfromtxt('C:\\\\Users\\\\Wei Zhou\\\\Desktop\\\\test\\\\图神经网络几个算法\\\\gcn\\\\data\\\\cora\\\\cora.cites',\n",
    "                                    dtype=np.int32)\n",
    "print(edges_unordered)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.727625700Z",
     "start_time": "2023-10-25T03:16:18.631739200Z"
    }
   },
   "id": "fe14a0a5454d3fc8"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 163  402]\n",
      " [ 163  659]\n",
      " [ 163 1696]\n",
      " ...\n",
      " [1887 2258]\n",
      " [1902 1887]\n",
      " [ 837 1686]]\n"
     ]
    }
   ],
   "source": [
    "##构成边的点的原有编码替换成新的点的编码\n",
    "edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),dtype=np.int32).reshape(edges_unordered.shape)\n",
    "print(edges)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.744598100Z",
     "start_time": "2023-10-25T03:16:18.663561700Z"
    }
   },
   "id": "ca2dc309046f338a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "##构建邻接矩阵且为稀疏矩阵\n",
    "adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "print(adj.A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.744598100Z",
     "start_time": "2023-10-25T03:16:18.681877400Z"
    }
   },
   "id": "86d4e948635a6304"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "##计算转置矩阵将有向图转化为无向图\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.744598100Z",
     "start_time": "2023-10-25T03:16:18.695278400Z"
    }
   },
   "id": "4225149ee090d74e"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "##归一化函数\n",
    "def normalize(mx):\n",
    "    rowsum = np.array(mx.sum(1)) #矩阵行求和\n",
    "    r_inv = np.power(rowsum, -1).flatten() ##求和的倒数\n",
    "    r_inv[np.isinf(r_inv)] = 0.#由于原本为0的数求倒数后可能会产生极大值，这里设置极大值为0\n",
    "    r_mat_inv = sp.diags(r_inv) ##构造对角线矩阵\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.744598100Z",
     "start_time": "2023-10-25T03:16:18.711748600Z"
    }
   },
   "id": "5ccdbb4674649bf3"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "## 对节点进行归一化\n",
    "features = normalize(features)\n",
    "print(features.A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.822566600Z",
     "start_time": "2023-10-25T03:16:18.731493Z"
    }
   },
   "id": "669bc80d1c0fbb82"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16666667 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.5        0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.2        ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.2        0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.2        0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "##先将稀疏矩阵转化为密集矩阵再进行归一化操作\n",
    "adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "print(adj.A)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.831466300Z",
     "start_time": "2023-10-25T03:16:18.761727800Z"
    }
   },
   "id": "807ebc9b17829c04"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "##SciPy稀疏矩阵转换为PyTorch稀疏张量\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.831466300Z",
     "start_time": "2023-10-25T03:16:18.796692700Z"
    }
   },
   "id": "6514525bea23f393"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "##将scipy的稀疏矩阵转化为密集矩阵，再转化其为tensor形式\n",
    "features = torch.FloatTensor(np.array(features.todense()))\n",
    "print(features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.831466300Z",
     "start_time": "2023-10-25T03:16:18.807651500Z"
    }
   },
   "id": "291d17e21dbfd6ea"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 5,  ..., 4, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "##将标签转化为longtensor形式\n",
    "labels = torch.LongTensor(np.where(labels)[1])\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.846876300Z",
     "start_time": "2023-10-25T03:16:18.823352200Z"
    }
   },
   "id": "f8689c79ee3d03b5"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
      "                       [   0,    8,   14,  ..., 1389, 2344, 2707]]),\n",
      "       values=tensor([0.1667, 0.1667, 0.1667,  ..., 0.2500, 0.2500, 0.2500]),\n",
      "       size=(2708, 2708), nnz=13264, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "##将邻接矩阵转化为tensor形式\n",
    "adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "print(adj)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.902462900Z",
     "start_time": "2023-10-25T03:16:18.841751800Z"
    }
   },
   "id": "d927c0c2d03f3983"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "##划分数据集\n",
    "idx_train = range(140)\n",
    "idx_val = range(200, 500)\n",
    "idx_test = range(500, 1500)\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.916668600Z",
     "start_time": "2023-10-25T03:16:18.854931900Z"
    }
   },
   "id": "bd668de3044090f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练前设置"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62f98ffc084873ba"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import argparse \n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "##--no-cuda: 如果指定了这个选项，将禁用 CUDA 训练。\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "##--fastmode: 如果指定了这个选项，在训练过程中会进行验证。\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "##--seed: 随机种子，默认值为 42。\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "\n",
    "##--epochs: 要训练的轮数，默认值为 200。\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "\n",
    "##--lr: 初始学习率，默认值为 0.01。\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "\n",
    "##--weight_decay: 权重衰减（用于 L2 正则化），默认值为 5e-4。\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "\n",
    "##--hidden: 隐藏层单元数，默认值为 16。\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "\n",
    "##--dropout: dropout 率（1 - 保持概率），默认值为 0.5。\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "##设置随机种子和检查 CUDA 可用性的\n",
    "args, _ = parser.parse_known_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.926464500Z",
     "start_time": "2023-10-25T03:16:18.871846300Z"
    }
   },
   "id": "9dbb223ccba1f76e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型设置"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "837117505bf9890a"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "##定义GNN模型\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)#构建第一层GCN\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)#构建第二层GCN\n",
    "        self.dropout = dropout\n",
    "        \n",
    "##正向传播\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))#将节点特征矩阵和邻接矩阵通过第一层GCN然后在对进行非线性化\n",
    "        x = F.dropout(x, self.dropout, training=self.training)##较少过拟合使用drop out\n",
    "        x = self.gc2(x, adj)##将第一层输出的节点特诊和邻接矩阵输入第二层图卷积\n",
    "        return F.log_softmax(x, dim=1)##应用 log softmax 函数以获得每个类别的预测概率。"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.965704100Z",
     "start_time": "2023-10-25T03:16:18.918255700Z"
    }
   },
   "id": "408e791f721e51ad"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "##定义图神经卷积层\n",
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        ##输入特征维度\n",
    "        self.in_features = in_features\n",
    "        ##输出特征维度\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        ##初始化权重矩阵。其形状是 (in_features, out_features)，这表示它将一个 in_features 维的向量转换为 out_features 维的向量。\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        \n",
    "        ##初始化偏置项。创建一个长度为out_fetures的向量\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        ##用于权重和偏置初始化的标准偏差\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        ##进行矩阵乘法：输入的节点特征和权重矩阵\n",
    "        support = torch.mm(input, self.weight)\n",
    "        ##执行矩阵乘法：稀疏矩阵（adj）和一般矩阵（support）的乘法\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        ##输出这个类的有用信息，如输入/输出特征的维度\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.982627600Z",
     "start_time": "2023-10-25T03:16:18.935023800Z"
    }
   },
   "id": "4ae34582142a5030"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#导入模型\n",
    "model = GCN(nfeat=features.shape[1],## 特征数量\n",
    "            nhid=args.hidden,## 隐藏层节点数量\n",
    "            nclass=labels.max().item() + 1,##类别数量，由标签最大值确定\n",
    "            dropout=args.dropout)##dropout，防止过拟合"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.982627600Z",
     "start_time": "2023-10-25T03:16:18.951900400Z"
    }
   },
   "id": "fb17e005c71ca362"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "#定义优化器\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(),lr=args.lr, weight_decay=args.weight_decay)\n",
    "#model.parameters(): 这是一个生成器，用于遍历模型的所有可训练参数。这些参数将会被优化器更新\n",
    "#lr=args.lr: 这里设置了学习率（Learning Rate）。该值从之前定义的 args 对象中获取。学习率是一个控制模型权重更新步长的超参数。\n",
    "#weight_decay=args.weight_decay: 权重衰减（通常用于 L2 正则化）也是从 args 对象中获取的。权重衰减可以用于防止模型过拟合。"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:18.982627600Z",
     "start_time": "2023-10-25T03:16:18.966525900Z"
    }
   },
   "id": "f2085c3cb02e797e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9706504f14ecdd3e"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import time\n",
    "def train(epoch):\n",
    "##记录开始训练的时间\n",
    "    t = time.time()\n",
    "##调整模型为训练模式\n",
    "    model.train()\n",
    "##梯度清零\n",
    "    optimizer.zero_grad()\n",
    "##前向传播，将节点特征和邻接矩阵相乘\n",
    "    output = model(features, adj)\n",
    "##损失函数计算\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "##计算准确度\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "##反向传播计算梯度\n",
    "    loss_train.backward()\n",
    "##更新模型参数\n",
    "    optimizer.step()\n",
    "\n",
    "##检查是否是快速模式\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:19.013935Z",
     "start_time": "2023-10-25T03:16:18.982627600Z"
    }
   },
   "id": "c3a5a1bc0adbfc99"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "##计算准确度的函数\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)##提取输出的的每一行的最大值\n",
    "    correct = preds.eq(labels).double()##对比预测和标签使用布尔张量表示\n",
    "    correct = correct.sum()#对上一步生成的浮点数张量求和，以得到预测正确的样本数。\n",
    "    return correct / len(labels)#正确预测的样本数与总样本数的比值"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:19.013935Z",
     "start_time": "2023-10-25T03:16:19.001576Z"
    }
   },
   "id": "3b85e2894e1db46e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 测试模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89801bdffc29bc47"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "##将模型调整为评估模式-关闭反向传播\n",
    "    output = model(features, adj)\n",
    "##求损失值\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "##求准确度\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:19.031834400Z",
     "start_time": "2023-10-25T03:16:19.013935Z"
    }
   },
   "id": "f7bf6ddeb599de95"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9240 acc_train: 0.2286 loss_val: 1.8940 acc_val: 0.3500 time: 0.0615s\n",
      "Epoch: 0002 loss_train: 1.9163 acc_train: 0.2929 loss_val: 1.8829 acc_val: 0.3500 time: 0.0080s\n",
      "Epoch: 0003 loss_train: 1.8986 acc_train: 0.2929 loss_val: 1.8725 acc_val: 0.3500 time: 0.0103s\n",
      "Epoch: 0004 loss_train: 1.8850 acc_train: 0.2929 loss_val: 1.8625 acc_val: 0.3500 time: 0.0000s\n",
      "Epoch: 0005 loss_train: 1.8798 acc_train: 0.2929 loss_val: 1.8528 acc_val: 0.3500 time: 0.0099s\n",
      "Epoch: 0006 loss_train: 1.8668 acc_train: 0.2929 loss_val: 1.8432 acc_val: 0.3500 time: 0.0035s\n",
      "Epoch: 0007 loss_train: 1.8630 acc_train: 0.2929 loss_val: 1.8340 acc_val: 0.3500 time: 0.0070s\n",
      "Epoch: 0008 loss_train: 1.8500 acc_train: 0.2929 loss_val: 1.8252 acc_val: 0.3500 time: 0.0000s\n",
      "Epoch: 0009 loss_train: 1.8367 acc_train: 0.2929 loss_val: 1.8167 acc_val: 0.3500 time: 0.0092s\n",
      "Epoch: 0010 loss_train: 1.8319 acc_train: 0.2929 loss_val: 1.8085 acc_val: 0.3500 time: 0.0000s\n",
      "Epoch: 0011 loss_train: 1.8283 acc_train: 0.2929 loss_val: 1.8008 acc_val: 0.3500 time: 0.0103s\n",
      "Epoch: 0012 loss_train: 1.8092 acc_train: 0.2929 loss_val: 1.7935 acc_val: 0.3500 time: 0.0056s\n",
      "Epoch: 0013 loss_train: 1.8146 acc_train: 0.2929 loss_val: 1.7868 acc_val: 0.3500 time: 0.0045s\n",
      "Epoch: 0014 loss_train: 1.7858 acc_train: 0.2929 loss_val: 1.7805 acc_val: 0.3500 time: 0.0116s\n",
      "Epoch: 0015 loss_train: 1.7992 acc_train: 0.2929 loss_val: 1.7747 acc_val: 0.3500 time: 0.0000s\n",
      "Epoch: 0016 loss_train: 1.7820 acc_train: 0.2929 loss_val: 1.7694 acc_val: 0.3500 time: 0.0085s\n",
      "Epoch: 0017 loss_train: 1.7757 acc_train: 0.2929 loss_val: 1.7644 acc_val: 0.3500 time: 0.0095s\n",
      "Epoch: 0018 loss_train: 1.7626 acc_train: 0.2929 loss_val: 1.7598 acc_val: 0.3500 time: 0.0000s\n",
      "Epoch: 0019 loss_train: 1.7628 acc_train: 0.2929 loss_val: 1.7555 acc_val: 0.3500 time: 0.0104s\n",
      "Epoch: 0020 loss_train: 1.7502 acc_train: 0.2929 loss_val: 1.7512 acc_val: 0.3500 time: 0.0025s\n",
      "Epoch: 0021 loss_train: 1.7622 acc_train: 0.2929 loss_val: 1.7469 acc_val: 0.3500 time: 0.0075s\n",
      "Epoch: 0022 loss_train: 1.7472 acc_train: 0.2929 loss_val: 1.7429 acc_val: 0.3500 time: 0.0000s\n",
      "Epoch: 0023 loss_train: 1.7438 acc_train: 0.2929 loss_val: 1.7391 acc_val: 0.3500 time: 0.0098s\n",
      "Epoch: 0024 loss_train: 1.7453 acc_train: 0.2929 loss_val: 1.7351 acc_val: 0.3500 time: 0.0000s\n",
      "Epoch: 0025 loss_train: 1.7242 acc_train: 0.3000 loss_val: 1.7308 acc_val: 0.3500 time: 0.0109s\n",
      "Epoch: 0026 loss_train: 1.7111 acc_train: 0.2929 loss_val: 1.7263 acc_val: 0.3500 time: 0.0044s\n",
      "Epoch: 0027 loss_train: 1.7259 acc_train: 0.3071 loss_val: 1.7213 acc_val: 0.3500 time: 0.0050s\n",
      "Epoch: 0028 loss_train: 1.7192 acc_train: 0.3000 loss_val: 1.7161 acc_val: 0.3500 time: 0.0000s\n",
      "Epoch: 0029 loss_train: 1.6984 acc_train: 0.3071 loss_val: 1.7106 acc_val: 0.3500 time: 0.0102s\n",
      "Epoch: 0030 loss_train: 1.6896 acc_train: 0.3286 loss_val: 1.7048 acc_val: 0.3500 time: 0.0096s\n",
      "Epoch: 0031 loss_train: 1.6915 acc_train: 0.3000 loss_val: 1.6987 acc_val: 0.3500 time: 0.0000s\n",
      "Epoch: 0032 loss_train: 1.6753 acc_train: 0.2929 loss_val: 1.6925 acc_val: 0.3500 time: 0.0099s\n",
      "Epoch: 0033 loss_train: 1.6626 acc_train: 0.3143 loss_val: 1.6861 acc_val: 0.3500 time: 0.0000s\n",
      "Epoch: 0034 loss_train: 1.6623 acc_train: 0.3143 loss_val: 1.6793 acc_val: 0.3600 time: 0.0100s\n",
      "Epoch: 0035 loss_train: 1.6472 acc_train: 0.3500 loss_val: 1.6724 acc_val: 0.3633 time: 0.0025s\n",
      "Epoch: 0036 loss_train: 1.6239 acc_train: 0.3786 loss_val: 1.6653 acc_val: 0.3633 time: 0.0075s\n",
      "Epoch: 0037 loss_train: 1.6065 acc_train: 0.3714 loss_val: 1.6581 acc_val: 0.3667 time: 0.0000s\n",
      "Epoch: 0038 loss_train: 1.6097 acc_train: 0.3714 loss_val: 1.6505 acc_val: 0.3700 time: 0.0100s\n",
      "Epoch: 0039 loss_train: 1.5821 acc_train: 0.4500 loss_val: 1.6426 acc_val: 0.3767 time: 0.0000s\n",
      "Epoch: 0040 loss_train: 1.5556 acc_train: 0.4429 loss_val: 1.6343 acc_val: 0.3767 time: 0.0145s\n",
      "Epoch: 0041 loss_train: 1.5854 acc_train: 0.4071 loss_val: 1.6251 acc_val: 0.3800 time: 0.0055s\n",
      "Epoch: 0042 loss_train: 1.5680 acc_train: 0.4357 loss_val: 1.6155 acc_val: 0.3833 time: 0.0000s\n",
      "Epoch: 0043 loss_train: 1.5586 acc_train: 0.4071 loss_val: 1.6054 acc_val: 0.4000 time: 0.0110s\n",
      "Epoch: 0044 loss_train: 1.5307 acc_train: 0.4143 loss_val: 1.5949 acc_val: 0.4067 time: 0.0000s\n",
      "Epoch: 0045 loss_train: 1.5258 acc_train: 0.4429 loss_val: 1.5844 acc_val: 0.4100 time: 0.0089s\n",
      "Epoch: 0046 loss_train: 1.4932 acc_train: 0.4500 loss_val: 1.5738 acc_val: 0.4167 time: 0.0065s\n",
      "Epoch: 0047 loss_train: 1.5008 acc_train: 0.4714 loss_val: 1.5630 acc_val: 0.4300 time: 0.0035s\n",
      "Epoch: 0048 loss_train: 1.5084 acc_train: 0.4500 loss_val: 1.5518 acc_val: 0.4400 time: 0.0105s\n",
      "Epoch: 0049 loss_train: 1.4502 acc_train: 0.4786 loss_val: 1.5401 acc_val: 0.4633 time: 0.0020s\n",
      "Epoch: 0050 loss_train: 1.4502 acc_train: 0.4643 loss_val: 1.5280 acc_val: 0.4633 time: 0.0080s\n",
      "Epoch: 0051 loss_train: 1.4061 acc_train: 0.5000 loss_val: 1.5160 acc_val: 0.4800 time: 0.0097s\n",
      "Epoch: 0052 loss_train: 1.4032 acc_train: 0.4857 loss_val: 1.5036 acc_val: 0.4867 time: 0.0000s\n",
      "Epoch: 0053 loss_train: 1.4049 acc_train: 0.5071 loss_val: 1.4907 acc_val: 0.4933 time: 0.0103s\n",
      "Epoch: 0054 loss_train: 1.3968 acc_train: 0.5071 loss_val: 1.4774 acc_val: 0.5167 time: 0.0035s\n",
      "Epoch: 0055 loss_train: 1.3775 acc_train: 0.5500 loss_val: 1.4640 acc_val: 0.5367 time: 0.0070s\n",
      "Epoch: 0056 loss_train: 1.3345 acc_train: 0.5500 loss_val: 1.4504 acc_val: 0.5400 time: 0.0000s\n",
      "Epoch: 0057 loss_train: 1.3318 acc_train: 0.5429 loss_val: 1.4371 acc_val: 0.5500 time: 0.0092s\n",
      "Epoch: 0058 loss_train: 1.3211 acc_train: 0.5500 loss_val: 1.4238 acc_val: 0.5600 time: 0.0100s\n",
      "Epoch: 0059 loss_train: 1.2997 acc_train: 0.5786 loss_val: 1.4107 acc_val: 0.5667 time: 0.0000s\n",
      "Epoch: 0060 loss_train: 1.2611 acc_train: 0.6429 loss_val: 1.3975 acc_val: 0.5767 time: 0.0121s\n",
      "Epoch: 0061 loss_train: 1.2745 acc_train: 0.6143 loss_val: 1.3844 acc_val: 0.5900 time: 0.0000s\n",
      "Epoch: 0062 loss_train: 1.2499 acc_train: 0.6357 loss_val: 1.3715 acc_val: 0.6067 time: 0.0080s\n",
      "Epoch: 0063 loss_train: 1.2248 acc_train: 0.6571 loss_val: 1.3583 acc_val: 0.6133 time: 0.0083s\n",
      "Epoch: 0064 loss_train: 1.2142 acc_train: 0.6500 loss_val: 1.3448 acc_val: 0.6267 time: 0.0015s\n",
      "Epoch: 0065 loss_train: 1.2217 acc_train: 0.6714 loss_val: 1.3310 acc_val: 0.6367 time: 0.0101s\n",
      "Epoch: 0066 loss_train: 1.1527 acc_train: 0.7000 loss_val: 1.3171 acc_val: 0.6533 time: 0.0035s\n",
      "Epoch: 0067 loss_train: 1.1551 acc_train: 0.7071 loss_val: 1.3031 acc_val: 0.6633 time: 0.0065s\n",
      "Epoch: 0068 loss_train: 1.1458 acc_train: 0.7286 loss_val: 1.2891 acc_val: 0.6733 time: 0.0000s\n",
      "Epoch: 0069 loss_train: 1.1422 acc_train: 0.7143 loss_val: 1.2756 acc_val: 0.6833 time: 0.0103s\n",
      "Epoch: 0070 loss_train: 1.1042 acc_train: 0.7357 loss_val: 1.2630 acc_val: 0.7033 time: 0.0000s\n",
      "Epoch: 0071 loss_train: 1.0885 acc_train: 0.7357 loss_val: 1.2510 acc_val: 0.7033 time: 0.0095s\n",
      "Epoch: 0072 loss_train: 1.0866 acc_train: 0.7357 loss_val: 1.2389 acc_val: 0.7100 time: 0.0054s\n",
      "Epoch: 0073 loss_train: 1.1051 acc_train: 0.7786 loss_val: 1.2268 acc_val: 0.7200 time: 0.0050s\n",
      "Epoch: 0074 loss_train: 1.0502 acc_train: 0.7929 loss_val: 1.2145 acc_val: 0.7267 time: 0.0100s\n",
      "Epoch: 0075 loss_train: 1.0212 acc_train: 0.8071 loss_val: 1.2024 acc_val: 0.7300 time: 0.0018s\n",
      "Epoch: 0076 loss_train: 1.0464 acc_train: 0.7571 loss_val: 1.1900 acc_val: 0.7300 time: 0.0080s\n",
      "Epoch: 0077 loss_train: 0.9887 acc_train: 0.8071 loss_val: 1.1778 acc_val: 0.7267 time: 0.0069s\n",
      "Epoch: 0078 loss_train: 0.9932 acc_train: 0.7786 loss_val: 1.1656 acc_val: 0.7367 time: 0.0040s\n",
      "Epoch: 0079 loss_train: 1.0187 acc_train: 0.7429 loss_val: 1.1541 acc_val: 0.7400 time: 0.0000s\n",
      "Epoch: 0080 loss_train: 0.9586 acc_train: 0.7857 loss_val: 1.1424 acc_val: 0.7433 time: 0.0119s\n",
      "Epoch: 0081 loss_train: 0.9614 acc_train: 0.7571 loss_val: 1.1311 acc_val: 0.7433 time: 0.0075s\n",
      "Epoch: 0082 loss_train: 0.9322 acc_train: 0.8214 loss_val: 1.1197 acc_val: 0.7500 time: 0.0000s\n",
      "Epoch: 0083 loss_train: 0.9721 acc_train: 0.7786 loss_val: 1.1078 acc_val: 0.7600 time: 0.0096s\n",
      "Epoch: 0084 loss_train: 0.9175 acc_train: 0.7929 loss_val: 1.0958 acc_val: 0.7667 time: 0.0000s\n",
      "Epoch: 0085 loss_train: 0.9031 acc_train: 0.7929 loss_val: 1.0847 acc_val: 0.7700 time: 0.0150s\n",
      "Epoch: 0086 loss_train: 0.9208 acc_train: 0.8071 loss_val: 1.0743 acc_val: 0.7800 time: 0.0054s\n",
      "Epoch: 0087 loss_train: 0.9104 acc_train: 0.8071 loss_val: 1.0638 acc_val: 0.7867 time: 0.0000s\n",
      "Epoch: 0088 loss_train: 0.8574 acc_train: 0.8286 loss_val: 1.0540 acc_val: 0.7833 time: 0.0101s\n",
      "Epoch: 0089 loss_train: 0.8476 acc_train: 0.8714 loss_val: 1.0451 acc_val: 0.7867 time: 0.0000s\n",
      "Epoch: 0090 loss_train: 0.8284 acc_train: 0.8571 loss_val: 1.0375 acc_val: 0.7900 time: 0.0094s\n",
      "Epoch: 0091 loss_train: 0.8754 acc_train: 0.8286 loss_val: 1.0294 acc_val: 0.7867 time: 0.0061s\n",
      "Epoch: 0092 loss_train: 0.7847 acc_train: 0.8714 loss_val: 1.0220 acc_val: 0.7800 time: 0.0040s\n",
      "Epoch: 0093 loss_train: 0.8309 acc_train: 0.8286 loss_val: 1.0135 acc_val: 0.7833 time: 0.0000s\n",
      "Epoch: 0094 loss_train: 0.8313 acc_train: 0.8429 loss_val: 1.0042 acc_val: 0.7900 time: 0.0120s\n",
      "Epoch: 0095 loss_train: 0.8108 acc_train: 0.8357 loss_val: 0.9939 acc_val: 0.7900 time: 0.0000s\n",
      "Epoch: 0096 loss_train: 0.7895 acc_train: 0.9000 loss_val: 0.9843 acc_val: 0.7900 time: 0.0090s\n",
      "Epoch: 0097 loss_train: 0.8085 acc_train: 0.8357 loss_val: 0.9751 acc_val: 0.7900 time: 0.0069s\n",
      "Epoch: 0098 loss_train: 0.7940 acc_train: 0.8571 loss_val: 0.9668 acc_val: 0.7900 time: 0.0025s\n",
      "Epoch: 0099 loss_train: 0.7486 acc_train: 0.8500 loss_val: 0.9596 acc_val: 0.7900 time: 0.0095s\n",
      "Epoch: 0100 loss_train: 0.7684 acc_train: 0.8571 loss_val: 0.9532 acc_val: 0.7933 time: 0.0040s\n",
      "Epoch: 0101 loss_train: 0.8088 acc_train: 0.8143 loss_val: 0.9476 acc_val: 0.7867 time: 0.0060s\n",
      "Epoch: 0102 loss_train: 0.7509 acc_train: 0.8500 loss_val: 0.9427 acc_val: 0.7867 time: 0.0000s\n",
      "Epoch: 0103 loss_train: 0.7322 acc_train: 0.8714 loss_val: 0.9378 acc_val: 0.7900 time: 0.0101s\n",
      "Epoch: 0104 loss_train: 0.7379 acc_train: 0.8500 loss_val: 0.9326 acc_val: 0.7900 time: 0.0008s\n",
      "Epoch: 0105 loss_train: 0.7432 acc_train: 0.8500 loss_val: 0.9270 acc_val: 0.7900 time: 0.0091s\n",
      "Epoch: 0106 loss_train: 0.7146 acc_train: 0.8500 loss_val: 0.9220 acc_val: 0.7900 time: 0.0059s\n",
      "Epoch: 0107 loss_train: 0.6762 acc_train: 0.8929 loss_val: 0.9145 acc_val: 0.7933 time: 0.0045s\n",
      "Epoch: 0108 loss_train: 0.7415 acc_train: 0.8571 loss_val: 0.9066 acc_val: 0.7933 time: 0.0000s\n",
      "Epoch: 0109 loss_train: 0.7048 acc_train: 0.8786 loss_val: 0.8993 acc_val: 0.7967 time: 0.0115s\n",
      "Epoch: 0110 loss_train: 0.6744 acc_train: 0.8714 loss_val: 0.8927 acc_val: 0.7967 time: 0.0000s\n",
      "Epoch: 0111 loss_train: 0.6566 acc_train: 0.9000 loss_val: 0.8865 acc_val: 0.8000 time: 0.0085s\n",
      "Epoch: 0112 loss_train: 0.6474 acc_train: 0.8929 loss_val: 0.8814 acc_val: 0.8000 time: 0.0000s\n",
      "Epoch: 0113 loss_train: 0.6870 acc_train: 0.8500 loss_val: 0.8776 acc_val: 0.8033 time: 0.0096s\n",
      "Epoch: 0114 loss_train: 0.6502 acc_train: 0.8786 loss_val: 0.8740 acc_val: 0.8033 time: 0.0000s\n",
      "Epoch: 0115 loss_train: 0.7072 acc_train: 0.8714 loss_val: 0.8706 acc_val: 0.8033 time: 0.0135s\n",
      "Epoch: 0116 loss_train: 0.6725 acc_train: 0.9000 loss_val: 0.8666 acc_val: 0.8000 time: 0.0065s\n",
      "Epoch: 0117 loss_train: 0.6465 acc_train: 0.8571 loss_val: 0.8630 acc_val: 0.8000 time: 0.0000s\n",
      "Epoch: 0118 loss_train: 0.6544 acc_train: 0.8714 loss_val: 0.8574 acc_val: 0.7967 time: 0.0101s\n",
      "Epoch: 0119 loss_train: 0.6172 acc_train: 0.8786 loss_val: 0.8521 acc_val: 0.8000 time: 0.0000s\n",
      "Epoch: 0120 loss_train: 0.5936 acc_train: 0.9214 loss_val: 0.8466 acc_val: 0.8033 time: 0.0099s\n",
      "Epoch: 0121 loss_train: 0.6284 acc_train: 0.8714 loss_val: 0.8418 acc_val: 0.8033 time: 0.0059s\n",
      "Epoch: 0122 loss_train: 0.6548 acc_train: 0.8857 loss_val: 0.8381 acc_val: 0.8033 time: 0.0040s\n",
      "Epoch: 0123 loss_train: 0.6233 acc_train: 0.8929 loss_val: 0.8354 acc_val: 0.8033 time: 0.0102s\n",
      "Epoch: 0124 loss_train: 0.6077 acc_train: 0.8786 loss_val: 0.8339 acc_val: 0.7967 time: 0.0018s\n",
      "Epoch: 0125 loss_train: 0.6313 acc_train: 0.8643 loss_val: 0.8323 acc_val: 0.7967 time: 0.0080s\n",
      "Epoch: 0126 loss_train: 0.5846 acc_train: 0.9143 loss_val: 0.8304 acc_val: 0.7967 time: 0.0073s\n",
      "Epoch: 0127 loss_train: 0.5779 acc_train: 0.9071 loss_val: 0.8273 acc_val: 0.7967 time: 0.0030s\n",
      "Epoch: 0128 loss_train: 0.6149 acc_train: 0.9000 loss_val: 0.8237 acc_val: 0.8000 time: 0.0000s\n",
      "Epoch: 0129 loss_train: 0.6036 acc_train: 0.9286 loss_val: 0.8204 acc_val: 0.8033 time: 0.0131s\n",
      "Epoch: 0130 loss_train: 0.5710 acc_train: 0.9143 loss_val: 0.8178 acc_val: 0.8033 time: 0.0000s\n",
      "Epoch: 0131 loss_train: 0.5964 acc_train: 0.9214 loss_val: 0.8153 acc_val: 0.7967 time: 0.0070s\n",
      "Epoch: 0132 loss_train: 0.6144 acc_train: 0.8571 loss_val: 0.8114 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0133 loss_train: 0.5743 acc_train: 0.8929 loss_val: 0.8081 acc_val: 0.8000 time: 0.0015s\n",
      "Epoch: 0134 loss_train: 0.5988 acc_train: 0.8929 loss_val: 0.8046 acc_val: 0.8000 time: 0.0092s\n",
      "Epoch: 0135 loss_train: 0.5577 acc_train: 0.9000 loss_val: 0.8009 acc_val: 0.8000 time: 0.0050s\n",
      "Epoch: 0136 loss_train: 0.5746 acc_train: 0.9214 loss_val: 0.7974 acc_val: 0.8033 time: 0.0051s\n",
      "Epoch: 0137 loss_train: 0.5600 acc_train: 0.9357 loss_val: 0.7935 acc_val: 0.8033 time: 0.0000s\n",
      "Epoch: 0138 loss_train: 0.5373 acc_train: 0.9143 loss_val: 0.7901 acc_val: 0.8000 time: 0.0097s\n",
      "Epoch: 0139 loss_train: 0.5595 acc_train: 0.9286 loss_val: 0.7876 acc_val: 0.8033 time: 0.0010s\n",
      "Epoch: 0140 loss_train: 0.5872 acc_train: 0.9214 loss_val: 0.7849 acc_val: 0.8000 time: 0.0093s\n",
      "Epoch: 0141 loss_train: 0.5267 acc_train: 0.8929 loss_val: 0.7826 acc_val: 0.8033 time: 0.0064s\n",
      "Epoch: 0142 loss_train: 0.5366 acc_train: 0.9286 loss_val: 0.7808 acc_val: 0.8033 time: 0.0035s\n",
      "Epoch: 0143 loss_train: 0.5246 acc_train: 0.9143 loss_val: 0.7798 acc_val: 0.8067 time: 0.0000s\n",
      "Epoch: 0144 loss_train: 0.4976 acc_train: 0.9429 loss_val: 0.7788 acc_val: 0.8067 time: 0.0130s\n",
      "Epoch: 0145 loss_train: 0.5034 acc_train: 0.9143 loss_val: 0.7783 acc_val: 0.8100 time: 0.0000s\n",
      "Epoch: 0146 loss_train: 0.5275 acc_train: 0.9000 loss_val: 0.7769 acc_val: 0.8133 time: 0.0070s\n",
      "Epoch: 0147 loss_train: 0.4891 acc_train: 0.9143 loss_val: 0.7747 acc_val: 0.8100 time: 0.0079s\n",
      "Epoch: 0148 loss_train: 0.5084 acc_train: 0.8929 loss_val: 0.7713 acc_val: 0.8100 time: 0.0058s\n",
      "Epoch: 0149 loss_train: 0.5146 acc_train: 0.8857 loss_val: 0.7670 acc_val: 0.8067 time: 0.0060s\n",
      "Epoch: 0150 loss_train: 0.5240 acc_train: 0.9286 loss_val: 0.7631 acc_val: 0.8100 time: 0.0000s\n",
      "Epoch: 0151 loss_train: 0.5450 acc_train: 0.9000 loss_val: 0.7600 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0152 loss_train: 0.5365 acc_train: 0.8929 loss_val: 0.7583 acc_val: 0.8133 time: 0.0000s\n",
      "Epoch: 0153 loss_train: 0.5433 acc_train: 0.9071 loss_val: 0.7577 acc_val: 0.8200 time: 0.0091s\n",
      "Epoch: 0154 loss_train: 0.5079 acc_train: 0.9000 loss_val: 0.7573 acc_val: 0.8133 time: 0.0053s\n",
      "Epoch: 0155 loss_train: 0.5237 acc_train: 0.9214 loss_val: 0.7573 acc_val: 0.8133 time: 0.0000s\n",
      "Epoch: 0156 loss_train: 0.4862 acc_train: 0.9357 loss_val: 0.7579 acc_val: 0.8133 time: 0.0116s\n",
      "Epoch: 0157 loss_train: 0.5166 acc_train: 0.9071 loss_val: 0.7561 acc_val: 0.8100 time: 0.0000s\n",
      "Epoch: 0158 loss_train: 0.5069 acc_train: 0.9071 loss_val: 0.7523 acc_val: 0.8067 time: 0.0085s\n",
      "Epoch: 0159 loss_train: 0.5006 acc_train: 0.9143 loss_val: 0.7487 acc_val: 0.8100 time: 0.0065s\n",
      "Epoch: 0160 loss_train: 0.5208 acc_train: 0.9286 loss_val: 0.7467 acc_val: 0.8067 time: 0.0035s\n",
      "Epoch: 0161 loss_train: 0.5019 acc_train: 0.9286 loss_val: 0.7444 acc_val: 0.8100 time: 0.0097s\n",
      "Epoch: 0162 loss_train: 0.4953 acc_train: 0.9357 loss_val: 0.7419 acc_val: 0.8100 time: 0.0025s\n",
      "Epoch: 0163 loss_train: 0.4413 acc_train: 0.9143 loss_val: 0.7396 acc_val: 0.8100 time: 0.0075s\n",
      "Epoch: 0164 loss_train: 0.5164 acc_train: 0.9000 loss_val: 0.7371 acc_val: 0.8100 time: 0.0000s\n",
      "Epoch: 0165 loss_train: 0.5033 acc_train: 0.9500 loss_val: 0.7344 acc_val: 0.8133 time: 0.0103s\n",
      "Epoch: 0166 loss_train: 0.4836 acc_train: 0.9286 loss_val: 0.7326 acc_val: 0.8133 time: 0.0000s\n",
      "Epoch: 0167 loss_train: 0.4377 acc_train: 0.9286 loss_val: 0.7311 acc_val: 0.8167 time: 0.0096s\n",
      "Epoch: 0168 loss_train: 0.4703 acc_train: 0.9429 loss_val: 0.7301 acc_val: 0.8133 time: 0.0045s\n",
      "Epoch: 0169 loss_train: 0.5178 acc_train: 0.9500 loss_val: 0.7289 acc_val: 0.8200 time: 0.0065s\n",
      "Epoch: 0170 loss_train: 0.4151 acc_train: 0.9500 loss_val: 0.7278 acc_val: 0.8200 time: 0.0000s\n",
      "Epoch: 0171 loss_train: 0.4896 acc_train: 0.9286 loss_val: 0.7262 acc_val: 0.8200 time: 0.0090s\n",
      "Epoch: 0172 loss_train: 0.4802 acc_train: 0.9571 loss_val: 0.7242 acc_val: 0.8200 time: 0.0000s\n",
      "Epoch: 0173 loss_train: 0.4397 acc_train: 0.9357 loss_val: 0.7235 acc_val: 0.8200 time: 0.0103s\n",
      "Epoch: 0174 loss_train: 0.4926 acc_train: 0.9143 loss_val: 0.7220 acc_val: 0.8133 time: 0.0096s\n",
      "Epoch: 0175 loss_train: 0.4621 acc_train: 0.9357 loss_val: 0.7210 acc_val: 0.8133 time: 0.0000s\n",
      "Epoch: 0176 loss_train: 0.4665 acc_train: 0.9143 loss_val: 0.7209 acc_val: 0.8133 time: 0.0102s\n",
      "Epoch: 0177 loss_train: 0.4500 acc_train: 0.9500 loss_val: 0.7207 acc_val: 0.8100 time: 0.0015s\n",
      "Epoch: 0178 loss_train: 0.4318 acc_train: 0.9286 loss_val: 0.7202 acc_val: 0.8067 time: 0.0085s\n",
      "Epoch: 0179 loss_train: 0.4222 acc_train: 0.9357 loss_val: 0.7183 acc_val: 0.8067 time: 0.0000s\n",
      "Epoch: 0180 loss_train: 0.4088 acc_train: 0.9429 loss_val: 0.7156 acc_val: 0.8133 time: 0.0103s\n",
      "Epoch: 0181 loss_train: 0.4461 acc_train: 0.9357 loss_val: 0.7135 acc_val: 0.8133 time: 0.0000s\n",
      "Epoch: 0182 loss_train: 0.4648 acc_train: 0.9143 loss_val: 0.7119 acc_val: 0.8100 time: 0.0096s\n",
      "Epoch: 0183 loss_train: 0.4776 acc_train: 0.9000 loss_val: 0.7109 acc_val: 0.8100 time: 0.0035s\n",
      "Epoch: 0184 loss_train: 0.4653 acc_train: 0.9429 loss_val: 0.7103 acc_val: 0.8100 time: 0.0065s\n",
      "Epoch: 0185 loss_train: 0.4549 acc_train: 0.9214 loss_val: 0.7112 acc_val: 0.8067 time: 0.0000s\n",
      "Epoch: 0186 loss_train: 0.4689 acc_train: 0.9500 loss_val: 0.7108 acc_val: 0.8100 time: 0.0104s\n",
      "Epoch: 0187 loss_train: 0.4451 acc_train: 0.9357 loss_val: 0.7085 acc_val: 0.8100 time: 0.0000s\n",
      "Epoch: 0188 loss_train: 0.4768 acc_train: 0.8929 loss_val: 0.7075 acc_val: 0.8100 time: 0.0106s\n",
      "Epoch: 0189 loss_train: 0.4264 acc_train: 0.9357 loss_val: 0.7069 acc_val: 0.8067 time: 0.0040s\n",
      "Epoch: 0190 loss_train: 0.4214 acc_train: 0.9214 loss_val: 0.7064 acc_val: 0.8067 time: 0.0050s\n",
      "Epoch: 0191 loss_train: 0.4515 acc_train: 0.9286 loss_val: 0.7055 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0192 loss_train: 0.4414 acc_train: 0.9429 loss_val: 0.7028 acc_val: 0.8100 time: 0.0000s\n",
      "Epoch: 0193 loss_train: 0.4719 acc_train: 0.9000 loss_val: 0.6991 acc_val: 0.8133 time: 0.0090s\n",
      "Epoch: 0194 loss_train: 0.4180 acc_train: 0.9500 loss_val: 0.6971 acc_val: 0.8067 time: 0.0068s\n",
      "Epoch: 0195 loss_train: 0.4365 acc_train: 0.9286 loss_val: 0.6971 acc_val: 0.8100 time: 0.0035s\n",
      "Epoch: 0196 loss_train: 0.3926 acc_train: 0.9429 loss_val: 0.6979 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0197 loss_train: 0.3966 acc_train: 0.9571 loss_val: 0.6990 acc_val: 0.8167 time: 0.0025s\n",
      "Epoch: 0198 loss_train: 0.4218 acc_train: 0.9429 loss_val: 0.6998 acc_val: 0.8100 time: 0.0075s\n",
      "Epoch: 0199 loss_train: 0.4218 acc_train: 0.9214 loss_val: 0.7005 acc_val: 0.8100 time: 0.0082s\n",
      "Epoch: 0200 loss_train: 0.4119 acc_train: 0.9571 loss_val: 0.6999 acc_val: 0.8133 time: 0.0015s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.1695s\n",
      "Test set results: loss= 0.7460 accuracy= 0.8370\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T03:16:20.216504700Z",
     "start_time": "2023-10-25T03:16:19.031834400Z"
    }
   },
   "id": "26d8e85f61bdb31c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "gnn",
   "language": "python",
   "display_name": "GNN"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
